"""
Pan115 Scraper - 115äº‘ç›˜æ™ºèƒ½æ–‡ä»¶åˆ®å‰Šå™¨
=====================================

ä½œè€…: jonntd@gmail.com
æœ€åæ›´æ–°: 2025-07-03
è®¸å¯: MIT License
"""

# æ ‡å‡†åº“å¯¼å…¥
import os
import json
import logging
import threading
import datetime
import requests
import re
import sys
import subprocess
import time
# hashlib å¯¼å…¥å·²åˆ é™¤ï¼ˆç¼“å­˜åŠŸèƒ½ä¸å†éœ€è¦ï¼‰
import math
import queue
import uuid
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
from collections import deque
from threading import Thread
from concurrent.futures import ThreadPoolExecutor, as_completed
from logging.handlers import RotatingFileHandler
from urllib import parse

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from flask import Flask, render_template, request, jsonify

# ================================
# åº”ç”¨ç¨‹åºåˆå§‹åŒ–å’Œå¸¸é‡å®šä¹‰
# ================================

app = Flask(__name__)

# ================================
# ç¼“å­˜åŠŸèƒ½å·²åˆ é™¤
# ================================



# ================================
# ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿ
# ================================

from enum import Enum
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
import uuid
import queue
import threading

# ğŸš€ ä»»åŠ¡é˜Ÿåˆ—é…ç½®
TASK_QUEUE_MAX_SIZE = 10  # æœ€å¤§é˜Ÿåˆ—å¤§å°
TASK_TIMEOUT_SECONDS = 300  # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
TASK_QUEUE_GET_TIMEOUT = 1.0  # ä»»åŠ¡é˜Ÿåˆ—è·å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# ä»»åŠ¡å–æ¶ˆæ§åˆ¶å…¨å±€å˜é‡
current_task_cancelled = False
current_task_id = None

class TaskCancelledException(Exception):
    """ä»»åŠ¡è¢«å–æ¶ˆå¼‚å¸¸"""
    pass

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"      # ç­‰å¾…ä¸­
    RUNNING = "running"      # æ‰§è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"        # å¤±è´¥
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ
    TIMEOUT = "timeout"      # è¶…æ—¶

@dataclass
class GroupingTask:
    """æ™ºèƒ½åˆ†ç»„ä»»åŠ¡æ•°æ®ç±»"""
    task_id: str
    folder_id: str
    folder_name: str
    status: TaskStatus = TaskStatus.PENDING
    created_at: float = field(default_factory=time.time)
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    progress: float = 0.0  # è¿›åº¦ç™¾åˆ†æ¯” 0-100
    processed_items: int = 0  # å·²å¤„ç†çš„é¡¹ç›®æ•°é‡
    total_items: int = 0  # æ€»é¡¹ç›®æ•°é‡

    def get_duration(self) -> Optional[float]:
        """è·å–ä»»åŠ¡æ‰§è¡Œæ—¶é•¿"""
        if self.started_at and self.completed_at:
            return self.completed_at - self.started_at
        elif self.started_at:
            return time.time() - self.started_at
        return None

class GroupingTaskManager:
    """æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self, max_queue_size: int = 10, task_timeout: int = 300):
        self.task_queue = queue.Queue(maxsize=max_queue_size)
        self.active_tasks: Dict[str, GroupingTask] = {}
        self.completed_tasks: Dict[str, GroupingTask] = {}
        self.max_completed_tasks = 50  # æœ€å¤šä¿ç•™50ä¸ªå·²å®Œæˆä»»åŠ¡
        self.task_timeout = task_timeout  # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.lock = threading.RLock()
        self.worker_thread = None
        self.is_running = False
        self._start_worker()

    def _start_worker(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        if not self.is_running:
            self.is_running = True
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            logging.info("ğŸš€ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨å·²å¯åŠ¨")

    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
                task = self.task_queue.get(timeout=TASK_QUEUE_GET_TIMEOUT)
                if task is None:  # åœæ­¢ä¿¡å·
                    break

                self._execute_task(task)
                self.task_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                logging.error(f"âŒ ä»»åŠ¡ç®¡ç†å™¨å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")

    def _execute_task(self, task: GroupingTask):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        with self.lock:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = TaskStatus.RUNNING
            task.started_at = time.time()
            self.active_tasks[task.task_id] = task

        logging.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œæ™ºèƒ½åˆ†ç»„ä»»åŠ¡: {task.task_id} (æ–‡ä»¶å¤¹: {task.folder_name})")

        # åˆ›å»ºè¶…æ—¶æ£€æŸ¥çº¿ç¨‹
        timeout_thread = threading.Thread(target=self._check_task_timeout, args=(task,), daemon=True)
        timeout_thread.start()

        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
            if task.status == TaskStatus.CANCELLED:
                return

            # æ‰§è¡Œå®é™…çš„åˆ†ç»„ä»»åŠ¡
            result = self._perform_grouping_analysis(task)

            with self.lock:
                if task.status not in [TaskStatus.CANCELLED, TaskStatus.TIMEOUT]:
                    task.status = TaskStatus.COMPLETED
                    task.completed_at = time.time()
                    task.result = result
                    task.progress = 100.0

                    # ç§»åŠ¨åˆ°å·²å®Œæˆä»»åŠ¡
                    self._move_to_completed(task)

                    logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å®Œæˆ: {task.task_id} (è€—æ—¶: {task.get_duration():.2f}ç§’)")

        except Exception as e:
            with self.lock:
                if task.status not in [TaskStatus.CANCELLED, TaskStatus.TIMEOUT]:
                    task.status = TaskStatus.FAILED
                    task.completed_at = time.time()
                    task.error = str(e)
                    self._move_to_completed(task)

                    logging.error(f"âŒ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å¤±è´¥: {task.task_id} - {e}")

    def _check_task_timeout(self, task: GroupingTask):
        """æ£€æŸ¥ä»»åŠ¡è¶…æ—¶"""
        time.sleep(self.task_timeout)

        with self.lock:
            if task.task_id in self.active_tasks and task.status == TaskStatus.RUNNING:
                # ä»»åŠ¡è¶…æ—¶
                task.status = TaskStatus.TIMEOUT
                task.completed_at = time.time()
                task.error = f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ (è¶…è¿‡ {self.task_timeout} ç§’)"
                self._move_to_completed(task)

                logging.warning(f"â° æ™ºèƒ½åˆ†ç»„ä»»åŠ¡è¶…æ—¶: {task.task_id} (è¶…æ—¶æ—¶é—´: {self.task_timeout}ç§’)")

    def _perform_grouping_analysis(self, task: GroupingTask) -> Dict[str, Any]:
        """æ‰§è¡Œå®é™…çš„åˆ†ç»„åˆ†æ"""
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        # è®¾ç½®å½“å‰ä»»åŠ¡IDï¼Œä»¥ä¾¿å…¨å±€å–æ¶ˆæœºåˆ¶èƒ½å¤Ÿå·¥ä½œ
        global current_task_id, current_task_cancelled
        current_task_id = task.task_id
        current_task_cancelled = False  # é‡ç½®å–æ¶ˆæ ‡å¿—

        # è¿™é‡Œè°ƒç”¨ç°æœ‰çš„åˆ†ç»„åˆ†æå‡½æ•°
        video_files = []
        get_video_files_recursively(task.folder_id, video_files)

        # å†æ¬¡æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        if not video_files:
            return {
                'success': False,
                'error': 'æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶',
                'movie_info': [],
                'count': 0,
                'size': '0GB'
            }

        # æ›´æ–°è¿›åº¦
        task.progress = 30.0

        # è°ƒç”¨åˆ†ç»„åˆ†æ
        def progress_callback(current, total, message):
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
            if task.status == TaskStatus.CANCELLED:
                raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

            # æ ¹æ®è¿›åº¦ç™¾åˆ†æ¯”æ›´æ–°ä»»åŠ¡è¿›åº¦
            if total > 0:
                progress_percent = (current / total) * 100
                # å°†è¿›åº¦æ˜ å°„åˆ°30-90%èŒƒå›´å†…ï¼ˆå‰é¢å·²ç»ç”¨äº†0-30%ï¼‰
                task.progress = 30.0 + (progress_percent * 0.6)

            # è¯¦ç»†çš„è¿›åº¦æ›´æ–°é€»è¾‘
            if 'å¼€å§‹æ™ºèƒ½åˆ†ç»„åˆ†æ' in message or 'å¼€å§‹æ™ºèƒ½æ–‡ä»¶åˆ†ç»„åˆ†æ' in message:
                task.progress = 40.0
                logging.info(f"ğŸ¯ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'å‡†å¤‡æ–‡ä»¶æ•°æ®' in message:
                task.progress = 45.0
                logging.info(f"ğŸ“ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'AIåˆ†æå®Œæˆ' in message:
                task.progress = 60.0
                logging.info(f"ğŸ”„ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'åˆ†ç»„åˆå¹¶å®Œæˆ' in message:
                task.progress = 80.0
                logging.info(f"â±ï¸ æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'ç”Ÿæˆæœ€ç»ˆç»“æœ' in message:
                task.progress = 90.0
                logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            elif 'åˆ†ç»„åˆ†æå®Œæˆ' in message:
                task.progress = 95.0
                logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")
            else:
                logging.info(f"ğŸ“Š æ™ºèƒ½åˆ†ç»„è¿›åº¦: {task.progress}% - {message}")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        grouping_result = get_folder_grouping_analysis_internal(video_files, task.folder_id, progress_callback)

        # æœ€åæ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
        if task.status == TaskStatus.CANCELLED:
            raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")

        return {
            'success': True,
            'movie_info': grouping_result.get('movie_info', []),
            'video_files': video_files,
            'count': len(video_files),
            'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB"
        }

    def _move_to_completed(self, task: GroupingTask):
        """å°†ä»»åŠ¡ç§»åŠ¨åˆ°å·²å®Œæˆåˆ—è¡¨"""
        if task.task_id in self.active_tasks:
            del self.active_tasks[task.task_id]

        self.completed_tasks[task.task_id] = task

        # æ¸…ç†è¿‡å¤šçš„å·²å®Œæˆä»»åŠ¡
        if len(self.completed_tasks) > self.max_completed_tasks:
            # åˆ é™¤æœ€æ—§çš„ä»»åŠ¡
            oldest_task_id = min(self.completed_tasks.keys(),
                                key=lambda tid: self.completed_tasks[tid].completed_at or 0)
            del self.completed_tasks[oldest_task_id]

    def submit_task(self, folder_id: str, folder_name: str) -> str:
        """æäº¤æ–°çš„åˆ†ç»„ä»»åŠ¡"""
        task_id = str(uuid.uuid4())
        task = GroupingTask(
            task_id=task_id,
            folder_id=str(folder_id),
            folder_name=folder_name
        )

        try:
            with self.lock:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶å¤¹çš„ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­æˆ–æ‰§è¡Œä¸­
                for existing_task in list(self.active_tasks.values()):
                    if existing_task.folder_id == str(folder_id) and existing_task.status in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                        raise ValueError(f"æ–‡ä»¶å¤¹ {folder_name} å·²æœ‰åˆ†ç»„ä»»åŠ¡åœ¨è¿›è¡Œä¸­")

                # æ£€æŸ¥é˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰ç›¸åŒæ–‡ä»¶å¤¹çš„ä»»åŠ¡
                temp_queue = []
                while not self.task_queue.empty():
                    try:
                        queued_task = self.task_queue.get_nowait()
                        if queued_task.folder_id == str(folder_id):
                            raise ValueError(f"æ–‡ä»¶å¤¹ {folder_name} å·²æœ‰åˆ†ç»„ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­")
                        temp_queue.append(queued_task)
                    except queue.Empty:
                        break

                # é‡æ–°æ”¾å›é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
                for queued_task in temp_queue:
                    self.task_queue.put_nowait(queued_task)

                # æ·»åŠ æ–°ä»»åŠ¡åˆ°é˜Ÿåˆ—å’Œæ´»åŠ¨ä»»åŠ¡åˆ—è¡¨
                self.task_queue.put_nowait(task)
                self.active_tasks[task_id] = task  # ç«‹å³æ·»åŠ åˆ°æ´»åŠ¨ä»»åŠ¡åˆ—è¡¨
                logging.info(f"ğŸ“ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤: {task_id} (æ–‡ä»¶å¤¹: {folder_name})")

                return task_id

        except queue.Full:
            raise ValueError("ä»»åŠ¡é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åå†è¯•")

    def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        with self.lock:
            if task_id in self.active_tasks:
                task = self.active_tasks[task_id]
                if task.status in [TaskStatus.PENDING, TaskStatus.RUNNING]:
                    task.status = TaskStatus.CANCELLED
                    task.completed_at = time.time()
                    self._move_to_completed(task)
                    logging.info(f"ğŸ›‘ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²å–æ¶ˆ: {task_id}")
                    return True
            return False

    def get_task_status(self, task_id: str) -> Optional[GroupingTask]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        with self.lock:
            if task_id in self.active_tasks:
                return self.active_tasks[task_id]
            elif task_id in self.completed_tasks:
                return self.completed_tasks[task_id]
            return None

    def get_queue_info(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—ä¿¡æ¯"""
        with self.lock:
            return {
                'queue_size': self.task_queue.qsize(),
                'active_tasks': len(self.active_tasks),
                'completed_tasks': len(self.completed_tasks),
                'worker_running': self.is_running and self.worker_thread and self.worker_thread.is_alive()
            }

    def cleanup_old_tasks(self, hours: int = 24):
        """æ¸…ç†æ—§ä»»åŠ¡"""
        cutoff_time = time.time() - (hours * 3600)
        with self.lock:
            old_task_ids = [
                task_id for task_id, task in self.completed_tasks.items()
                if task.completed_at and task.completed_at < cutoff_time
            ]
            for task_id in old_task_ids:
                del self.completed_tasks[task_id]
            if old_task_ids:
                logging.info(f"ğŸ§¹ æ¸…ç†äº† {len(old_task_ids)} ä¸ªæ—§ä»»åŠ¡")

    def get_health_status(self) -> Dict[str, Any]:
        """è·å–å¥åº·çŠ¶æ€"""
        with self.lock:
            return {
                'worker_running': self.is_running and self.worker_thread and self.worker_thread.is_alive(),
                'queue_size': self.task_queue.qsize(),
                'active_tasks': len(self.active_tasks),
                'completed_tasks': len(self.completed_tasks),
                'max_queue_size': self.task_queue.maxsize,
                'task_timeout': self.task_timeout
            }

    def restart_worker_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œé‡å¯å·¥ä½œçº¿ç¨‹"""
        if not self.is_running or not self.worker_thread or not self.worker_thread.is_alive():
            logging.warning("ğŸ”„ æ£€æµ‹åˆ°å·¥ä½œçº¿ç¨‹å¼‚å¸¸ï¼Œæ­£åœ¨é‡å¯...")
            self.is_running = False
            if self.worker_thread and self.worker_thread.is_alive():
                try:
                    self.task_queue.put(None, timeout=1.0)  # å‘é€åœæ­¢ä¿¡å·
                    self.worker_thread.join(timeout=5.0)
                except:
                    pass
            self._start_worker()

    def force_cleanup_stuck_tasks(self) -> int:
        """å¼ºåˆ¶æ¸…ç†å¡ä½çš„ä»»åŠ¡"""
        stuck_count = 0
        current_time = time.time()
        with self.lock:
            stuck_task_ids = []
            for task_id, task in self.active_tasks.items():
                if (task.status == TaskStatus.RUNNING and
                    task.started_at and
                    current_time - task.started_at > self.task_timeout * 2):
                    stuck_task_ids.append(task_id)
                    task.status = TaskStatus.TIMEOUT
                    task.completed_at = current_time
                    task.error = "ä»»åŠ¡è¢«å¼ºåˆ¶æ¸…ç†ï¼ˆç–‘ä¼¼å¡ä½ï¼‰"
                    stuck_count += 1

            for task_id in stuck_task_ids:
                self._move_to_completed(self.active_tasks[task_id])

        if stuck_count > 0:
            logging.warning(f"ğŸ§¹ å¼ºåˆ¶æ¸…ç†äº† {stuck_count} ä¸ªå¡ä½çš„ä»»åŠ¡")
        return stuck_count


# ================================
# æ™ºèƒ½åˆ†ç»„ç›¸å…³å‡½æ•°
# ================================

def get_video_files_recursively(folder_id, file_list, current_path="", depth=0):
    """
    é€’å½’è·å–æŒ‡å®šæ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        folder_id (int): è¦æœç´¢çš„æ–‡ä»¶å¤¹ID
        file_list (list): ç”¨äºå­˜å‚¨æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶çš„åˆ—è¡¨ï¼ˆä¼šè¢«ä¿®æ”¹ï¼‰
        current_path (str): å½“å‰æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        depth (int): é€’å½’æ·±åº¦ï¼Œç”¨äºæ§åˆ¶æ—¥å¿—è¾“å‡º
        use_concurrent (bool): æ˜¯å¦ä½¿ç”¨å¹¶å‘ä¼˜åŒ–ï¼Œé»˜è®¤True

    Note:
        æ­¤å‡½æ•°ä¼šä¿®æ”¹ä¼ å…¥çš„file_listå‚æ•°ï¼Œå°†æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶æ·»åŠ åˆ°å…¶ä¸­
    """
    try:
        if not current_path:
            current_path = get_folder_full_path(folder_id)
            logging.info(f"ğŸ” è°ƒè¯• - ä½¿ç”¨get_folder_full_pathè·å–è·¯å¾„: {current_path}")
        else:
            logging.info(f"ğŸ” è°ƒè¯• - ä½¿ç”¨ä¼ é€’çš„è·¯å¾„: {current_path}")

        # è·å–æ–‡ä»¶å¤¹å†…å®¹
        all_files = get_all_files_in_folder(folder_id, limit=100)
        logging.info(f"ğŸ“‚ æ‰«æ {folder_id} ({current_path}) - {len(all_files)} ä¸ªé¡¹ç›® (æ·±åº¦: {depth})")

        # åˆ†ç¦»è§†é¢‘æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
        video_files_in_folder = []
        subfolders = []

        for file_item in all_files:
            if file_item['type'] == 0:  # æ–‡ä»¶
                _, ext = os.path.splitext(file_item['filename'])
                if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                    video_files_in_folder.append(file_item)
            elif file_item['type'] == 1:  # æ–‡ä»¶å¤¹
                subfolders.append(file_item)

        # æ‰¹é‡å¤„ç†è§†é¢‘æ–‡ä»¶
        if video_files_in_folder:
            gb_in_bytes = 1024 ** 3
            for file_item in video_files_in_folder:
                bytes_value = file_item['size']
                gb_value = bytes_value / gb_in_bytes
                # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                full_file_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']

                # é™åˆ¶è·¯å¾„æœ€å¤šæ˜¾ç¤ºå€’æ•°ä¸‰å±‚
                file_path = limit_path_depth(full_file_path, 3)

                # åˆ›å»ºå¢å¼ºçš„æ–‡ä»¶é¡¹ï¼Œä¿ç•™åŸæœ‰ä¿¡æ¯å¹¶æ·»åŠ è®¡ç®—å­—æ®µ
                enhanced_file_item = file_item.copy()
                enhanced_file_item['file_path'] = file_path
                enhanced_file_item['size_gb'] = f"{gb_value:.1f}GB"

                file_list.append(enhanced_file_item)

            logging.info(f"âœ… å‘ç° {len(video_files_in_folder)} ä¸ªè§†é¢‘æ–‡ä»¶: {current_path}")

        # å¤„ç†å­æ–‡ä»¶å¤¹
        if subfolders:
            logging.info(f"ğŸ“ å‘ç° {len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå¼€å§‹é€’å½’å¤„ç†")

            # ä¸ºå­æ–‡ä»¶å¤¹æ„å»ºè·¯å¾„ç¼“å­˜
            for file_item in subfolders:
                subfolder_path = os.path.join(current_path, file_item['filename']) if current_path else file_item['filename']

            # é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            for file_item in subfolders:
                try:
                    # æ ¹æ®QPSé™åˆ¶è°ƒæ•´å»¶è¿Ÿæ—¶é—´ï¼Œç¡®ä¿ä¸è¶…è¿‡APIé™åˆ¶
                    time.sleep(1.2)  # 1.2ç§’å»¶è¿Ÿï¼Œç¡®ä¿QPS=1çš„é™åˆ¶
                    get_video_files_recursively(file_item['fileId'], file_list, subfolder_path, depth + 1)
                except Exception as e:
                    logging.error(f"å¤„ç†å­æ–‡ä»¶å¤¹ {file_item['filename']} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue

    except Exception as e:
        logging.error(f"é€’å½’è·å–è§†é¢‘æ–‡ä»¶å¤±è´¥ (æ–‡ä»¶å¤¹ID: {folder_id}): {e}")
        raise


def get_all_files_in_folder(folder_id, limit=100):
    """
    è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆè‡ªåŠ¨å¤„ç†åˆ†é¡µï¼‰

    Args:
        folder_id (int): æ–‡ä»¶å¤¹ID
        limit (int): æ¯é¡µè¿”å›çš„æ–‡ä»¶æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100
        check_cancellation (bool): æ˜¯å¦æ£€æŸ¥ä»»åŠ¡å–æ¶ˆçŠ¶æ€ï¼Œé»˜è®¤False

    Returns:
        list: åŒ…å«æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯çš„åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨115äº‘ç›˜APIè·å–æ–‡ä»¶å¤¹å†…å®¹
        response_data = _get_single_level_content_from_115(str(folder_id), 0, limit)

        if not response_data or response_data.get("success") is False:
            logging.warning(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {folder_id}")
            return []

        all_files = response_data.get("data", [])

        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted_files = []
        for item in all_files:
            if 'fid' in item:  # æ–‡ä»¶
                formatted_files.append({
                    'fileId': item['fid'],
                    'filename': item.get('n', ''),
                    'size': item.get('s', 0),
                    'type': 0,  # æ–‡ä»¶ç±»å‹
                    'parentFileId': folder_id
                })
            elif 'cid' in item:  # æ–‡ä»¶å¤¹
                formatted_files.append({
                    'fileId': item['cid'],
                    'filename': item.get('n', ''),
                    'size': 0,
                    'type': 1,  # æ–‡ä»¶å¤¹ç±»å‹
                    'parentFileId': folder_id
                })

        return formatted_files
    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥ (æ–‡ä»¶å¤¹ID: {folder_id}): {e}")
        return []


def get_folder_full_path(folder_id):
    """è·å–æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„"""
    try:
        if folder_id == 0 or folder_id == '0':
            return ""

        # ä½¿ç”¨115äº‘ç›˜APIè·å–è·¯å¾„ä¿¡æ¯
        response_data = _get_single_level_content_from_115(folder_id, 0, 1)
        if not response_data or 'path' not in response_data:
            logging.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤¹ {folder_id} çš„è·¯å¾„ä¿¡æ¯")
            return ""

        # ä»APIå“åº”ä¸­æå–è·¯å¾„
        path_parts = response_data.get('path', [])
        if len(path_parts) <= 1:
            return ""

        # æ„å»ºè·¯å¾„å­—ç¬¦ä¸²ï¼ˆè·³è¿‡æ ¹ç›®å½•ï¼‰
        path_names = [part.get('name', '') for part in path_parts[1:]]
        full_path = "/".join(path_names)

        logging.info(f"ğŸ” æ–‡ä»¶å¤¹ {folder_id} çš„å®Œæ•´è·¯å¾„: {full_path}")
        return full_path

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹è·¯å¾„å¤±è´¥: {e}")
        return ""


def limit_path_depth(path, max_depth=3):
    """é™åˆ¶è·¯å¾„æ˜¾ç¤ºæ·±åº¦"""
    if not path:
        return path

    parts = path.split(os.sep)
    if len(parts) <= max_depth:
        return path

    return os.sep.join(parts[-max_depth:])


def get_folder_grouping_analysis_internal(video_files, folder_id, progress_callback=None):
    """
    æ™ºèƒ½åˆ†ç»„åˆ†æå†…éƒ¨å‡½æ•° - å®Œæ•´ç‰ˆæœ¬ä»pan115-scraperå¤åˆ¶

    Args:
        video_files: è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        folder_id: æ–‡ä»¶å¤¹ID
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        dict: åˆ†ç»„åˆ†æç»“æœ
    """
    try:
        if not video_files:
            logging.warning("ğŸ“‚ æ²¡æœ‰æä¾›è§†é¢‘æ–‡ä»¶ï¼Œè¿”å›ç©ºç»“æœ")
            return {'movie_info': []}

        logging.info(f"ğŸ¬ å¼€å§‹æ™ºèƒ½åˆ†ç»„åˆ†æ: {len(video_files)} ä¸ªæ–‡ä»¶")

        if progress_callback:
            progress_callback(0, 100, "å¼€å§‹æ™ºèƒ½åˆ†ç»„åˆ†æ")

        # ç¼“å­˜æ£€æŸ¥å·²åˆ é™¤

        if progress_callback:
            progress_callback(10, 100, "å‡†å¤‡æ–‡ä»¶æ•°æ®")

        # 2. æ‰¹é‡å¤„ç†æ–‡ä»¶
        CHUNK_SIZE = app_config.get('CHUNK_SIZE', 50)

        if len(video_files) > CHUNK_SIZE:
            logging.info(f"ğŸ“Š æ–‡ä»¶æ•°é‡ {len(video_files)} è¶…è¿‡æ‰¹æ¬¡å¤§å° {CHUNK_SIZE}ï¼Œå¯ç”¨æ‰¹é‡å¤„ç†")
            all_groups = process_files_for_grouping(video_files, f"æ–‡ä»¶å¤¹ {folder_id}")
        else:
            logging.info(f"ğŸ“Š æ–‡ä»¶æ•°é‡ {len(video_files)} åœ¨æ‰¹æ¬¡å¤§å°å†…ï¼Œç›´æ¥å¤„ç†")
            all_groups = _call_ai_for_grouping(video_files)

        if progress_callback:
            progress_callback(60, 100, f"AIåˆ†æå®Œæˆï¼Œè·å¾— {len(all_groups)} ä¸ªåˆå§‹åˆ†ç»„")

        # 3. åˆå¹¶é‡å¤åˆ†ç»„
        if all_groups:
            logging.info(f"ğŸ”„ å¼€å§‹åˆå¹¶é‡å¤åˆ†ç»„: {len(all_groups)} ä¸ªåˆ†ç»„")
            all_groups = merge_duplicate_named_groups(all_groups)
            logging.info(f"âœ… é‡å¤åˆ†ç»„åˆå¹¶å®Œæˆ: {len(all_groups)} ä¸ªåˆ†ç»„")

        if progress_callback:
            progress_callback(80, 100, f"åˆ†ç»„åˆå¹¶å®Œæˆ: {len(all_groups)} ä¸ªåˆ†ç»„")

        # 4. æ™ºèƒ½åˆå¹¶åŒç³»åˆ—åˆ†ç»„
        if len(all_groups) > 1:
            logging.info(f"ğŸ¤– å¼€å§‹æ™ºèƒ½åˆå¹¶åŒç³»åˆ—åˆ†ç»„")
            all_groups = merge_same_series_groups(all_groups)
            logging.info(f"âœ… æ™ºèƒ½åˆå¹¶å®Œæˆ: {len(all_groups)} ä¸ªæœ€ç»ˆåˆ†ç»„")

        if progress_callback:
            progress_callback(90, 100, f"ç”Ÿæˆæœ€ç»ˆç»“æœ: {len(all_groups)} ä¸ªåˆ†ç»„")

        # 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_size_bytes = _calculate_total_size(video_files)
        size_str = _format_file_size(total_size_bytes)

        # 6. æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            'success': True,
            'count': len(video_files),
            'size': size_str,
            'total_size_bytes': int(total_size_bytes),
            'movie_info': all_groups,
            'video_files': video_files,
            'processing_stats': {
                'total_files': len(video_files),
                'total_groups': len(all_groups),
                'chunk_size': CHUNK_SIZE
            }
        }

        # ç¼“å­˜ç»“æœå·²åˆ é™¤

        if progress_callback:
            progress_callback(100, 100, f"åˆ†ç»„åˆ†æå®Œæˆ: {len(all_groups)} ä¸ªåˆ†ç»„")

        logging.info(f"âœ… æ™ºèƒ½åˆ†ç»„åˆ†æå®Œæˆ: {len(all_groups)} ä¸ªåˆ†ç»„ï¼Œ{len(video_files)} ä¸ªæ–‡ä»¶")
        return result

    except TaskCancelledException:
        logging.info("ğŸ›‘ æ™ºèƒ½åˆ†ç»„ä»»åŠ¡è¢«ç”¨æˆ·å–æ¶ˆ")
        if progress_callback:
            progress_callback(0, 100, "ä»»åŠ¡å·²å–æ¶ˆ")
        return {'movie_info': [], 'error': 'ä»»åŠ¡å·²è¢«ç”¨æˆ·å–æ¶ˆ', 'cancelled': True}

    except Exception as e:
        logging.error(f"âŒ æ™ºèƒ½åˆ†ç»„åˆ†æå¤±è´¥: {e}", exc_info=True)
        if progress_callback:
            progress_callback(0, 100, f"åˆ†æå¤±è´¥: {str(e)}")
        return {'movie_info': [], 'error': f'åˆ†ç»„åˆ†æå¤±è´¥: {str(e)}'}


# ================================
# æ™ºèƒ½åˆ†ç»„AIæç¤ºè¯æ¨¡æ¿ - ä»pan115-scraperå¤åˆ¶
# ================================

MAGIC_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†æ–‡ä»¶åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ–‡ä»¶åˆ—è¡¨ï¼Œæ ¹æ®æ–‡ä»¶åå°†ç›¸å…³æ–‡ä»¶åˆ†ç»„ã€‚

âš ï¸ **ä¸¥æ ¼è­¦å‘Š**ï¼šåªæœ‰ä¸»æ ‡é¢˜ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ä»¶æ‰èƒ½åˆ†ç»„ï¼

ğŸš« **ç»å¯¹ç¦æ­¢çš„é”™è¯¯åˆ†ç»„**ï¼š
- âŒ ä¸è¦å°†"æµ·åº•å°çºµé˜Ÿ"ã€"ç–¯ç‹‚å…ƒç´ åŸ"ã€"è“ç²¾çµ"åˆ†åœ¨ä¸€ç»„
- âŒ ä¸è¦ä»…å› ä¸ºéƒ½æ˜¯åŠ¨ç”»ç‰‡å°±åˆ†ç»„
- âŒ ä¸è¦ä»…å› ä¸ºéƒ½æ˜¯è¿ªå£«å°¼/çš®å…‹æ–¯å°±åˆ†ç»„
- âŒ ä¸è¦ä»…å› ä¸ºå¹´ä»½ç›¸è¿‘å°±åˆ†ç»„
- âŒ ä¸è¦å°†å®Œå…¨ä¸åŒçš„IP/å“ç‰Œæ··åˆ

ğŸ“º **ç”µè§†å‰§å‘½åæ ¼å¼ä¸¥æ ¼è¦æ±‚**ï¼š
- âœ… å¿…é¡»ä½¿ç”¨ï¼š`æ ‡é¢˜ (é¦–æ’­å¹´ä»½) Så­£æ•°`
- âœ… æ­£ç¡®ç¤ºä¾‹ï¼šSEAL Team (2017) S01, SEAL Team (2018) S02
- âŒ ç¦æ­¢æ ¼å¼ï¼šSEAL Team S01, SEAL Team (Season 1), SEAL Team (S01), SEAL Team (2018-2019) S02

âœ… **æ­£ç¡®åˆ†ç»„æ ‡å‡†**ï¼šä¸»æ ‡é¢˜å¿…é¡»ç›¸åŒæˆ–ä¸ºåŒä¸€ç³»åˆ—çš„ç»­é›†/å‰ä¼ 

## ğŸ“‹ æ–‡ä»¶åæ¨¡å¼è¯†åˆ«æŒ‡å—

### ğŸ¬ å¸¸è§æ–‡ä»¶åæ ¼å¼ï¼š
1. **ç”µè§†å‰§æ ¼å¼**ï¼š
   - `æ ‡é¢˜ (å¹´ä»½) S01E01.mkv` â†’ é—´è°è¿‡å®¶å®¶ (2022) S01E01.mkv
   - `æ ‡é¢˜ ç¬¬1å­£ ç¬¬1é›†.mkv` â†’ 180å¤©é‡å¯è®¡åˆ’ ç¬¬1å­£ ç¬¬1é›†.mkv
   - `Title S01E01.mkv` â†’ SPYÃ—FAMILY S01E01.mkv
   - `æ ‡é¢˜.S01E01.mkv` â†’ äº²çˆ±çš„å…¬ä¸»ç—….S01E01.mkv
   - `æ ‡é¢˜ S01 E01.mkv` â†’ æŸå‰§ S01 E01.mkv

2. **ç”µå½±æ ¼å¼**ï¼š
   - `æ ‡é¢˜ (å¹´ä»½).mkv` â†’ å¤ä»‡è€…è”ç›Ÿ (2012).mkv
   - `æ ‡é¢˜.å¹´ä»½.mkv` â†’ Avengers.2012.mkv
   - `æ ‡é¢˜ å¹´ä»½.mkv` â†’ é’¢é“ä¾  2008.mkv

3. **ç³»åˆ—ç”µå½±æ ¼å¼**ï¼š
   - `æ ‡é¢˜1.mkv, æ ‡é¢˜2.mkv` â†’ å¤ä»‡è€…è”ç›Ÿ1.mkv, å¤ä»‡è€…è”ç›Ÿ2.mkv
   - `æ ‡é¢˜ Part1.mkv` â†’ å“ˆåˆ©æ³¢ç‰¹ä¸æ­»äº¡åœ£å™¨ Part1.mkv

### ğŸ“Š è¿”å›æ ¼å¼è¦æ±‚ï¼š
å¿…é¡»è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œæ¯ä¸ªåˆ†ç»„åŒ…å«ï¼š
```json
[
  {
    "group_name": "åˆ†ç»„åç§°",
    "fileIds": ["æ–‡ä»¶ID1", "æ–‡ä»¶ID2", ...]
  }
]
```

### âš ï¸ æœ€ç»ˆè­¦å‘Šï¼š
- **åªæœ‰ä¸»æ ‡é¢˜ç›¸åŒçš„æ–‡ä»¶æ‰èƒ½åˆ†ç»„ï¼**
- **ç»å¯¹ç¦æ­¢å°†ä¸åŒIP/å“ç‰Œçš„ä½œå“åˆ†åœ¨ä¸€ç»„ï¼**
- **ç»å¯¹ç¦æ­¢æŒ‰ç±»å‹ã€å…¬å¸ã€å¹´ä»½ã€ä¸»é¢˜åˆ†ç»„ï¼**
- **åŒä¸€ç³»åˆ—çš„æ‰€æœ‰æ–‡ä»¶åº”è¯¥æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼ˆå¦‚æ‰€æœ‰å®å¯æ¢¦å‰§åœºç‰ˆï¼‰ï¼**
- å¿…é¡»è¿”å›å®Œæ•´çš„JSONæ ¼å¼
- fileIdsæ•°ç»„å¿…é¡»åŒ…å«æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„ID
- å¦‚æœæ²¡æœ‰å¯åˆ†ç»„çš„æ–‡ä»¶ï¼Œè¿”å› []
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—è¯´æ˜
- **å®å¯è¿”å›ç©ºæ•°ç»„[]ï¼Œä¹Ÿä¸è¦é”™è¯¯åˆ†ç»„ï¼**
- **å†æ¬¡å¼ºè°ƒï¼šæµ·åº•å°çºµé˜Ÿã€ç–¯ç‹‚å…ƒç´ åŸã€è“ç²¾çµæ˜¯å®Œå…¨ä¸åŒçš„IPï¼Œç»ä¸èƒ½åˆ†åœ¨ä¸€ç»„ï¼**
- **å®å¯æ¢¦å‰§åœºç‰ˆç³»åˆ—åº”è¯¥å…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼Œä¸è¦æŒ‰å¹´ä»½åˆ†æ®µï¼**
- **åä¾¦æ¢æŸ¯å—å‰§åœºç‰ˆã€èœ¡ç¬”å°æ–°å‰§åœºç‰ˆç­‰é•¿æœŸç³»åˆ—ä¹Ÿåº”è¯¥å…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„é‡Œï¼**
- **ç»å¯¹ä¸è¦å› ä¸ºæ–‡ä»¶æ•°é‡å¤šå°±æŒ‰å¹´ä»½åˆ†æ®µï¼ç³»åˆ—å®Œæ•´æ€§æœ€é‡è¦ï¼**
- **ç”µè§†å‰§å‘½åæ ¼å¼å¿…é¡»ä¸¥æ ¼ç»Ÿä¸€ï¼šæ ‡é¢˜ (é¦–æ’­å¹´ä»½) Så­£æ•°ï¼**
- **ç»å¯¹ç¦æ­¢çš„ç”µè§†å‰§æ ¼å¼ï¼šSEAL Team S01, SEAL Team (Season 1), SEAL Team (S01), SEAL Team (2018-2019) S02ï¼**
- **å¿…é¡»ä½¿ç”¨çš„æ­£ç¡®æ ¼å¼ï¼šSEAL Team (2017) S01, SEAL Team (2018) S02, SEAL Team (2019) S03ï¼**

"""

# æ™ºèƒ½åˆ†ç»„åˆå¹¶æç¤ºè¯æ¨¡æ¿
GROUP_MERGE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å½±è§†åˆ†ç±»ä¸“å®¶ã€‚åˆ†ææä¾›çš„åˆ†ç»„åˆ—è¡¨ï¼Œåˆ¤æ–­å“ªäº›åˆ†ç»„å±äºåŒä¸€ç³»åˆ—åº”è¯¥åˆå¹¶ã€‚

åˆå¹¶è§„åˆ™ï¼š
1. åŒä¸€ç³»åˆ—çš„ä¸åŒå­£/éƒ¨åˆ†åº”è¯¥åˆå¹¶
2. ç›¸åŒä¸»æ ‡é¢˜çš„ä½œå“åº”è¯¥åˆå¹¶
3. æ˜æ˜¾çš„ç»­é›†/å‰ä¼ å…³ç³»åº”è¯¥åˆå¹¶
4. ä¿æŒåŸæœ‰çš„åˆç†åˆ†ç»„

è¿”å›JSONæ ¼å¼ï¼š
{
  "merges": [
    {
      "target_group": "ç›®æ ‡åˆ†ç»„å",
      "source_groups": ["æºåˆ†ç»„1", "æºåˆ†ç»„2"],
      "reason": "åˆå¹¶åŸå› "
    }
  ]
}

å¦‚æœæ— éœ€åˆå¹¶ï¼Œè¿”å›ï¼š{"merges": []}
"""

# æ€§èƒ½ä¼˜åŒ–å¸¸é‡
API_CALL_DELAY = 0.1  # APIè°ƒç”¨é—´éš”ï¼Œé¿å…é¢‘ç‡é™åˆ¶
BATCH_PROCESS_SIZE = 20  # æ‰¹å¤„ç†å¤§å°ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

# æ€§èƒ½ä¼˜åŒ–æç¤ºï¼š
# 1. åˆ é™¤ç¼“å­˜åï¼Œæ‰€æœ‰APIè°ƒç”¨éƒ½æ˜¯å®æ—¶çš„ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™
# 2. å»ºè®®åœ¨é«˜é¢‘æ“ä½œæ—¶é€‚å½“å¢åŠ å»¶è¿Ÿï¼Œé¿å…APIé¢‘ç‡é™åˆ¶
# 3. å¤§é‡æ–‡ä»¶æ“ä½œæ—¶å»ºè®®åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜

# ================================
# å¼‚å¸¸ç±»å®šä¹‰
# ================================

class TaskCancelledException(Exception):
    """ä»»åŠ¡è¢«å–æ¶ˆå¼‚å¸¸"""
    pass

# ================================
# æ™ºèƒ½åˆ†ç»„æ”¯æŒå‡½æ•° - ä»pan115-scraperå¤åˆ¶
# ================================

def check_task_cancelled():
    """æ£€æŸ¥å½“å‰ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ"""
    # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—ä¸­çš„å–æ¶ˆçŠ¶æ€
    if 'grouping_task_manager' in globals() and grouping_task_manager:
        # æ£€æŸ¥æ˜¯å¦æœ‰è¢«å–æ¶ˆçš„ä»»åŠ¡
        pass  # ç®€åŒ–å®ç°ï¼Œé¿å…å¤æ‚çš„çŠ¶æ€æ£€æŸ¥

def call_ai_api(prompt, model=None, temperature=0.1):
    """
    è°ƒç”¨AI APIè¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰

    Args:
        prompt (str): å‘é€ç»™AIçš„æç¤ºè¯
        model (str, optional): ä½¿ç”¨çš„AIæ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        temperature (float): ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œ0.0-1.0ä¹‹é—´

    Returns:
        str or None: AIç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹æˆ–é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
        ai_model = model or app_config.get('MODEL', '')
        if not ai_model:
            logging.error("âŒ æœªé…ç½®AIæ¨¡å‹")
            return None

        # æ£€æŸ¥APIé…ç½®
        if not GEMINI_API_KEY or not GEMINI_API_URL:
            logging.error("âŒ AI APIé…ç½®ä¸å®Œæ•´")
            return None

        headers = {
            'Authorization': f'Bearer {GEMINI_API_KEY}',
            'Content-Type': 'application/json'
        }

        payload = {
            "model": ai_model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 8000,
            "temperature": temperature
        }

        logging.info(f"ğŸ¤– è°ƒç”¨AI API: {GEMINI_API_URL}")
        logging.info(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {ai_model}")

        # ä½¿ç”¨å…¨å±€é…ç½®çš„è¶…æ—¶æ—¶é—´
        AI_API_TIMEOUT = app_config.get('AI_API_TIMEOUT', 60)
        response = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=AI_API_TIMEOUT)

        logging.info(f"ğŸ“Š APIå“åº”çŠ¶æ€ç : {response.status_code}")

        response.raise_for_status()
        data = response.json()

        # æ£€æŸ¥å“åº”æ ¼å¼
        if "choices" not in data:
            logging.error(f"âŒ APIå“åº”æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘choiceså­—æ®µ: {data}")
            return None

        if not data["choices"] or len(data["choices"]) == 0:
            logging.error(f"âŒ APIå“åº”choicesä¸ºç©º: {data}")
            return None

        if "message" not in data["choices"][0]:
            logging.error(f"âŒ APIå“åº”ç¼ºå°‘messageå­—æ®µ: {data['choices'][0]}")
            return None

        if "content" not in data["choices"][0]["message"]:
            logging.error(f"âŒ APIå“åº”ç¼ºå°‘contentå­—æ®µ: {data['choices'][0]['message']}")
            return None

        content = data["choices"][0]["message"]["content"]
        logging.info(f"âœ… AI APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
        return content

    except requests.exceptions.Timeout:
        logging.error(f"âŒ AI APIè¯·æ±‚è¶…æ—¶ (è¶…æ—¶æ—¶é—´: {AI_API_TIMEOUT}ç§’)")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ AI APIè¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        logging.error(f"âŒ AI APIè°ƒç”¨å¼‚å¸¸: {e}")
        return None

# æ—§çš„parse_json_from_ai_responseå‡½æ•°å·²è¢«_parse_ai_responseæ›¿ä»£

# ================================
# ç¼“å­˜ç®¡ç†å‡½æ•° - ä»pan115-scraperå¤åˆ¶
# ================================

# ç¼“å­˜é”®ç”Ÿæˆå‡½æ•°å·²åˆ é™¤

# åˆ†ç»„ç¼“å­˜å‡½æ•°å·²åˆ é™¤

def _calculate_total_size(video_files):
    """è®¡ç®—æ–‡ä»¶æ€»å¤§å°"""
    total_bytes = 0
    for file_item in video_files:
        try:
            # å°è¯•ä»ä¸åŒå­—æ®µè·å–æ–‡ä»¶å¤§å°
            size_value = file_item.get('size', 0)
            if isinstance(size_value, (int, float)):
                total_bytes += size_value
            elif isinstance(size_value, str):
                # è§£æä¸åŒå•ä½çš„æ–‡ä»¶å¤§å°å­—ç¬¦ä¸²
                size_str = size_value
                if 'GB' in size_str:
                    total_bytes += float(size_str.replace('GB', '')) * (1024 ** 3)
                elif 'MB' in size_str:
                    total_bytes += float(size_str.replace('MB', '')) * (1024 ** 2)
                elif 'KB' in size_str:
                    total_bytes += float(size_str.replace('KB', '')) * 1024
                elif size_str.isdigit():
                    total_bytes += int(size_str)
        except (ValueError, KeyError):
            continue
    return total_bytes

def _format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    units = [(1024**4, 'TB'), (1024**3, 'GB'), (1024**2, 'MB'), (1024, 'KB')]
    for threshold, unit in units:
        if size_bytes >= threshold:
            return f"{size_bytes / threshold:.1f}{unit}"
    return f"{int(size_bytes)}B"

# ================================
# æ‰¹å¤„ç†å’Œåˆ†ç»„æ”¯æŒå‡½æ•° - ä»pan115-scraperå¤åˆ¶
# ================================

def split_files_into_batches(files, batch_size):
    """å°†æ–‡ä»¶åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡"""
    batches = []
    for i in range(0, len(files), batch_size):
        batch = files[i:i + batch_size]
        batches.append(batch)
    return batches

def process_files_for_grouping(files, source_name):
    """å¤„ç†æ–‡ä»¶è¿›è¡Œæ™ºèƒ½åˆ†ç»„ - ä¼˜åŒ–ç‰ˆ"""
    if not files:
        return []

    logging.info(f"ğŸ”„ å¤„ç† '{source_name}': {len(files)} ä¸ªæ–‡ä»¶")

    # æ‰¹æ¬¡å¤„ç†é€»è¾‘ - ä¼˜åŒ–APIè°ƒç”¨æ¬¡æ•°
    MAX_BATCH_SIZE = app_config.get('CHUNK_SIZE', 50)
    if len(files) > MAX_BATCH_SIZE:
        return _process_files_in_batches(files, MAX_BATCH_SIZE)
    else:
        return _process_single_batch(files)

def _process_files_in_batches(files, batch_size):
    """åˆ†æ‰¹å¤„ç†æ–‡ä»¶"""
    batches = split_files_into_batches(files, batch_size)
    all_groups = []

    for i, batch in enumerate(batches):
        logging.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {i+1}/{len(batches)}: {len(batch)} ä¸ªæ–‡ä»¶")
        batch_groups = _process_single_batch(batch)
        if batch_groups:
            all_groups.extend(batch_groups)

    return all_groups

def _process_single_batch(files):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶"""
    return _call_ai_for_grouping(files)

def _call_ai_for_grouping(files):
    """è°ƒç”¨AIè¿›è¡Œåˆ†ç»„å¹¶éªŒè¯ç»“æœ"""
    file_list = [{'fileId': f.get('fid', f.get('fileId', '')), 'filename': f.get('name', f.get('filename', ''))} for f in files]
    user_input = repr(file_list)

    logging.info(f"ğŸ¤– å¼€å§‹AIåˆ†ç»„åˆ†æ: {len(files)} ä¸ªæ–‡ä»¶")
    start_time = time.time()

    try:
        raw_result = extract_movie_info_from_filename_enhanced(user_input, MAGIC_PROMPT, app_config.get('GROUPING_MODEL', ''))
        process_time = time.time() - start_time

        if raw_result:
            logging.info(f"â±ï¸ AIåˆ†ç»„è€—æ—¶: {process_time:.2f}ç§’ - æˆåŠŸ")
            # éªŒè¯å’Œå¢å¼ºåˆ†ç»„ç»“æœ
            return _validate_and_enhance_groups(raw_result, files, "AIåˆ†ç»„")
        else:
            logging.warning(f"â±ï¸ AIåˆ†ç»„è€—æ—¶: {process_time:.2f}ç§’ - æ— ç»“æœ")
            return []
    except Exception as e:
        process_time = time.time() - start_time
        logging.error(f"âŒ AIåˆ†ç»„å¤±è´¥: {process_time:.2f}ç§’ - {e}")
        return []

def extract_movie_info_from_filename_enhanced(user_input, prompt, model):
    """å¢å¼ºç‰ˆç”µå½±ä¿¡æ¯æå–å‡½æ•°"""
    try:
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        full_prompt = f"{prompt}\n\n{user_input}"

        # è°ƒç”¨AI API
        response_content = call_ai_api(full_prompt, model)

        if response_content:
            logging.info(f"âœ… AIå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(response_content)} å­—ç¬¦")
            # è§£æJSONå“åº”
            parsed_result = _parse_ai_response(response_content)
            if parsed_result:
                logging.info(f"âœ… JSONè§£ææˆåŠŸ")
                return parsed_result
            else:
                logging.warning(f"âš ï¸ JSONè§£æå¤±è´¥")
                logging.error(f"å®Œæ•´AIå“åº”å†…å®¹: {response_content}")
        else:
            logging.warning(f"âš ï¸ AIå“åº”ä¸ºç©º")

        return None

    except Exception as e:
        logging.error(f"AIä¿¡æ¯æå–å¤±è´¥: {e}")
        return None

def _parse_ai_response(input_string):
    """è§£æAIå“åº” - ç®€åŒ–ç‰ˆ"""
    # å°è¯•æå–JSONä»£ç å—
    logging.info(f"å°è¯•è§£æAIå“åº”: {input_string[:200]}...")
    pattern = r'```json(.*?)```'
    match = re.search(pattern, input_string, re.DOTALL)

    if match:
        json_data = match.group(1).strip()
        try:
            return json.loads(json_data)
        except json.JSONDecodeError as e:
            logging.error(f"JSONä»£ç å—è§£æå¤±è´¥: {e}")

    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
    input_string = input_string.strip()
    if input_string.startswith(('[', '{')):
        try:
            return json.loads(input_string)
        except json.JSONDecodeError as e:
            logging.error(f"ç›´æ¥JSONè§£æå¤±è´¥: {e}")

            # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
            if input_string.startswith('{'):
                brace_count = 0
                json_end = -1

                for i, char in enumerate(input_string):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            json_end = i + 1
                            break

                if json_end > 0:
                    json_str = input_string[:json_end]
                    try:
                        logging.info(f"å°è¯•è§£ææˆªå–çš„JSONï¼Œé•¿åº¦: {len(json_str)}")
                        return json.loads(json_str)
                    except json.JSONDecodeError as e2:
                        logging.error(f"æˆªå–JSONè§£æå¤±è´¥: {e2}")

    return None

def _validate_and_enhance_groups(raw_result, files, source_name):
    """éªŒè¯å’Œå¢å¼ºåˆ†ç»„ç»“æœ"""
    if not raw_result or not isinstance(raw_result, (list, dict)):
        return []

    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–groupså­—æ®µ
    if isinstance(raw_result, dict):
        groups = raw_result.get('groups', [])
    else:
        groups = raw_result

    if not groups:
        return []

    # åˆ›å»ºæ–‡ä»¶IDåˆ°æ–‡ä»¶åçš„æ˜ å°„ï¼Œä»¥åŠæ–‡ä»¶ååˆ°æ–‡ä»¶IDçš„æ˜ å°„
    file_id_to_name = {}
    file_name_to_id = {}
    for file in files:
        file_id = file.get('fid') or file.get('fileId') or file.get('fileID')
        file_name = file.get('name') or file.get('filename') or file.get('n')
        if file_id and file_name:
            file_id_to_name[str(file_id)] = file_name
            file_name_to_id[file_name] = str(file_id)

    # è°ƒè¯•ï¼šæ‰“å°æ–‡ä»¶æ˜ å°„
    logging.info(f"ğŸ” æ–‡ä»¶æ˜ å°„è°ƒè¯• - æ€»æ–‡ä»¶æ•°: {len(files)}")
    logging.info(f"ğŸ“ æ–‡ä»¶ååˆ°IDæ˜ å°„: {list(file_name_to_id.keys())[:3]}{'...' if len(file_name_to_id) > 3 else ''}")

    enhanced_groups = []
    for group in groups:
        if isinstance(group, dict) and 'group_name' in group:
            # è°ƒè¯•ï¼šæ‰“å°åŸå§‹åˆ†ç»„æ•°æ®
            logging.info(f"ğŸ” åŸå§‹åˆ†ç»„æ•°æ®: {group}")

            # è·å–åˆ†ç»„ä¸­çš„æ–‡ä»¶IDåˆ—è¡¨
            file_ids = group.get('fileIds', [])
            if not file_ids:
                # å°è¯•ä»fileså­—æ®µä¸­æå–fileId
                files_list = group.get('files', [])
                if isinstance(files_list, list):
                    for file_item in files_list:
                        if isinstance(file_item, dict):
                            # é¦–å…ˆå°è¯•ç›´æ¥è·å–æ–‡ä»¶ID
                            file_id = file_item.get('fileId') or file_item.get('fid')
                            if file_id:
                                file_ids.append(str(file_id))
                            else:
                                # å¦‚æœæ²¡æœ‰æ–‡ä»¶IDï¼Œå°è¯•é€šè¿‡æ–‡ä»¶ååŒ¹é…
                                original_filename = file_item.get('original_filename')
                                if original_filename and original_filename in file_name_to_id:
                                    file_ids.append(file_name_to_id[original_filename])
                                    logging.info(f"ğŸ” é€šè¿‡æ–‡ä»¶ååŒ¹é…åˆ°ID: {original_filename} -> {file_name_to_id[original_filename]}")

            # è°ƒè¯•ï¼šæ‰“å°æå–çš„æ–‡ä»¶ID
            logging.info(f"ğŸ” æå–çš„æ–‡ä»¶ID: {file_ids}")

            # ç”Ÿæˆfile_namesåˆ—è¡¨
            file_names = []
            for file_id in file_ids:
                file_name = file_id_to_name.get(str(file_id))
                if file_name:
                    file_names.append(file_name)

            enhanced_group = {
                'group_name': group.get('group_name', 'æœªçŸ¥åˆ†ç»„'),
                'description': group.get('description', ''),
                'files': group.get('files', []),
                'fileIds': file_ids,
                'file_names': file_names,  # æ–°å¢ï¼šæ–‡ä»¶ååˆ—è¡¨
                'type': group.get('type', 'movie'),
                'confidence': group.get('confidence', 0.5),
                'file_count': len(file_names)  # ä½¿ç”¨å®é™…çš„æ–‡ä»¶åæ•°é‡
            }

            # è°ƒè¯•æ—¥å¿—
            logging.info(f"ğŸ” åˆ†ç»„éªŒè¯è°ƒè¯• - {enhanced_group['group_name']}: fileIds={len(file_ids)}, file_names={len(file_names)}")
            logging.info(f"ğŸ“ fileIds: {file_ids[:3]}{'...' if len(file_ids) > 3 else ''}")
            logging.info(f"ğŸ“ file_names: {file_names[:3]}{'...' if len(file_names) > 3 else ''}")

            enhanced_groups.append(enhanced_group)

    logging.info(f"âœ… {source_name}éªŒè¯å®Œæˆ: {len(enhanced_groups)} ä¸ªæœ‰æ•ˆåˆ†ç»„")
    return enhanced_groups

def merge_duplicate_named_groups(groups):
    """åˆå¹¶é‡å¤å‘½åçš„åˆ†ç»„"""
    if not groups:
        return groups

    # æŒ‰åˆ†ç»„åç§°åˆå¹¶
    merged_dict = {}
    for group in groups:
        group_name = group.get('group_name', 'æœªçŸ¥åˆ†ç»„')
        if group_name in merged_dict:
            # åˆå¹¶æ–‡ä»¶åˆ—è¡¨
            existing_files = set(merged_dict[group_name].get('files', []))
            new_files = set(group.get('files', []))
            merged_files = list(existing_files.union(new_files))

            existing_file_ids = set(merged_dict[group_name].get('fileIds', []))
            new_file_ids = set(group.get('fileIds', []))
            merged_file_ids = list(existing_file_ids.union(new_file_ids))

            # åˆå¹¶æ–‡ä»¶ååˆ—è¡¨
            existing_file_names = set(merged_dict[group_name].get('file_names', []))
            new_file_names = set(group.get('file_names', []))
            merged_file_names = list(existing_file_names.union(new_file_names))

            merged_dict[group_name]['files'] = merged_files
            merged_dict[group_name]['fileIds'] = merged_file_ids
            merged_dict[group_name]['file_names'] = merged_file_names
            merged_dict[group_name]['file_count'] = len(merged_file_names)
        else:
            merged_dict[group_name] = group.copy()

    return list(merged_dict.values())

def merge_same_series_groups(groups):
    """æ™ºèƒ½åˆå¹¶åŒç³»åˆ—åˆ†ç»„"""
    if not groups or len(groups) <= 1:
        return groups

    # ç®€åŒ–ç‰ˆæœ¬ï¼šåªè¿›è¡ŒåŸºæœ¬çš„é‡å¤åç§°åˆå¹¶
    return merge_duplicate_named_groups(groups)

# ================================
# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
# ================================

grouping_task_manager = GroupingTaskManager(
    max_queue_size=TASK_QUEUE_MAX_SIZE,
    task_timeout=TASK_TIMEOUT_SECONDS
)

# ================================
# ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤ç³»ç»Ÿ
# ================================

def start_task_manager_maintenance():
    """å¯åŠ¨ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤çº¿ç¨‹"""
    def maintenance_worker():
        while True:
            try:
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ç»´æŠ¤
                grouping_task_manager.restart_worker_if_needed()
                grouping_task_manager.cleanup_old_tasks(24)  # æ¸…ç†24å°æ—¶å‰çš„ä»»åŠ¡
                stuck_count = grouping_task_manager.force_cleanup_stuck_tasks()
                health = grouping_task_manager.get_health_status()

                if not health['worker_running']:
                    logging.warning("ğŸš¨ ä»»åŠ¡ç®¡ç†å™¨å·¥ä½œçº¿ç¨‹æœªè¿è¡Œï¼Œæ­£åœ¨é‡å¯...")
                    grouping_task_manager.restart_worker_if_needed()

                if stuck_count > 0:
                    logging.warning(f"ğŸ§¹ ç»´æŠ¤æ¸…ç†äº† {stuck_count} ä¸ªå¡ä½çš„ä»»åŠ¡")

            except Exception as e:
                logging.error(f"âŒ ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤å¼‚å¸¸: {e}")

    maintenance_thread = threading.Thread(target=maintenance_worker, daemon=True)
    maintenance_thread.start()
    logging.info("ğŸ”§ ä»»åŠ¡ç®¡ç†å™¨ç»´æŠ¤çº¿ç¨‹å·²å¯åŠ¨")

# å¯åŠ¨ç»´æŠ¤ç³»ç»Ÿ
start_task_manager_maintenance()

# å…¼å®¹æ€§ï¼šä¸ºæ—§ä»£ç æä¾›task_managerå¼•ç”¨
class LegacyTaskManager:
    """ä¸ºæ—§ä»£ç æä¾›å…¼å®¹æ€§çš„ä»»åŠ¡ç®¡ç†å™¨"""
    def __init__(self):
        self.tasks = {}
        self.lock = threading.RLock()

    def add_task(self, task):
        """æ·»åŠ ä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        with self.lock:
            self.tasks[task.task_id] = task
            logging.info(f"å…¼å®¹æ€§ä»»åŠ¡ç®¡ç†å™¨ï¼šæ·»åŠ ä»»åŠ¡ {task.task_id}")

    def get_task(self, task_id):
        """è·å–ä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self.tasks.get(task_id)

    def get_all_tasks(self):
        """è·å–æ‰€æœ‰ä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return {task_id: task.__dict__ for task_id, task in self.tasks.items()}

    def get_task_stats(self):
        """è·å–ä»»åŠ¡ç»Ÿè®¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return {
            'total_tasks': len(self.tasks),
            'pending': 0,
            'running': 0,
            'completed': 0,
            'failed': 0,
            'cancelled': 0
        }

    def cancel_task(self, task_id):
        """å–æ¶ˆä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        task = self.tasks.get(task_id)
        if task:
            task.cancelled = True
            return True
        return False

    def get_running_tasks_count(self):
        """è·å–è¿è¡Œä¸­ä»»åŠ¡æ•°é‡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return 0

    def create_task(self, cid, task_type="grouping"):
        """åˆ›å»ºä»»åŠ¡ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        task_id = f"{task_type}_{int(time.time())}_{hash(cid) % 10000}"
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä»»åŠ¡å¯¹è±¡
        class SimpleTask:
            def __init__(self, task_id, cid, task_type):
                self.task_id = task_id
                self.cid = cid
                self.task_type = task_type
                self.status = "pending"
                self.cancelled = False
                self.total_items = 0

            def start(self):
                self.status = "running"

            def to_dict(self):
                return self.__dict__

        task = SimpleTask(task_id, cid, task_type)
        self.add_task(task)
        return task

task_manager = LegacyTaskManager()


# ================================
# æ™ºèƒ½åˆ†ç»„APIç«¯ç‚¹
# ================================

@app.route('/api/grouping_task/submit', methods=['POST'])
def submit_grouping_task():
    """æäº¤æ™ºèƒ½åˆ†ç»„ä»»åŠ¡"""
    try:
        folder_id = request.form.get('folder_id')
        folder_name = request.form.get('folder_name', f'æ–‡ä»¶å¤¹_{folder_id}')

        if not folder_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘folder_idå‚æ•°'})

        # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
        task_id = grouping_task_manager.submit_task(folder_id, folder_name)

        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': f'æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤: {task_id}'
        })

    except ValueError as e:
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.error(f"æäº¤æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'æäº¤ä»»åŠ¡å¤±è´¥: {str(e)}'})


@app.route('/api/grouping_task/status/<task_id>', methods=['GET'])
def get_grouping_task_status(task_id):
    """è·å–æ™ºèƒ½åˆ†ç»„ä»»åŠ¡çŠ¶æ€"""
    try:
        task = grouping_task_manager.get_task_status(task_id)

        if task is None:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'})

        return jsonify({
            'success': True,
            'task': {
                'task_id': task.task_id,
                'folder_id': task.folder_id,
                'folder_name': task.folder_name,
                'status': task.status.value,
                'progress': task.progress,
                'created_at': task.created_at,
                'started_at': task.started_at,
                'completed_at': task.completed_at,
                'duration': task.get_duration(),
                'result': task.result,
                'error': task.error
            }
        })

    except Exception as e:
        logging.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}'})


@app.route('/api/grouping_task/cancel/<task_id>', methods=['POST'])
def cancel_grouping_task(task_id):
    """å–æ¶ˆæ™ºèƒ½åˆ†ç»„ä»»åŠ¡"""
    try:
        success = grouping_task_manager.cancel_task(task_id)

        if success:
            return jsonify({'success': True, 'message': f'ä»»åŠ¡ {task_id} å·²å–æ¶ˆ'})
        else:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ'})

    except Exception as e:
        logging.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}'})


@app.route('/api/grouping_task/queue_info', methods=['GET'])
def get_grouping_queue_info():
    """è·å–æ™ºèƒ½åˆ†ç»„ä»»åŠ¡é˜Ÿåˆ—ä¿¡æ¯"""
    try:
        with grouping_task_manager.lock:
            queue_size = grouping_task_manager.task_queue.qsize()
            active_count = len(grouping_task_manager.active_tasks)
            completed_count = len(grouping_task_manager.completed_tasks)

            return jsonify({
                'success': True,
                'queue_info': {
                    'queue_size': queue_size,
                    'active_tasks': active_count,
                    'completed_tasks': completed_count,
                    'max_queue_size': grouping_task_manager.task_queue.maxsize,
                    'is_running': grouping_task_manager.is_running
                }
            })

    except Exception as e:
        logging.error(f"è·å–é˜Ÿåˆ—ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–é˜Ÿåˆ—ä¿¡æ¯å¤±è´¥: {str(e)}'})


@app.route('/api/grouping_task/health', methods=['GET'])
def get_grouping_task_health():
    """è·å–æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        return jsonify({
            'success': True,
            'health': {
                'system_running': grouping_task_manager.is_running,
                'worker_thread_alive': grouping_task_manager.worker_thread.is_alive() if grouping_task_manager.worker_thread else False,
                'timestamp': time.time()
            }
        })

    except Exception as e:
        logging.error(f"è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'è·å–å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}'})


@app.route('/api/grouping_task/maintenance', methods=['POST'])
def grouping_task_maintenance():
    """æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç³»ç»Ÿç»´æŠ¤æ“ä½œ"""
    try:
        action = request.form.get('action')

        if action == 'restart_worker':
            # é‡å¯å·¥ä½œçº¿ç¨‹
            grouping_task_manager.is_running = False
            if grouping_task_manager.worker_thread and grouping_task_manager.worker_thread.is_alive():
                grouping_task_manager.task_queue.put(None)  # å‘é€åœæ­¢ä¿¡å·
                grouping_task_manager.worker_thread.join(timeout=5)

            grouping_task_manager._start_worker()

            return jsonify({'success': True, 'message': 'å·¥ä½œçº¿ç¨‹å·²é‡å¯'})

        elif action == 'clear_completed':
            # æ¸…ç†å·²å®Œæˆä»»åŠ¡
            with grouping_task_manager.lock:
                cleared_count = len(grouping_task_manager.completed_tasks)
                grouping_task_manager.completed_tasks.clear()

            return jsonify({'success': True, 'message': f'å·²æ¸…ç† {cleared_count} ä¸ªå·²å®Œæˆä»»åŠ¡'})

        else:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„ç»´æŠ¤æ“ä½œ'})

    except Exception as e:
        logging.error(f"ç³»ç»Ÿç»´æŠ¤æ“ä½œå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': f'ç»´æŠ¤æ“ä½œå¤±è´¥: {str(e)}'})

# ================================
# æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
# ================================

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.api_calls = {}
        self.response_times = {}
        self.error_counts = {}
        self.lock = threading.RLock()
        self.start_time = datetime.datetime.now()

        # QPSé™åˆ¶å™¨
        self.qps_limiter = {}
        self.qps_timestamps = {}

        logging.info("æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")

    def record_api_call(self, api_name, response_time, success=True):
        """è®°å½•APIè°ƒç”¨"""
        with self.lock:
            # è®°å½•è°ƒç”¨æ¬¡æ•°
            if api_name not in self.api_calls:
                self.api_calls[api_name] = {'total': 0, 'success': 0, 'error': 0}

            self.api_calls[api_name]['total'] += 1
            if success:
                self.api_calls[api_name]['success'] += 1
            else:
                self.api_calls[api_name]['error'] += 1
                self.error_counts[api_name] = self.error_counts.get(api_name, 0) + 1

            # è®°å½•å“åº”æ—¶é—´
            if api_name not in self.response_times:
                self.response_times[api_name] = []

            self.response_times[api_name].append(response_time)

            # ä¿æŒæœ€è¿‘1000æ¬¡è®°å½•
            if len(self.response_times[api_name]) > 1000:
                self.response_times[api_name] = self.response_times[api_name][-1000:]

    def check_qps_limit(self, api_name, qps_limit):
        """æ£€æŸ¥QPSé™åˆ¶"""
        with self.lock:
            current_time = time.time()

            if api_name not in self.qps_timestamps:
                self.qps_timestamps[api_name] = []

            # æ¸…ç†1ç§’å‰çš„æ—¶é—´æˆ³
            self.qps_timestamps[api_name] = [
                ts for ts in self.qps_timestamps[api_name]
                if current_time - ts < 1.0
            ]

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.qps_timestamps[api_name]) >= qps_limit:
                return False

            # è®°å½•å½“å‰æ—¶é—´æˆ³
            self.qps_timestamps[api_name].append(current_time)
            return True

    def get_api_stats(self, api_name):
        """è·å–APIç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            if api_name not in self.api_calls:
                return None

            calls = self.api_calls[api_name]
            times = self.response_times.get(api_name, [])

            stats = {
                'api_name': api_name,
                'total_calls': calls['total'],
                'success_calls': calls['success'],
                'error_calls': calls['error'],
                'success_rate': round(calls['success'] / calls['total'] * 100, 2) if calls['total'] > 0 else 0,
                'error_count': self.error_counts.get(api_name, 0)
            }

            if times:
                stats.update({
                    'avg_response_time': round(sum(times) / len(times), 3),
                    'min_response_time': round(min(times), 3),
                    'max_response_time': round(max(times), 3),
                    'recent_calls': len(times)
                })

            return stats

    def get_all_stats(self):
        """è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            all_stats = {}
            for api_name in self.api_calls.keys():
                all_stats[api_name] = self.get_api_stats(api_name)

            # æ·»åŠ ç³»ç»Ÿç»Ÿè®¡
            uptime = datetime.datetime.now() - self.start_time
            all_stats['_system'] = {
                'uptime': str(uptime),
                'total_apis': len(self.api_calls),
                }

            return all_stats

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            self.api_calls.clear()
            self.response_times.clear()
            self.error_counts.clear()
            self.qps_timestamps.clear()
            self.start_time = datetime.datetime.now()
            logging.info("æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®")

# ================================
# å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹
# ================================

performance_monitor = PerformanceMonitor()

# ================================
# QPSé™åˆ¶è£…é¥°å™¨
# ================================

def qps_limit(api_name, limit=1):
    """QPSé™åˆ¶è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # åŠ¨æ€è·å–QPSé™åˆ¶ - å¦‚æœlimitæ˜¯å­—ç¬¦ä¸²'QPS_LIMIT'ï¼Œåˆ™ä½¿ç”¨å½“å‰é…ç½®å€¼
            actual_limit = QPS_LIMIT if limit == 'QPS_LIMIT' else limit

            # æ£€æŸ¥QPSé™åˆ¶
            if not performance_monitor.check_qps_limit(api_name, actual_limit):
                time.sleep(0.5)  # ç­‰å¾…500ms
                if not performance_monitor.check_qps_limit(api_name, actual_limit):
                    raise Exception(f"API {api_name} QPSé™åˆ¶è¶…å‡º: {actual_limit}/s")

            # æ‰§è¡Œå‡½æ•°å¹¶è®°å½•æ€§èƒ½
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                response_time = time.time() - start_time
                performance_monitor.record_api_call(api_name, response_time, True)
                return result
            except Exception as e:
                response_time = time.time() - start_time
                performance_monitor.record_api_call(api_name, response_time, False)
                raise

        return wrapper
    return decorator

# ================================
# åº”ç”¨ç¨‹åºå¸¸é‡å’Œé…ç½®
# ================================

TMDB_API_URL_BASE = "https://api.themoviedb.org/3"
CONFIG_FILE = 'config.json'

# ================================
# åº”ç”¨ç¨‹åºé…ç½®ç®¡ç†
# ================================

# å…¨å±€é…ç½®å­—å…¸ï¼ŒåŒ…å«é»˜è®¤å€¼
app_config = {
    "QPS_LIMIT": 1,
    "CHUNK_SIZE": 50,
    "MAX_WORKERS": 4,
    "COOKIES": "",
    "TMDB_API_KEY": "",
    "GEMINI_API_KEY": "",
    "GEMINI_API_URL": "",
    "MODEL": "gemini-2.5-flash-lite-preview-06-17-search",
    "GROUPING_MODEL": "gemini-2.5-flash-lite-preview-06-17-search",
    "LANGUAGE": "zh-CN",
    "API_MAX_RETRIES": 3,
    "API_RETRY_DELAY": 2,
    "AI_API_TIMEOUT": 60,
    "AI_MAX_RETRIES": 3,
    "AI_RETRY_DELAY": 2,
    "TMDB_API_TIMEOUT": 60,
    "TMDB_MAX_RETRIES": 3,
    "TMDB_RETRY_DELAY": 2,
    "CLOUD_API_MAX_RETRIES": 3,
    "CLOUD_API_RETRY_DELAY": 2,
    "GROUPING_MAX_RETRIES": 1,
    "GROUPING_RETRY_DELAY": 2,
    "TASK_QUEUE_GET_TIMEOUT": 1,
    "ENABLE_QUALITY_ASSESSMENT": False,
    "ENABLE_SCRAPING_QUALITY_ASSESSMENT": True,
    "KILL_OCCUPIED_PORT_PROCESS": True,

}

# ================================
# å…¨å±€å˜é‡åˆå§‹åŒ–
# ================================

# æ—¥å¿—é˜Ÿåˆ—
log_queue = deque(maxlen=5000)

# åŸºç¡€é…ç½®å˜é‡
QPS_LIMIT = app_config["QPS_LIMIT"]
COOKIES = app_config["COOKIES"]
CHUNK_SIZE = app_config["CHUNK_SIZE"]
MAX_WORKERS = app_config["MAX_WORKERS"]
LANGUAGE = app_config["LANGUAGE"]

# APIé…ç½®å˜é‡
TMDB_API_KEY = app_config["TMDB_API_KEY"]
GEMINI_API_KEY = app_config["GEMINI_API_KEY"]
GEMINI_API_URL = app_config["GEMINI_API_URL"]
MODEL = app_config["MODEL"]
GROUPING_MODEL = app_config["GROUPING_MODEL"]

# ç®€åŒ–çš„å¸¸é‡å®šä¹‰
MAX_FILENAME_LENGTH = 255
ENABLE_INPUT_VALIDATION = True
MAX_RETRIES = 3
RETRY_DELAY = 2.0
RETRY_BACKOFF = 2.0
TIMEOUT = 60  # å¢åŠ åˆ°60ç§’ä»¥å¤„ç†å¤æ‚çš„AIæå–ä»»åŠ¡

# ç¼“å­˜åŠŸèƒ½å·²åˆ é™¤

# æ€§èƒ½ç›‘æ§é…ç½®
PERFORMANCE_MONITORING = True

# é»˜è®¤å€¼é…ç½®
DEFAULT_CID = "0"
DEFAULT_PID = "0"

# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
ALLOWED_FILE_EXTENSIONS = [
    # è§†é¢‘æ–‡ä»¶
    ".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv", ".webm", ".m4v", ".3gp",
    ".ts", ".m2ts", ".mts", ".vob", ".rmvb", ".rm", ".asf", ".divx", ".xvid",
    ".f4v", ".mpg", ".mpeg", ".m1v", ".m2v", ".dat", ".ogv", ".dv", ".mxf",
    ".gxf", ".lxf", ".wrf", ".wtv", ".dvr-ms", ".rec", ".trp", ".tp", ".m2p",
    ".ps", ".evo", ".ifo", ".bup",
    # å…‰ç›˜é•œåƒæ–‡ä»¶
    ".iso", ".img", ".nrg", ".mdf", ".cue", ".bin", ".ccd",
    # å­—å¹•æ–‡ä»¶
    ".sub", ".idx", ".srt", ".ass", ".ssa", ".vtt", ".sup", ".pgs", ".usf",
    ".xml", ".ttml", ".dfxp", ".sami", ".smi", ".rt", ".sbv", ".stl", ".ttml2",
    ".cap", ".scr", ".dks", ".lrc", ".ksc", ".pan", ".son", ".psb", ".aqt",
    ".jss", ".asc", ".txt"
]




def load_config():
    """
    ä»æ–‡ä»¶ä¸­åŠ è½½é…ç½®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤é…ç½®å¹¶åˆ›å»ºæ–‡ä»¶
    æ”¯æŒé…ç½®éªŒè¯å’Œç±»å‹è½¬æ¢
    """
    global app_config

    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                loaded_config = json.load(f)

                # éªŒè¯å’Œæ›´æ–°é…ç½®
                for key, value in loaded_config.items():
                    if key in app_config:
                        # ç±»å‹éªŒè¯
                        expected_type = type(app_config[key])
                        if isinstance(value, expected_type):
                            app_config[key] = value
                        else:
                            logging.warning(f"é…ç½®é¡¹ {key} ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ› {expected_type.__name__}ï¼Œå®é™… {type(value).__name__}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    else:
                        logging.warning(f"æœªçŸ¥é…ç½®é¡¹: {key}")

            logging.info(f"é…ç½®å·²ä» {CONFIG_FILE} åŠ è½½")
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶ {CONFIG_FILE} å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
    else:
        logging.info(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºå¹¶ä¿å­˜é»˜è®¤é…ç½®")
        save_config()

    # æ›´æ–°å…¨å±€å˜é‡
    _update_global_variables()

    # ç¼“å­˜åˆå§‹åŒ–å·²åˆ é™¤

    # é‡æ–°åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨ï¼ˆå¦‚æœé…ç½®å‘ç”Ÿå˜åŒ–ï¼‰
    _reinitialize_task_manager()

    logging.info(f"é…ç½®åŠ è½½å®Œæˆ - TMDB: {'âœ“' if TMDB_API_KEY else 'âœ—'}, Gemini: {'âœ“' if GEMINI_API_KEY else 'âœ—'}")

def _update_global_variables():
    """æ›´æ–°å…¨å±€å˜é‡"""
    global QPS_LIMIT, COOKIES, CHUNK_SIZE, MAX_WORKERS, LANGUAGE
    global TMDB_API_KEY, GEMINI_API_KEY, GEMINI_API_URL, MODEL, GROUPING_MODEL

    # åŸºç¡€é…ç½®
    QPS_LIMIT = app_config["QPS_LIMIT"]
    COOKIES = app_config["COOKIES"]
    CHUNK_SIZE = app_config["CHUNK_SIZE"]
    MAX_WORKERS = app_config["MAX_WORKERS"]
    LANGUAGE = app_config["LANGUAGE"]

    # APIé…ç½®
    TMDB_API_KEY = app_config["TMDB_API_KEY"]
    GEMINI_API_KEY = app_config["GEMINI_API_KEY"]
    GEMINI_API_URL = app_config["GEMINI_API_URL"]
    MODEL = app_config["MODEL"]
    GROUPING_MODEL = app_config["GROUPING_MODEL"]

# ç¼“å­˜åˆå§‹åŒ–å‡½æ•°å·²åˆ é™¤

def _reinitialize_task_manager():
    """é‡æ–°åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨"""
    global grouping_task_manager

    # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥é‡æ–°åˆ›å»ºgrouping_task_managerï¼Œå› ä¸ºå¯èƒ½æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
    # åªæ›´æ–°é…ç½®å‚æ•°
    # CONFIGå˜é‡åœ¨è¿™é‡Œä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    grouping_task_manager.task_timeout = 300
    logging.info(f"æ™ºèƒ½åˆ†ç»„ä»»åŠ¡ç®¡ç†å™¨é…ç½®å·²æ›´æ–°ï¼Œä»»åŠ¡è¶…æ—¶: {grouping_task_manager.task_timeout}ç§’")

def save_config():
    """å°†å½“å‰é…ç½®ä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(app_config, f, ensure_ascii=False, indent=4)
        logging.info(f"é…ç½®å·²ä¿å­˜åˆ° {CONFIG_FILE}ã€‚")
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶ {CONFIG_FILE} å¤±è´¥: {e}")
        return False


TMDB_API_URL_BASE = "https://api.themoviedb.org/3"

EXTRACTION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“ä¿¡æ¯æå–å’Œå…ƒæ•°æ®åŒ¹é…åŠ©æ‰‹ã€‚
**ç›®æ ‡ï¼š** æ ¹æ®æä¾›çš„ç”µå½±ã€ç”µè§†å‰§æˆ–ç•ªå‰§æ–‡ä»¶ååˆ—è¡¨ï¼Œæ™ºèƒ½è§£ææ¯ä¸ªæ–‡ä»¶åï¼Œå¹¶ä»åœ¨çº¿æ•°æ®åº“ä¸­åŒ¹é…å¹¶æå–è¯¦ç»†å…ƒæ•°æ®ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœæ±‡æ€»ä¸ºä¸€ä¸ªJSONæ•°ç»„ã€‚
**è¾“å…¥ï¼š** ä¸€ä¸ªåŒ…å«å¤šä¸ªç”µå½±/ç”µè§†å‰§/ç•ªå‰§æ–‡ä»¶åï¼Œæ¯è¡Œä¸€ä¸ªæ–‡ä»¶åã€‚
**è¾“å‡ºï¼š** ä¸¥æ ¼çš„JSONæ ¼å¼ç»“æœã€‚
**å¤„ç†æ­¥éª¤ï¼ˆå¯¹æ¯ä¸ªæ–‡ä»¶åé‡å¤æ‰§è¡Œï¼‰ï¼š**
1.  **æ–‡ä»¶åè§£æä¸ä¿¡æ¯æå–ï¼š**
    *   **æ ¸å¿ƒåŸåˆ™ï¼š** å°½æœ€å¤§å¯èƒ½è¯†åˆ«å¹¶ç§»é™¤æ‰€æœ‰éæ ‡é¢˜çš„æŠ€æœ¯æ€§åç¼€ã€å‰ç¼€åŠä¸­é—´æ ‡è®°ï¼Œæå–å‡ºæœ€å¯èƒ½ã€æœ€ç®€æ´çš„åŸå§‹æ ‡é¢˜éƒ¨åˆ†ã€‚
    *   **éœ€è¦ç§»é™¤çš„å¸¸è§æ ‡è®°ï¼ˆä½†ä¸é™äºï¼‰ï¼š**
        *   **åˆ†è¾¨ç‡:** 2160p, 1080p, 720p, 4K, UHD, SD
        *   **è§†é¢‘ç¼–ç :** H264, H265, HEVC, x264, x265, AVC, VP9, AV1, DivX, XviD
        *   **æ¥æº/å‹åˆ¶:** WEB-DL, BluRay, HDTV, WEBRip, BDRip, DVDRip, KORSUB, iNTERNAL, Remux, PROPER, REPACK, RETAIL, Disc, VOSTFR, DUBBED, SUBBED, FanSub, CBR, VBR, P2P
        *   **éŸ³é¢‘ç¼–ç /å£°é“:** DDP5.1, Atmos, DTS-HD MA, TrueHD, AC3, AAC, FLAC, DD+7.1, Opus, MP3, 2.0, 5.1, 7.1, Multi-Audio, Dual Audio
        *   **HDR/æœæ¯”è§†ç•Œ:** DV, HDR, HDR10, DoVi, HLG, HDR10+, WCG
        *   **ç‰ˆæœ¬ä¿¡æ¯:** Director's Cut, Extended, Uncut, Theatrical, Special Edition, Ultimate Edition, Remastered, ReCut, Criterion, IMAX, Limited Series
        *   **å‘å¸ƒç»„/ç«™ç‚¹:** [RARBG], [YTS.AM], FGT, CtrlHD, DEFLATE, xixi, EVO, GHOULS, FRDS, PANTHEON, WiKi, CHDBits, OurBits, MTeam, LoL, TRP, FWB, x264-GROUP, VCB-Studio, ANi, Lilith-Raws
        *   **å­£/é›†å·:** S01E01, S1E1, Season 1 Episode 1, Part 1, P1, Ep01, Vol.1, ç¬¬1å­£ç¬¬1é›†, SP (Special), OVA, ONA, Movie (å¯¹äºç•ªå‰§å‰§åœºç‰ˆ), NCED, NCOP (æ— å­—å¹•OP/ED)
        *   **å¹´ä»½:** (2023), [2023], .2023., _2023_
        *   **å…¶ä»–:** (R), _ , -, ., ~ , { }, [ ], ` `, + ç­‰å¸¸è§åˆ†éš”ç¬¦ï¼Œä»¥åŠå¹¿å‘Šè¯ã€å¤šä½™çš„ç©ºæ ¼ã€å¤šä½™çš„è¯­è¨€ä»£ç ï¼ˆå¦‚CHS, ENG, JPNï¼‰ç­‰ã€‚
    *   **æå–ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š**
        *   **åŸå§‹æ ‡é¢˜ (title):** æœ€å¯èƒ½ã€æœ€ç®€æ´çš„ç”µå½±/ç”µè§†å‰§/ç•ªå‰§æ ‡é¢˜ï¼Œå°½é‡ä¿ç•™åŸå§‹è¯­è¨€ï¼ˆè‹¥æ–‡ä»¶åä¸­æ˜ç¡®ï¼‰ã€‚å¦‚æœæ–‡ä»¶åæ˜¯ä¸­æ–‡ï¼Œä¼˜å…ˆæå–ä¸­æ–‡æ ‡é¢˜ã€‚
        *   **å¹´ä»½ (year):** è¯†åˆ«åˆ°çš„å‘è¡Œå¹´ä»½ã€‚
        *   **å­£å· (season):** ç”µè§†å‰§æˆ–ç•ªå‰§çš„å­£å·ï¼Œé€šå¸¸ä¸ºæ•°å­—ã€‚
        *   **é›†å· (episode):** ç”µè§†å‰§æˆ–ç•ªå‰§çš„é›†å·ï¼Œå¿…é¡»ä¸ºæ•°å­—ã€‚æ ¼å¼ï¼šS01E01â†’1, EP01â†’1, ç¬¬1é›†â†’1, E01â†’1
        *   **éƒ¨åˆ† (part):** å¦‚æœæ˜¯ç”µå½±çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆå¦‚ Part 1, Disc 2ï¼Œéç³»åˆ—ç”µå½±çš„ç»­é›†ï¼‰ï¼Œæˆ–ç•ªå‰§çš„OVA/SPç­‰ç‰¹æ®Šé›†ã€‚

2.  **åœ¨çº¿æ•°æ®åº“æœç´¢ä¸åŒ¹é…ï¼š**
    *   **æ“ä½œæŒ‡ç¤ºï¼š** **å¿…é¡»ä½¿ç”¨ä½ çš„è”ç½‘æœç´¢å·¥å…·**ã€‚
    *   **æœç´¢å…³é”®è¯æ„å»ºï¼š**
        *   ä¼˜å…ˆä½¿ç”¨è§£æå‡ºçš„ `title`ã€`year`ã€`season`ï¼ˆå¦‚æœé€‚ç”¨ï¼‰ç»„åˆæˆç²¾ç¡®çš„æœç´¢å…³é”®è¯ã€‚
        *   å¯¹äºä¸­æ–‡æ ‡é¢˜ï¼Œä½¿ç”¨å…¶ç¿»è¯‘åçš„è‹±æ–‡æ ‡é¢˜è¿›è¡Œæœç´¢ã€‚
        *   ç¤ºä¾‹æœç´¢è¯ï¼š`"Movie Title 2023 TMDB"`, `"TV Show Season 1 IMDb"`, `"Anime Name AniDB"`ã€‚
    *   **ä¼˜å…ˆé¡ºåºï¼š**
        1.  **themoviedb.org (TMDB):** é’ˆå¯¹ç”µå½±å’Œç”µè§†å‰§çš„é¦–é€‰ã€‚
        2.  **AniDB:** é’ˆå¯¹åŠ¨ç”»ã€åŠ¨æ¼«ï¼ˆç•ªå‰§ï¼‰çš„é¦–é€‰ã€‚
        3.  **IMDb:** ä½œä¸ºTMDBçš„è¡¥å……æˆ–å›é€€ã€‚
        4.  **è±†ç“£ç”µå½± (Douban):** å¯¹äºä¸­æ–‡å†…å®¹æœ‰é¢å¤–å‚è€ƒä»·å€¼ï¼Œä½†åŒ¹é…åä»éœ€è½¬æ¢ä¸ºè‹±æ–‡æ ‡é¢˜ã€‚
        5.  **çƒ‚ç•ªèŒ„ (Rotten Tomatoes):** ä½œä¸ºè¯„åˆ†å’Œè¡¥å……ä¿¡æ¯æ¥æºã€‚
    *   **åŒ¹é…ç­–ç•¥ï¼š**
        *   ä½¿ç”¨æå–å‡ºçš„æ ‡é¢˜ã€å¹´ä»½ã€å­£å·ï¼ˆå¦‚æœé€‚ç”¨ï¼‰è¿›è¡Œç²¾å‡†æœç´¢ã€‚
        *   **é«˜ç½®ä¿¡åº¦åŒ¹é…ï¼š** åªæœ‰å½“æœç´¢ç»“æœä¸è§£æå‡ºçš„æ ‡é¢˜é«˜åº¦ç›¸ä¼¼ï¼ˆè€ƒè™‘å¤§å°å†™ã€æ ‡ç‚¹ç¬¦å·ã€å¸¸è§ç¼©å†™ç­‰ï¼‰ï¼Œå¹´ä»½ç²¾ç¡®åŒ¹é…ï¼Œä¸”åª’ä½“ç±»å‹ï¼ˆç”µå½±/ç”µè§†å‰§/ç•ªå‰§ï¼‰ä¸€è‡´æ—¶ï¼Œæ‰è®¤å®šä¸ºå‡†ç¡®åŒ¹é…ã€‚
        *   **å”¯ä¸€æ€§åŸåˆ™ï¼š** å¦‚æœæœç´¢ç»“æœåŒ…å«å¤šä¸ªæ¡ç›®ï¼Œé€‰æ‹©ä¸æ–‡ä»¶åä¿¡æ¯ï¼ˆç‰¹åˆ«æ˜¯å¹´ä»½ã€ç‰ˆæœ¬ã€å­£é›†å·ï¼‰æœ€åŒ¹é…çš„**å”¯ä¸€**æ¡ç›®ã€‚
        *   **æ¨¡ç³ŠåŒ¹é…å›é€€ï¼š** å¦‚æœç²¾å‡†åŒ¹é…å¤±è´¥ï¼Œå¯ä»¥å°è¯•è¿›è¡Œè½»å¾®çš„æ¨¡ç³ŠåŒ¹é…ï¼ˆä¾‹å¦‚ç§»é™¤å‰¯æ ‡é¢˜ã€å°è¯•å¸¸è§ç¼©å†™ï¼‰ï¼Œä½†éœ€é™ä½ç½®ä¿¡åº¦ã€‚
        *   **æ— æ³•åŒ¹é…çš„å¤„ç†ï¼š** å¦‚æœæ— æ³•æ‰¾åˆ°é«˜ç½®ä¿¡åº¦çš„åŒ¹é…é¡¹ï¼Œåˆ™è¯¥æ¡ç›®çš„å…ƒæ•°æ®å­—æ®µåº”ä¸ºç©ºæˆ– nullã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
*   è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œä¸”åªåŒ…å«JSONå†…å®¹ï¼Œä¸é™„å¸¦ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–é¢å¤–æ–‡æœ¬ã€‚
*   æ ¹å…ƒç´ å¿…é¡»æ˜¯ä¸€ä¸ªJSONæ•°ç»„ `[]`ã€‚
*   æ•°ç»„çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ªæ–‡ä»¶åçš„è§£æç»“æœã€‚
*   JSONç»“æ„å¦‚ä¸‹ï¼š
    ```json
    [
      {
        "file_name": "string",
        "title": "string",            // åŸå§‹è‹±æ–‡æ ‡é¢˜
        "original_title": "string",   // åª’ä½“çš„åŸå§‹è¯­è¨€æ ‡é¢˜ (å¦‚æ—¥æ–‡, ä¸­æ–‡ç­‰)
        "year": "string",             // å‘è¡Œå¹´ä»½
        "media_type": "string",       // "movie", "tv_show", "anime"
        "tmdb_id": "string",          // TMDB ID
        "imdb_id": "string",          // IMDb ID
        "anidb_id": "string",         // AniDB ID (å¦‚æœé€‚ç”¨)
        "douban_id": "string",        // è±†ç“£ ID (å¦‚æœé€‚ç”¨)
        "season": "number | null",    // æ–‡ä»¶åè§£æå‡ºçš„å­£å·
        "episode": "number | null"    // æ–‡ä»¶åè§£æå‡ºçš„é›†å·
      }
    ]
    ```
*   **å­—æ®µè¯´æ˜ï¼š**
    *   `file_name`: åŸå§‹æ–‡ä»¶åã€‚
    *   `title`: æå–å‡ºçš„åª’ä½“åŸå§‹è‹±æ–‡æ ‡é¢˜ã€‚
    *   `original_title`: åª’ä½“åœ¨åŸäº§åœ°çš„åŸå§‹è¯­è¨€æ ‡é¢˜ (ä¾‹å¦‚ï¼Œæ—¥å‰§çš„æ—¥æ–‡æ ‡é¢˜ï¼ŒéŸ©å‰§çš„éŸ©æ–‡æ ‡é¢˜)ã€‚å¦‚æœæ•°æ®åº“æœªæä¾›æˆ–ä¸ `title` ç›¸åŒï¼Œåˆ™ä½¿ç”¨ `title` çš„å€¼ã€‚
    *   `year`: ç”µå½±/ç”µè§†å‰§/ç•ªå‰§çš„å‘è¡Œå¹´ä»½ã€‚
    *   `media_type`: è¯†åˆ«å‡ºçš„åª’ä½“ç±»å‹ï¼Œåªèƒ½æ˜¯ `"movie"`, `"tv_show"`, `"anime"` ä¹‹ä¸€ã€‚
    *   `tmdb_id`, `imdb_id`, `anidb_id`, `douban_id`: å¯¹åº”æ•°æ®åº“çš„å”¯ä¸€IDã€‚å¦‚æœæœªæ‰¾åˆ°æˆ–ä¸é€‚ç”¨ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸² `""`ã€‚
*   **å€¼çº¦å®šï¼š**
    *   å­—ç¬¦ä¸²å­—æ®µï¼ˆ`title`, `original_title`, `year`, `media_type`, `tmdb_id` ç­‰ï¼‰å¦‚æœä¿¡æ¯ç¼ºå¤±æˆ–æ— æ³•å‡†ç¡®è¯†åˆ«ï¼Œè¯·ä½¿ç”¨ç©ºå­—ç¬¦ä¸² `""`ã€‚
    *   æ•°å­—å­—æ®µï¼ˆ`season`, `episode`ï¼‰å¦‚æœä¿¡æ¯ç¼ºå¤±æˆ–ä¸é€‚ç”¨ï¼Œè¯·ä½¿ç”¨ `null`ã€‚
    *   **ä¸¥æ ¼æ€§è¦æ±‚ï¼š** ä»»ä½•æ—¶å€™éƒ½ä¸è¦åœ¨JSONè¾“å‡ºä¸­åŒ…å«é¢å¤–çš„æ–‡æœ¬ã€è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚
"""

# ================================
# æ™ºèƒ½åˆ†ç»„AIæç¤ºè¯æ¨¡æ¿
# ================================

MAGIC_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“æ–‡ä»¶æ™ºèƒ½åˆ†ç»„åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€ç»„åª’ä½“æ–‡ä»¶ï¼Œå¹¶æ ¹æ®å®ƒä»¬çš„å†…å®¹ç‰¹å¾è¿›è¡Œæ™ºèƒ½åˆ†ç»„å’Œå‘½åã€‚

**è¾“å…¥ï¼š** ä¸€ä¸ªåŒ…å«å¤šä¸ªåª’ä½“æ–‡ä»¶ä¿¡æ¯çš„JSONæ•°ç»„ï¼Œæ¯ä¸ªæ–‡ä»¶åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- filename: åŸå§‹æ–‡ä»¶å
- title: æ ‡å‡†åŒ–æ ‡é¢˜
- year: å‘è¡Œå¹´ä»½
- season: å­£å·ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
- episode: é›†å·ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
- media_type: åª’ä½“ç±»å‹ï¼ˆmovie/tv/animeï¼‰
- tmdb_info: TMDBæ•°æ®åº“ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰

**ç›®æ ‡ï¼š**
1. åˆ†ææ–‡ä»¶ä¹‹é—´çš„å…³è”æ€§ï¼ˆåŒä¸€éƒ¨ä½œå“ã€åŒä¸€å­£ã€åŒä¸€ç³»åˆ—ç­‰ï¼‰
2. åˆ›å»ºåˆç†çš„åˆ†ç»„ç»“æ„
3. ä¸ºæ¯ä¸ªåˆ†ç»„ç”Ÿæˆæ ‡å‡†åŒ–çš„æ–‡ä»¶å¤¹åç§°
4. ç¡®ä¿åˆ†ç»„é€»è¾‘æ¸…æ™°ã€å‘½åè§„èŒƒ

**åˆ†ç»„è§„åˆ™ï¼š**
1. **ç”µè§†å‰§/åŠ¨ç”»åˆ†ç»„ï¼š**
   - åŒä¸€éƒ¨ä½œå“çš„ä¸åŒé›†æ•°åº”å½’ä¸ºä¸€ç»„
   - åŒä¸€éƒ¨ä½œå“çš„ä¸åŒå­£åº”åˆ†åˆ«åˆ†ç»„
   - ç‰¹æ®Šé›†ï¼ˆOVAã€SPã€ç”µå½±ç‰ˆï¼‰å¯å•ç‹¬åˆ†ç»„æˆ–å½’å…¥ä¸»ç³»åˆ—

2. **ç”µå½±åˆ†ç»„ï¼š**
   - ç‹¬ç«‹ç”µå½±é€šå¸¸å•ç‹¬åˆ†ç»„
   - ç³»åˆ—ç”µå½±ï¼ˆå¦‚ä¸‰éƒ¨æ›²ï¼‰å¯å½’ä¸ºä¸€ç»„
   - åŒä¸€å¯¼æ¼”æˆ–ä¸»é¢˜çš„ç”µå½±å¯è€ƒè™‘åˆ†ç»„

3. **å‘½åè§„èŒƒï¼š**
   - ä½¿ç”¨æ ‡å‡†åŒ–çš„ä¸­è‹±æ–‡æ ‡é¢˜
   - åŒ…å«å¹´ä»½ä¿¡æ¯ï¼ˆå‘è¡Œå¹´ä»½æˆ–å¹´ä»½èŒƒå›´ï¼‰
   - å¯¹äºç”µè§†å‰§ï¼ŒåŒ…å«å­£å·ä¿¡æ¯
   - é¿å…ç‰¹æ®Šå­—ç¬¦å’Œè¿‡é•¿çš„åç§°
   - æ ¼å¼ç¤ºä¾‹ï¼š
     * "æƒåŠ›çš„æ¸¸æˆ Game of Thrones S01 (2011)"
     * "å¤ä»‡è€…è”ç›Ÿç³»åˆ— Avengers Collection (2012-2019)"
     * "ä½ çš„åå­— Your Name (2016)"

**è¾“å‡ºæ ¼å¼ï¼š**
è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹ç»“æ„ï¼š
```json
{
  "groups": [
    {
      "group_id": "unique_group_identifier",
      "group_name": "æ ‡å‡†åŒ–çš„åˆ†ç»„åç§°",
      "group_type": "tv_season|movie_series|single_movie|anime_series",
      "description": "åˆ†ç»„æè¿°",
      "files": [
        {
          "original_filename": "åŸå§‹æ–‡ä»¶å",
          "suggested_filename": "å»ºè®®çš„æ–°æ–‡ä»¶å",
          "file_info": "æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯å¯¹è±¡"
        }
      ],
      "metadata": {
        "total_episodes": "æ€»é›†æ•°ï¼ˆå¦‚æœé€‚ç”¨ï¼‰",
        "year_range": "å¹´ä»½èŒƒå›´",
        "genre": "ç±»å‹",
        "rating": "è¯„åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰"
      }
    }
  ],
  "summary": {
    "total_groups": "æ€»åˆ†ç»„æ•°",
    "total_files": "æ€»æ–‡ä»¶æ•°",
    "grouping_confidence": "åˆ†ç»„ç½®ä¿¡åº¦ï¼ˆ0-100ï¼‰",
    "recommendations": ["åˆ†ç»„å»ºè®®å’Œæ³¨æ„äº‹é¡¹"]
  }
}
```

**è´¨é‡è¦æ±‚ï¼š**
- åˆ†ç»„é€»è¾‘å¿…é¡»åˆç†ä¸”ä¸€è‡´
- å‘½åå¿…é¡»è§„èŒƒä¸”æ˜“äºç†è§£
- é¿å…åˆ›å»ºè¿‡å¤šçš„å°åˆ†ç»„
- ç¡®ä¿æ¯ä¸ªæ–‡ä»¶éƒ½è¢«æ­£ç¡®å½’ç±»
- æä¾›æ¸…æ™°çš„åˆ†ç»„è¯´æ˜å’Œå»ºè®®

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–ä»£ç å—æ ‡è®°ã€‚
"""

GROUP_MERGE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“æ–‡ä»¶åˆ†ç»„ä¼˜åŒ–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç°æœ‰çš„æ–‡ä»¶åˆ†ç»„ï¼Œè¯†åˆ«å¯ä»¥åˆå¹¶çš„ç›¸å…³åˆ†ç»„ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚

**è¾“å…¥ï¼š** ä¸€ä¸ªåŒ…å«å¤šä¸ªåˆ†ç»„çš„JSONå¯¹è±¡ï¼Œæ¯ä¸ªåˆ†ç»„åŒ…å«ï¼š
- group_id: åˆ†ç»„å”¯ä¸€æ ‡è¯†
- group_name: åˆ†ç»„åç§°
- group_type: åˆ†ç»„ç±»å‹
- files: æ–‡ä»¶åˆ—è¡¨
- metadata: åˆ†ç»„å…ƒæ•°æ®

**ç›®æ ‡ï¼š**
1. è¯†åˆ«ç›¸å…³è”çš„åˆ†ç»„ï¼ˆåŒä¸€ç³»åˆ—ã€åŒä¸€ä½œè€…ã€ç›¸å…³ä¸»é¢˜ç­‰ï¼‰
2. åˆ†æåˆ†ç»„çš„åˆç†æ€§å’Œä¸€è‡´æ€§
3. æä¾›åˆå¹¶å»ºè®®å’Œä¼˜åŒ–æ–¹æ¡ˆ
4. ç”Ÿæˆæœ€ç»ˆçš„ä¼˜åŒ–åˆ†ç»„ç»“æ„

**åˆå¹¶è§„åˆ™ï¼š**
1. **ç³»åˆ—ä½œå“åˆå¹¶ï¼š**
   - åŒä¸€éƒ¨ä½œå“çš„ä¸åŒå­£å¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ªå¤§åˆ†ç»„
   - ç³»åˆ—ç”µå½±å¯ä»¥åˆå¹¶ä¸ºç”µå½±é›†åˆ
   - ç›¸å…³çš„OVAã€ç‰¹åˆ«ç¯‡å¯ä»¥åˆå¹¶åˆ°ä¸»ç³»åˆ—

2. **ä¸»é¢˜ç›¸å…³åˆå¹¶ï¼š**
   - åŒä¸€å¯¼æ¼”çš„ä½œå“
   - åŒä¸€åˆ¶ä½œå…¬å¸çš„ä½œå“
   - ç›¸åŒç±»å‹æˆ–ä¸»é¢˜çš„ä½œå“

3. **è´¨é‡ä¼˜åŒ–ï¼š**
   - é¿å…è¿‡åº¦ç»†åˆ†
   - ç¡®ä¿åˆ†ç»„å¤§å°åˆç†ï¼ˆä¸è¦å¤ªå¤§ä¹Ÿä¸è¦å¤ªå°ï¼‰
   - ä¿æŒå‘½åä¸€è‡´æ€§

**è¾“å‡ºæ ¼å¼ï¼š**
è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«ï¼š
```json
{
  "optimized_groups": [
    {
      "group_id": "ä¼˜åŒ–åçš„åˆ†ç»„ID",
      "group_name": "ä¼˜åŒ–åçš„åˆ†ç»„åç§°",
      "group_type": "åˆ†ç»„ç±»å‹",
      "merged_from": ["åŸå§‹åˆ†ç»„IDåˆ—è¡¨"],
      "files": ["åˆå¹¶åçš„æ–‡ä»¶åˆ—è¡¨"],
      "metadata": "åˆå¹¶åçš„å…ƒæ•°æ®",
      "merge_reason": "åˆå¹¶åŸå› è¯´æ˜"
    }
  ],
  "merge_summary": {
    "original_groups": "åŸå§‹åˆ†ç»„æ•°",
    "optimized_groups": "ä¼˜åŒ–ååˆ†ç»„æ•°",
    "merge_operations": "åˆå¹¶æ“ä½œæ•°",
    "optimization_score": "ä¼˜åŒ–è¯„åˆ†ï¼ˆ0-100ï¼‰",
    "recommendations": ["ä¼˜åŒ–å»ºè®®"]
  }
}
```

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–ä»£ç å—æ ‡è®°ã€‚
"""

# ================================
# è´¨é‡è¯„ä¼°æç¤ºè¯æ¨¡æ¿
# ================================

QUALITY_ASSESSMENT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åª’ä½“æ–‡ä»¶è´¨é‡è¯„ä¼°åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°æ–‡ä»¶åçš„è´¨é‡ã€åˆ†ç»„çš„åˆç†æ€§ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

**è¯„ä¼°ç»´åº¦ï¼š**

1. **æ–‡ä»¶åè´¨é‡ï¼ˆ0-100åˆ†ï¼‰ï¼š**
   - ä¿¡æ¯å®Œæ•´æ€§ï¼šæ˜¯å¦åŒ…å«å¿…è¦çš„æ ‡é¢˜ã€å¹´ä»½ã€å­£é›†ä¿¡æ¯
   - å‘½åè§„èŒƒæ€§ï¼šæ˜¯å¦ç¬¦åˆæ ‡å‡†å‘½åè§„èŒƒ
   - å¯è¯»æ€§ï¼šæ˜¯å¦æ˜“äºç†è§£å’Œè¯†åˆ«
   - ä¸€è‡´æ€§ï¼šåŒä¸€åˆ†ç»„å†…å‘½åæ˜¯å¦ä¸€è‡´

2. **åˆ†ç»„åˆç†æ€§ï¼ˆ0-100åˆ†ï¼‰ï¼š**
   - é€»è¾‘æ€§ï¼šåˆ†ç»„é€»è¾‘æ˜¯å¦æ¸…æ™°åˆç†
   - å®Œæ•´æ€§ï¼šç›¸å…³æ–‡ä»¶æ˜¯å¦éƒ½è¢«æ­£ç¡®åˆ†ç»„
   - å¹³è¡¡æ€§ï¼šåˆ†ç»„å¤§å°æ˜¯å¦åˆç†
   - å®ç”¨æ€§ï¼šåˆ†ç»„æ˜¯å¦ä¾¿äºç®¡ç†å’ŒæŸ¥æ‰¾

3. **å…ƒæ•°æ®å‡†ç¡®æ€§ï¼ˆ0-100åˆ†ï¼‰ï¼š**
   - åŒ¹é…åº¦ï¼šä¸åœ¨çº¿æ•°æ®åº“çš„åŒ¹é…å‡†ç¡®æ€§
   - å®Œæ•´æ€§ï¼šå…ƒæ•°æ®ä¿¡æ¯æ˜¯å¦å®Œæ•´
   - ä¸€è‡´æ€§ï¼šåŒä¸€ä½œå“çš„å…ƒæ•°æ®æ˜¯å¦ä¸€è‡´

**è¾“å‡ºæ ¼å¼ï¼š**
```json
{
  "overall_score": "æ€»ä½“è´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰",
  "detailed_scores": {
    "filename_quality": "æ–‡ä»¶åè´¨é‡è¯„åˆ†",
    "grouping_rationality": "åˆ†ç»„åˆç†æ€§è¯„åˆ†",
    "metadata_accuracy": "å…ƒæ•°æ®å‡†ç¡®æ€§è¯„åˆ†"
  },
  "issues": [
    {
      "type": "é—®é¢˜ç±»å‹",
      "severity": "ä¸¥é‡ç¨‹åº¦ï¼ˆhigh/medium/lowï¼‰",
      "description": "é—®é¢˜æè¿°",
      "affected_files": ["å—å½±å“çš„æ–‡ä»¶"],
      "suggestion": "æ”¹è¿›å»ºè®®"
    }
  ],
  "recommendations": [
    {
      "priority": "ä¼˜å…ˆçº§ï¼ˆhigh/medium/lowï¼‰",
      "action": "å»ºè®®æ“ä½œ",
      "description": "è¯¦ç»†è¯´æ˜",
      "expected_improvement": "é¢„æœŸæ”¹è¿›æ•ˆæœ"
    }
  ],
  "summary": {
    "total_files": "æ€»æ–‡ä»¶æ•°",
    "problematic_files": "æœ‰é—®é¢˜çš„æ–‡ä»¶æ•°",
    "improvement_potential": "æ”¹è¿›æ½œåŠ›è¯„ä¼°",
    "next_steps": ["ä¸‹ä¸€æ­¥å»ºè®®"]
  }
}
```

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–ä»£ç å—æ ‡è®°ã€‚
"""

# ================================
# å·¥å…·å‡½æ•°å’Œè¾…åŠ©æ–¹æ³•
# ================================

def safe_filename(filename):
    """
    å®‰å…¨åŒ–æ–‡ä»¶åï¼Œç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦

    Args:
        filename (str): åŸå§‹æ–‡ä»¶å

    Returns:
        str: å®‰å…¨åŒ–åçš„æ–‡ä»¶å
    """
    if not filename:
        return "untitled"

    # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
    unsafe_chars = '<>:"/\\|?*'
    for char in unsafe_chars:
        filename = filename.replace(char, '_')

    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    filename = ''.join(char for char in filename if ord(char) >= 32)

    # é™åˆ¶é•¿åº¦
    if len(filename) > MAX_FILENAME_LENGTH:
        name, ext = os.path.splitext(filename)
        max_name_length = MAX_FILENAME_LENGTH - len(ext)
        filename = name[:max_name_length] + ext

    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    filename = filename.strip(' .')

    return filename or "untitled"

def format_file_size(size_bytes):
    """
    æ ¼å¼åŒ–æ–‡ä»¶å¤§å°ä¸ºäººç±»å¯è¯»çš„æ ¼å¼

    Args:
        size_bytes (int): æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰

    Returns:
        str: æ ¼å¼åŒ–åçš„æ–‡ä»¶å¤§å°
    """
    if size_bytes == 0:
        return "0 B"

    size_names = ["B", "KB", "MB", "GB", "TB", "PB"]
    i = int(math.floor(math.log(size_bytes, 1024)))
    p = math.pow(1024, i)
    s = round(size_bytes / p, 2)

    return f"{s} {size_names[i]}"

def validate_input(input_data, input_type="string", max_length=None, allowed_chars=None):
    """
    è¾“å…¥éªŒè¯å‡½æ•°

    Args:
        input_data: è¾“å…¥æ•°æ®
        input_type (str): æ•°æ®ç±»å‹ (string, int, float, list, dict)
        max_length (int): æœ€å¤§é•¿åº¦é™åˆ¶
        allowed_chars (str): å…è®¸çš„å­—ç¬¦é›†

    Returns:
        bool: éªŒè¯æ˜¯å¦é€šè¿‡
    """
    if not ENABLE_INPUT_VALIDATION:
        return True

    try:
        # ç±»å‹éªŒè¯
        if input_type == "string" and not isinstance(input_data, str):
            return False
        elif input_type == "int" and not isinstance(input_data, int):
            return False
        elif input_type == "float" and not isinstance(input_data, (int, float)):
            return False
        elif input_type == "list" and not isinstance(input_data, list):
            return False
        elif input_type == "dict" and not isinstance(input_data, dict):
            return False

        # é•¿åº¦éªŒè¯
        if max_length and hasattr(input_data, '__len__') and len(input_data) > max_length:
            return False

        # å­—ç¬¦éªŒè¯
        if allowed_chars and isinstance(input_data, str):
            if not all(char in allowed_chars for char in input_data):
                return False

        return True
    except Exception as e:
        logging.error(f"è¾“å…¥éªŒè¯é”™è¯¯: {e}")
        return False

# ç¼“å­˜é”®ç”Ÿæˆå‡½æ•°å·²åˆ é™¤

# ç¼“å­˜APIè°ƒç”¨å‡½æ•°å·²åˆ é™¤

def retry_on_failure(max_retries=None, delay=None, backoff=None):
    """
    é‡è¯•è£…é¥°å™¨

    Args:
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
        delay (float): é‡è¯•å»¶è¿Ÿ
        backoff (float): é€€é¿å€æ•°

    Returns:
        è£…é¥°å™¨å‡½æ•°
    """
    if max_retries is None:
        max_retries = MAX_RETRIES
    if delay is None:
        delay = RETRY_DELAY
    if backoff is None:
        backoff = RETRY_BACKOFF

    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        logging.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}, {current_delay}ç§’åé‡è¯•")
                        time.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logging.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥")

            raise last_exception

        return wrapper
    return decorator

def log_performance(func_name, start_time, success=True, **kwargs):
    """
    è®°å½•æ€§èƒ½æ—¥å¿—

    Args:
        func_name (str): å‡½æ•°å
        start_time (float): å¼€å§‹æ—¶é—´
        success (bool): æ˜¯å¦æˆåŠŸ
        **kwargs: é¢å¤–çš„æ—¥å¿—ä¿¡æ¯
    """
    if PERFORMANCE_MONITORING:
        duration = time.time() - start_time
        performance_monitor.record_api_call(func_name, duration, success)

        log_data = {
            'function': func_name,
            'duration': round(duration, 3),
            'success': success,
            'timestamp': datetime.datetime.now().isoformat()
        }
        log_data.update(kwargs)

        # ç¼“å­˜ç›¸å…³æ—¥å¿—å·²åˆ é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨INFOçº§åˆ«
        logging.info(f"æ€§èƒ½æ—¥å¿—: {json.dumps(log_data, ensure_ascii=False)}")

# ================================
# å¢å¼ºçš„APIå‡½æ•°
# ================================

@qps_limit("extract_movie_info", 'QPS_LIMIT')
@retry_on_failure()
def extract_movie_info_from_filename(filenames):
    """
    ä»æ–‡ä»¶åæå–ç”µå½±ä¿¡æ¯ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

    Args:
        filenames (list): æ–‡ä»¶ååˆ—è¡¨

    Returns:
        list: æå–çš„ç”µå½±ä¿¡æ¯åˆ—è¡¨
    """
    start_time = time.time()

    try:
        # è¾“å…¥éªŒè¯
        if not validate_input(filenames, "list", max_length=100):
            raise ValueError("æ— æ•ˆçš„æ–‡ä»¶ååˆ—è¡¨")

        # ç¼“å­˜é€»è¾‘å·²åˆ é™¤

        # å‡†å¤‡APIè¯·æ±‚
        headers = {
            "Authorization": f"Bearer {GEMINI_API_KEY}",
            "Content-Type": "application/json",
        }

        user_input_content = "\n".join(filenames)
        payload = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": EXTRACTION_PROMPT},
                {"role": "user", "content": user_input_content}
            ],
        }

        # å‘é€APIè¯·æ±‚
        logging.info(f"æ­£åœ¨æå– {len(filenames)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯...")
        response = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=TIMEOUT)
        response.raise_for_status()

        data = response.json()
        input_string = data["choices"][0]["message"]["content"]

        # è§£æJSONå“åº”
        pattern = r'```json(.*?)```'
        match = re.search(pattern, input_string, re.DOTALL)

        if match:
            json_data = match.group(1).strip()
            logging.debug(f"æå–çš„JSONå­—ç¬¦ä¸²: {json_data}")
            json_data = json.loads(json_data)

            # ç¡®ä¿è¿”å›åˆ—è¡¨æ ¼å¼
            if not isinstance(json_data, list):
                json_data = [json_data] if json_data else []

            # ç¼“å­˜å­˜å‚¨å·²åˆ é™¤

            log_performance("extract_movie_info_from_filename", start_time, True,
                          files_processed=len(filenames), results_count=len(json_data))

            logging.info(f"æˆåŠŸæå– {len(json_data)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")
            return json_data
        else:
            logging.error(f"Gemini API å“åº”ä¸­æœªæ‰¾åˆ° JSON å—ã€‚å®Œæ•´å“åº”: {input_string}")
            log_performance("extract_movie_info_from_filename", start_time, False, error="no_json_block")
            return None

    except Exception as e:
        log_performance("extract_movie_info_from_filename", start_time, False, error=str(e))
        logging.error(f"æå–ç”µå½±ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        raise


@retry_on_failure()
def search_movie_in_tmdb(movie_info):
    """
    åœ¨TMDBä¸­æœç´¢ç”µå½±ä¿¡æ¯ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

    Args:
        movie_info (dict): ç”µå½±ä¿¡æ¯å­—å…¸

    Returns:
        dict: TMDBæœç´¢ç»“æœ
    """
    start_time = time.time()

    try:
        # è¾“å…¥éªŒè¯
        if not validate_input(movie_info, "dict"):
            raise ValueError("æ— æ•ˆçš„ç”µå½±ä¿¡æ¯")

        original_title = movie_info.get('original_title', '')
        title = movie_info.get('title', '')
        media_type = movie_info.get('media_type', 'movie')
        tmdb_id = movie_info.get('tmdb_id', '')
        year = movie_info.get('year', '')

        # TMDBæœç´¢ç¼“å­˜é€»è¾‘å·²åˆ é™¤

        logging.debug(f"æ­£åœ¨TMDBä¸­æœç´¢: {original_title} ({year})")

        # æ¸…ç†æ ‡é¢˜
        original_title = str(re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5]', ' ', original_title))
        min_year = 3 if media_type == 'movie' else 5

        def perform_tmdb_search(query, media_type_search, language):
            """æ‰§è¡ŒTMDBæœç´¢"""
            # ä½¿ç”¨QPSé™åˆ¶å™¨æ§åˆ¶è¯·æ±‚é¢‘ç‡
            qps_limiter.acquire()

            if media_type_search == 'movie':
                url = f"{TMDB_API_URL_BASE}/search/movie"
            elif media_type_search == 'tv':
                url = f"{TMDB_API_URL_BASE}/search/tv"
            else:
                return []

            params = {
                "api_key": TMDB_API_KEY,
                "query": query,
                "language": language,
            }

            response = requests.get(url, params=params, timeout=TIMEOUT)
            response.raise_for_status()
            return response.json().get('results', [])

        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨åŸå§‹æ ‡é¢˜æœç´¢
        search_results = perform_tmdb_search(original_title, 'tv' if media_type != 'movie' else 'movie', LANGUAGE)
        logging.debug(f"TMDBæœç´¢ç»“æœ '{original_title}': {len(search_results)} ä¸ªç»“æœ")

        # ç²¾ç¡®åŒ¹é…ï¼šå¹´ä»½å’ŒTMDB ID
        for result in search_results:
            try:
                data = result.get('release_date') or result.get('first_air_date')
                if data and data[:4] == year:
                    logging.info(f"ç²¾ç¡®å¹´ä»½åŒ¹é…: {result}")
                    # ç¼“å­˜å­˜å‚¨å·²åˆ é™¤
                    log_performance("search_movie_in_tmdb", start_time, True, match_type="exact_year")
                    return result
                elif tmdb_id and tmdb_id == str(result['id']):
                    logging.info(f"TMDB IDåŒ¹é…: {result}")
                    # ç¼“å­˜å­˜å‚¨å·²åˆ é™¤
                    log_performance("search_movie_in_tmdb", start_time, True, match_type="tmdb_id")
                    return result
            except (KeyError, IndexError) as e:
                logging.warning(f"å¤„ç†TMDBç»“æœæ—¶å‡ºé”™: {e}")
                continue

        # æ¨¡ç³ŠåŒ¹é…ï¼šå¹´ä»½å·®å¼‚åœ¨å…è®¸èŒƒå›´å†…
        for result in search_results:
            try:
                data = result.get('release_date') or result.get('first_air_date')
                if data and year:
                    year_diff = abs(int(data[:4]) - int(year))
                    result_title = result.get('title', result.get('name', ''))
                    logging.debug(f"å¹´ä»½å·®å¼‚ '{result_title}': {year_diff} ({data[:4]} vs {year})")

                    if year_diff <= min_year or title == result_title:
                        logging.info(f"æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {result}")
                        log_performance("search_movie_in_tmdb", start_time, True, match_type="fuzzy_year")
                        return result
            except (KeyError, IndexError, ValueError) as e:
                logging.warning(f"å¤„ç†TMDBç»“æœæ—¶å‡ºé”™: {e}")
                continue

        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜æœç´¢
        if title and title != original_title:
            cleaned_title = str(re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5]', ' ', title))
            logging.debug(f"ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜æœç´¢: '{cleaned_title}'")

            cleaned_search_results = perform_tmdb_search(cleaned_title, 'tv' if media_type != 'movie' else 'movie', LANGUAGE)
            logging.debug(f"æ¸…ç†æ ‡é¢˜æœç´¢ç»“æœ '{cleaned_title}': {len(cleaned_search_results)} ä¸ªç»“æœ")

            # å¯¹æ¸…ç†åçš„ç»“æœè¿›è¡Œç›¸åŒçš„åŒ¹é…é€»è¾‘
            for result in cleaned_search_results:
                try:
                    data = result.get('release_date') or result.get('first_air_date')
                    if data and data[:4] == year:
                        logging.info(f"æ¸…ç†æ ‡é¢˜ç²¾ç¡®åŒ¹é…: {result}")
                        log_performance("search_movie_in_tmdb", start_time, True, match_type="cleaned_exact")
                        return result
                    elif tmdb_id and tmdb_id == str(result['id']):
                        logging.info(f"æ¸…ç†æ ‡é¢˜TMDB IDåŒ¹é…: {result}")
                        log_performance("search_movie_in_tmdb", start_time, True, match_type="cleaned_tmdb_id")
                        return result
                except (KeyError, IndexError) as e:
                    logging.warning(f"å¤„ç†æ¸…ç†æ ‡é¢˜TMDBç»“æœæ—¶å‡ºé”™: {e}")
                    continue

            for result in cleaned_search_results:
                try:
                    data = result.get('release_date') or result.get('first_air_date')
                    if data and year:
                        year_diff = abs(int(data[:4]) - int(year))
                        result_title = result.get('title', result.get('name', ''))

                        if year_diff <= min_year or title == result_title:
                            logging.info(f"æ¸…ç†æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…: {result}")
                            log_performance("search_movie_in_tmdb", start_time, True, match_type="cleaned_fuzzy")
                            return result
                except (KeyError, IndexError, ValueError) as e:
                    logging.warning(f"å¤„ç†æ¸…ç†æ ‡é¢˜TMDBç»“æœæ—¶å‡ºé”™: {e}")
                    continue

        # æœªæ‰¾åˆ°åŒ¹é…ç»“æœ
        logging.info(f"æœªåœ¨TMDBä¸­æ‰¾åˆ°åŒ¹é…ç»“æœ: {original_title} ({year})")
        log_performance("search_movie_in_tmdb", start_time, True, match_type="no_match")
        return None

    except Exception as e:
        log_performance("search_movie_in_tmdb", start_time, False, error=str(e))
        logging.error(f"TMDBæœç´¢å‡ºé”™: {e}", exc_info=True)
        raise


class QueueHandler(logging.Handler):
    def emit(self, record):
        log_entry = self.format(record)
        log_queue.append(log_entry)

# é…ç½® Flask åº”ç”¨çš„æ—¥å¿—
# ç§»é™¤é»˜è®¤çš„basicConfigï¼Œæ‰‹åŠ¨æ·»åŠ å¤„ç†å™¨
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG) # è®¾ç½®æ ¹æ—¥å¿—çº§åˆ«

# æ¸…é™¤æ‰€æœ‰ç°æœ‰çš„å¤„ç†å™¨ï¼Œé¿å…é‡å¤æ—¥å¿—
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

# æ·»åŠ æ–‡ä»¶å¤„ç†å™¨ (ä½¿ç”¨ RotatingFileHandler)
# maxBytes: 1MB, backupCount: 5
file_handler = RotatingFileHandler('rename_log.log', maxBytes=1024 * 1024, backupCount=5)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
root_logger.addHandler(file_handler)

    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨ï¼ˆWindowså…¼å®¹æ€§ï¼šè®¾ç½®é”™è¯¯å¤„ç†ï¼‰
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
# åœ¨Windowsç³»ç»Ÿä¸­ï¼Œæ§åˆ¶å°å¯èƒ½ä¸æ”¯æŒæŸäº›Unicodeå­—ç¬¦ï¼Œè®¾ç½®é”™è¯¯å¤„ç†ç­–ç•¥
if hasattr(console_handler.stream, 'reconfigure'):
    try:
        console_handler.stream.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass  # å¦‚æœé‡é…ç½®å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤è®¾ç½®
root_logger.addHandler(console_handler)

# æ·»åŠ è‡ªå®šä¹‰é˜Ÿåˆ—å¤„ç†å™¨
queue_handler = QueueHandler()
queue_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
root_logger.addHandler(queue_handler)

# ç¡®ä¿ Flask è‡ªå·±çš„æ—¥å¿—ä¹Ÿé€šè¿‡ root_logger å¤„ç†
app.logger.addHandler(queue_handler) # Flask app.logger é»˜è®¤ä¼šç»§æ‰¿ root_logger çš„å¤„ç†å™¨

# ç¦ç”¨ Werkzeug çš„è®¿é—®æ—¥å¿—
logging.getLogger('werkzeug').setLevel(logging.ERROR)

# åª’ä½“æ–‡ä»¶ç±»å‹
MEDIATYPES = [
    "wma", "wav", "mp3", "aac", "ra", "ram", "mp2", "ogg", "aif",
    "mpega", "amr", "mid", "midi", "m4a", "m4v", "wmv", "rmvb",
    "mpeg4", "mpeg2", "flv", "avi", "3gp", "mpga", "qt", "rm",
    "wmz", "wmd", "wvx", "wmx", "wm", "swf", "mpg", "mp4", "mkv",
    "mpeg", "mov", "mdf", "asf",
    "webm", "ogv", "m2ts", "ts", "vob", "divx", "xvid", "f4v", "m2v",
    "amv", "drc", "evo", "flc", "h264", "h265", "icod", "mod", "tod",
    "mts", "ogm", "qtff", "rec", "vp6", "vp7", "vp8", "vp9", "iso"
]

# æ”¯æŒçš„åª’ä½“æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆä¸pan115-scraperå…¼å®¹ï¼‰
SUPPORTED_MEDIA_TYPES = [
    "wma", "wav", "mp3", "aac", "ra", "ram", "mp2", "ogg", "aif",
    "mpega", "amr", "mid", "midi", "m4a", "m4v", "wmv", "rmvb",
    "mpeg4", "mpeg2", "flv", "avi", "3gp", "mpga", "qt", "rm",
    "wmz", "wmd", "wvx", "wmx", "wm", "swf", "mpg", "mp4", "mkv",
    "mpeg", "mov", "mdf", "asf", "webm", "ogv", "m2ts", "ts", "vob",
    "divx", "xvid", "f4v", "m2v", "amv", "drc", "evo", "flc",
    "h264", "h265", "icod", "mod", "tod", "mts", "ogm", "qtff",
    "rec", "vp6", "vp7", "vp8", "vp9", "iso"
]

# 115 ç½‘ç›˜è¯·æ±‚å¤´å’Œ Cookies
headers = {'Content-Type': 'application/x-www-form-urlencoded','User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',}
cookies = {}



def load_cookies():
    global cookies
    # ä» app_config ä¸­è·å– COOKIES
    cookies_str = app_config.get("COOKIES")
    if cookies_str:
        cookies = {}
        for part in cookies_str.split('; '):
            if '=' in part:
                key, value = part.split('=', 1)
                cookies[key] = value
            else:
                cookies[part] = ''
        logging.info(f"Cookies loaded successfully from app_config. Parsed cookies: {cookies}")
    else:
        cookies = {} # ç¡®ä¿åœ¨ COOKIES ä¸ºç©ºæ—¶ï¼Œcookies å­—å…¸ä¹Ÿè¢«æ¸…ç©º
        logging.warning("COOKIES é…ç½®é¡¹ä¸ºç©ºã€‚è¯·åœ¨ç½‘é¡µé…ç½®ä¸­è®¾ç½®ã€‚")

# åœ¨åº”ç”¨å¯åŠ¨æ—¶åŠ è½½é…ç½®å’Œ Cookies
load_config()
load_cookies()

# QPS é™åˆ¶å™¨
class QPSLimiter:
    def __init__(self, qps_limit):
        self.qps_limit = float(qps_limit) # Convert to float
        self.interval = 1.0 / self.qps_limit
        self.last_request_time = 0
        self.lock = threading.Lock()

    def acquire(self):
        with self.lock:
            current_time = time.time()
            elapsed_time = current_time - self.last_request_time
            if elapsed_time < self.interval:
                time.sleep(self.interval - elapsed_time)
            self.last_request_time = time.time()

qps_limiter = QPSLimiter(qps_limit=QPS_LIMIT)

# ================================
# 115äº‘ç›˜APIå¢å¼ºå‡½æ•°
# ================================

@qps_limit("batch_rename", QPS_LIMIT)
@retry_on_failure()
def batch_rename_file(namedict):
    """
    æ‰¹é‡é‡å‘½åæ–‡ä»¶ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

    Args:
        namedict (dict): é‡å‘½åå­—å…¸ {file_id: new_name}

    Returns:
        dict: æ“ä½œç»“æœ
    """
    start_time = time.time()

    try:
        # è¾“å…¥éªŒè¯
        if not validate_input(namedict, "dict", max_length=100):
            raise ValueError("æ— æ•ˆçš„é‡å‘½åå­—å…¸")

        logging.info(f"å¼€å§‹æ‰¹é‡é‡å‘½å {len(namedict)} ä¸ªæ–‡ä»¶")

        # æ„å»ºè¯·æ±‚æ•°æ®
        newnames = {}
        for key, value in namedict.items():
            safe_name = safe_filename(value)
            newnames[f'files_new_name[{key}]'] = safe_name

        data = parse.urlencode(newnames)
        url = 'https://webapi.115.com/files/batch_rename'

        # å‘é€è¯·æ±‚
        response = requests.post(url, headers=headers, cookies=cookies, data=data, timeout=TIMEOUT)
        response.raise_for_status()
        result = response.json()

        # è¯¦ç»†æ‰“å°è¿”å›å€¼
        logging.info(f"ğŸ” é‡å‘½åAPIè¿”å›å€¼: {json.dumps(result, ensure_ascii=False, indent=2)}")

        if result.get('state'):
            log_performance("batch_rename_file", start_time, True, files_renamed=len(namedict))
            logging.info(f"âœ… æ‰¹é‡é‡å‘½åæˆåŠŸ: {len(namedict)} ä¸ªæ–‡ä»¶")

            # æ‰“å°æˆåŠŸçš„è¯¦ç»†ä¿¡æ¯
            if 'data' in result:
                logging.info(f"ğŸ“‹ é‡å‘½åè¯¦æƒ…: {json.dumps(result['data'], ensure_ascii=False, indent=2)}")

            # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜æ¸…ç†æœºåˆ¶

            return {'success': True, 'result': result, 'message': f'æˆåŠŸé‡å‘½å {len(namedict)} ä¸ªæ–‡ä»¶'}
        else:
            log_performance("batch_rename_file", start_time, False, error=result.get('error', 'Unknown'))
            error_msg = result.get('error') or result.get('errno_desc') or result.get('errcode') or 'Unknown error'
            logging.error(f"âŒ æ‰¹é‡é‡å‘½åå¤±è´¥: {error_msg}")
            logging.error(f"ğŸ” å¤±è´¥è¯¦æƒ…: {json.dumps(result, ensure_ascii=False, indent=2)}")
            return {'success': False, 'error': error_msg, 'result': result}

    except requests.exceptions.HTTPError as e:
        log_performance("batch_rename_file", start_time, False, error=f"HTTP_{e.response.status_code}")
        logging.error(f"HTTPé”™è¯¯ - æ‰¹é‡é‡å‘½å: {e.response.status_code} - {e.response.text}")
        return {'success': False, 'error': str(e), 'status_code': e.response.status_code, 'content': e.response.text}
    except Exception as e:
        log_performance("batch_rename_file", start_time, False, error=str(e))
        logging.error(f"æ‰¹é‡é‡å‘½åå‡ºé”™: {e}", exc_info=True)
        raise

@qps_limit("batch_move", QPS_LIMIT)
@retry_on_failure()
def batch_move_file(cids, pid):
    """
    æ‰¹é‡ç§»åŠ¨æ–‡ä»¶ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

    Args:
        cids: æ–‡ä»¶IDåˆ—è¡¨æˆ–å•ä¸ªæ–‡ä»¶ID
        pid: ç›®æ ‡çˆ¶ç›®å½•ID

    Returns:
        dict: æ“ä½œç»“æœ
    """
    start_time = time.time()

    try:
        # è¾“å…¥éªŒè¯
        if isinstance(cids, str):
            file_count = 1
        elif isinstance(cids, list):
            file_count = len(cids)
            if not validate_input(cids, "list", max_length=100):
                raise ValueError("æ— æ•ˆçš„æ–‡ä»¶IDåˆ—è¡¨")
        else:
            raise ValueError("cidså¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")

        if not validate_input(str(pid), "string"):
            raise ValueError("æ— æ•ˆçš„ç›®æ ‡ç›®å½•ID")

        logging.info(f"å¼€å§‹æ‰¹é‡ç§»åŠ¨ {file_count} ä¸ªæ–‡ä»¶åˆ°ç›®å½• {pid}")

        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {'move_proid': '1749745456990_-69_0', 'pid': pid}
        if isinstance(cids, str):
            data['fid[0]'] = cids
        else:
            for i in range(len(cids)):
                data[f'fid[{i}]'] = cids[i]

        # å‘é€è¯·æ±‚
        response = requests.post('https://webapi.115.com/files/move',
                               headers=headers, cookies=cookies, data=data, timeout=TIMEOUT)
        response.raise_for_status()

        # å¤„ç†å“åº”
        result = response.json() if 'application/json' in response.headers.get('Content-Type', '') else response.text

        log_performance("batch_move_file", start_time, True, files_moved=file_count)
        logging.info(f"æ‰¹é‡ç§»åŠ¨æˆåŠŸ: {file_count} ä¸ªæ–‡ä»¶ç§»åŠ¨åˆ° {pid}")

        # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜æ¸…ç†æœºåˆ¶

        return {'success': True, 'result': result}

    except requests.exceptions.HTTPError as e:
        log_performance("batch_move_file", start_time, False, error=f"HTTP_{e.response.status_code}")
        logging.error(f"HTTPé”™è¯¯ - æ‰¹é‡ç§»åŠ¨: {e.response.status_code} - {e.response.text}")
        return {'success': False, 'error': str(e), 'status_code': e.response.status_code, 'content': e.response.text}
    except Exception as e:
        log_performance("batch_move_file", start_time, False, error=str(e))
        logging.error(f"æ‰¹é‡ç§»åŠ¨å‡ºé”™: {e}", exc_info=True)
        raise



@retry_on_failure()
def extract_movie_name_and_info(chunk):
    """
    æå–ç”µå½±åç§°å’Œä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒç¼“å­˜å’Œæ€§èƒ½ç›‘æ§

    Args:
        chunk (list): æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨

    Returns:
        list: åˆ®å‰Šç»“æœåˆ—è¡¨
    """
    start_time = time.time()
    results = []

    try:
        # è¾“å…¥éªŒè¯
        if not validate_input(chunk, "list", max_length=50):
            raise ValueError("æ— æ•ˆçš„æ–‡ä»¶åˆ—è¡¨")

        names = [f.get("file_name") for f in chunk if f.get("file_name")]
        if not names:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å")
            return results

        logging.info(f"å¼€å§‹æå– {len(names)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")

        # ç”Ÿæˆç¼“å­˜é”®

        # å°è¯•ä»ç¼“å­˜è·å– - åˆ®å‰Šç¼“å­˜å·²ç¦ç”¨

        # æå–ç”µå½±ä¿¡æ¯
        movie_info_list = extract_movie_info_from_filename(names)
        if not movie_info_list:
            logging.warning("æ²¡æœ‰ä»æ–‡ä»¶åä¸­æå–åˆ°ç”µå½±ä¿¡æ¯")
            # ä¸ºæ‰€æœ‰æ–‡ä»¶åˆ›å»ºæå–å¤±è´¥çš„ç»“æœ
            for fids in chunk:
                original_name = fids.get('n', '')
                results.append({
                    'fid': fids.get('fid'),
                    'original_name': original_name,
                    'suggested_name': '',
                    'size': fids.get('s'),
                    'tmdb_info': None,
                    'file_info': None,
                    'status': 'extraction_failed',
                    'error': 'æ— æ³•ä»æ–‡ä»¶åæå–ç”µå½±ä¿¡æ¯'
                })
            log_performance("extract_movie_name_and_info", start_time, True, extracted_count=0)
            return results

        logging.info(f"æå–åˆ° {len(movie_info_list)} ä¸ªç”µå½±ä¿¡æ¯")

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘TMDBæœç´¢
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(movie_info_list))) as executor:
            future_to_file_info = {
                executor.submit(search_movie_in_tmdb, file_info): (fids, file_info)
                for fids, file_info in zip(chunk, movie_info_list)
            }

            for future in as_completed(future_to_file_info):
                fids, file_info = future_to_file_info[future]
                original_name = fids.get('n', '')
                suggested_name = ""

                try:
                    tmdb_result = future.result()
                    media_type = file_info.get('media_type', 'movie')
                    _, ext = os.path.splitext(original_name)

                    if tmdb_result:
                        if media_type == 'movie':
                            title = tmdb_result.get('title', '')
                            release_date = tmdb_result.get('release_date', '')
                            tmdb_id = tmdb_result.get('id', '')
                            size = fids.get('s', '')

                            if title and release_date:
                                year = release_date[:4]
                                suggested_name = safe_filename(f"{title} ({year}) {{tmdb-{tmdb_id}}} {size}{ext}")
                        else:
                            # TV/åŠ¨ç”»å¤„ç†
                            name = tmdb_result.get('name', '')
                            first_air_date = tmdb_result.get('first_air_date', '')
                            tmdb_id = tmdb_result.get('id', '')
                            size = fids.get('s', '')

                            season = int(file_info.get('season', 1) or 1)
                            episode = file_info.get('episode')

                            # ç®€åŒ–è°ƒè¯•æ—¥å¿—
                            if episode is None:
                                # åå¤‡é€»è¾‘ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸ºçº¯æ•°å­—ï¼ˆå¦‚ 1.mp4, 5.mp4ï¼‰
                                base_name = os.path.splitext(original_name)[0]  # å»æ‰æ‰©å±•å
                                if base_name.isdigit():
                                    episode = int(base_name)
                                    logging.info(f"ğŸ”§ ä½¿ç”¨æ•°å­—æ–‡ä»¶åä½œä¸ºé›†å· - æ–‡ä»¶: {original_name} -> ç¬¬{episode}é›†")
                                else:
                                    logging.warning(f"âš ï¸ TVå‰§é›†å·æå–å¤±è´¥ - æ–‡ä»¶: {original_name}")
                                    logging.debug(f"  æå–ç»“æœ: name={name}, episode={episode}, season={season}")
                            else:
                                logging.debug(f"âœ… TVå‰§ä¿¡æ¯æå–æˆåŠŸ - {original_name} -> S{season:02d}E{episode:02d}")

                            if name and first_air_date and episode is not None:
                                year = first_air_date[:4]
                                episode = int(episode or 1)
                                suggested_name = safe_filename(f"{name} ({year}) S{season:02d}E{episode:02d} {{tmdb-{tmdb_id}}} {size}{ext}")

                        if suggested_name:
                            logging.info(f"æˆåŠŸåŒ¹é…: {original_name} -> {suggested_name}")
                            results.append({
                                'fid': fids.get('fid'),
                                'original_name': original_name,
                                'suggested_name': suggested_name,
                                'size': fids.get('s'),
                                'tmdb_info': tmdb_result,
                                'file_info': file_info,
                                'status': 'success'
                            })
                        else:
                            logging.warning(f"æ— æ³•ç”Ÿæˆå»ºè®®åç§°: {original_name}")
                            results.append({
                                'fid': fids.get('fid'),
                                'original_name': original_name,
                                'suggested_name': '',
                                'size': fids.get('s'),
                                'tmdb_info': tmdb_result,
                                'file_info': file_info,
                                'status': 'no_suggestion',
                                'error': 'æ— æ³•ç”Ÿæˆå»ºè®®åç§°'
                            })
                    else:
                        logging.warning(f"æœªæ‰¾åˆ°TMDBç»“æœ: {original_name}")
                        results.append({
                            'fid': fids.get('fid'),
                            'original_name': original_name,
                            'suggested_name': '',
                            'size': fids.get('s'),
                            'tmdb_info': None,
                            'file_info': file_info,
                            'status': 'no_tmdb_result',
                            'error': 'æœªæ‰¾åˆ°TMDBåŒ¹é…ç»“æœ'
                        })

                except Exception as exc:
                    logging.error(f"å¤„ç†æ–‡ä»¶ {original_name} æ—¶å‡ºé”™: {exc}")
                    results.append({
                        'fid': fids.get('fid'),
                        'original_name': original_name,
                        'suggested_name': '',
                        'size': fids.get('s'),
                        'tmdb_info': None,
                        'file_info': file_info,
                        'status': 'error',
                        'error': str(exc)
                    })

        # ç¼“å­˜ç»“æœ - åˆ®å‰Šç¼“å­˜å·²ç¦ç”¨

        log_performance("extract_movie_name_and_info", start_time, True,
                       processed_files=len(chunk), successful_matches=len([r for r in results if r.get('status') == 'success']))

        logging.info(f"å®Œæˆä¿¡æ¯æå–: {len(results)} ä¸ªç»“æœ")
        return results

    except Exception as e:
        log_performance("extract_movie_name_and_info", start_time, False, error=str(e))
        logging.error(f"æå–ç”µå½±ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        raise



@retry_on_failure()
def _get_single_level_content_from_115(cid, offset, limit):
    """
    è·å–115ç½‘ç›˜æŒ‡å®šç›®å½•ä¸‹çš„å•å±‚æ–‡ä»¶å’Œç›®å½•åˆ—è¡¨ - å¢å¼ºç‰ˆæœ¬

    Args:
        cid (str): ç›®å½•ID
        offset (int): åç§»é‡
        limit (int): é™åˆ¶æ•°é‡

    Returns:
        dict: APIå“åº”æ•°æ®
    """
    start_time = time.time()

    try:
        # QPSé™åˆ¶æ§åˆ¶
        qps_limiter.acquire()

        # è¾“å…¥éªŒè¯
        if not validate_input(str(cid), "string"):
            raise ValueError("æ— æ•ˆçš„ç›®å½•ID")
        if not validate_input(offset, "int") or offset < 0:
            raise ValueError("æ— æ•ˆçš„åç§»é‡")
        if not validate_input(limit, "int") or limit <= 0 or limit > 1000:
            raise ValueError("æ— æ•ˆçš„é™åˆ¶æ•°é‡")

        # ç”Ÿæˆç¼“å­˜é”®

        # å°è¯•ä»ç¼“å­˜è·å–

        logging.debug(f"è·å–115ç›®å½•å†…å®¹: CID={cid}, offset={offset}, limit={limit}")

        # æ„å»ºURL
        url1 = f'https://webapi.115.com/files?aid=1&cid={cid}&o=user_ptime&asc=0&offset={offset}&show_dir=1&limit={limit}&code=&scid=&snap=0&natsort=1&record_open_time=1&count_folders=1&type=&source=&format=json'
        url2 = f'https://aps.115.com/natsort/files.php?aid=1&cid={cid}&o=file_name&asc=1&offset={offset}&show_dir=1&limit={limit}&code=&scid=&snap=0&natsort=1&record_open_time=1&count_folders=1&type=&source=&format=json&fc_mix=0'

        # é¦–å…ˆå°è¯•URL2ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰
        logging.debug(f"å°è¯•URL2: {url2}")
        response = requests.get(url2, headers=headers, cookies=cookies, timeout=TIMEOUT)
        response.raise_for_status()
        response_data = response.json()

        # å¦‚æœURL2æ²¡æœ‰è¿”å›æ•°æ®ï¼Œå°è¯•URL1
        if not response_data.get("data", []):
            logging.debug(f"URL2æ— æ•°æ®ï¼Œå°è¯•URL1: {url1}")
            response = requests.get(url1, headers=headers, cookies=cookies, timeout=TIMEOUT)
            response.raise_for_status()
            response_data = response.json()

        # ç¼“å­˜ç»“æœ

        log_performance("_get_single_level_content_from_115", start_time, True,
                       items_count=len(response_data.get("data", [])))

        return response_data

    except requests.exceptions.HTTPError as e:
        log_performance("_get_single_level_content_from_115", start_time, False,
                       error=f"HTTP_{e.response.status_code}")
        logging.error(f"HTTPé”™è¯¯ - è·å–115ç›®å½•å†…å®¹ CID {cid}: {e.response.status_code} - {e.response.text}")
        return {"success": False, "error": str(e), "status_code": e.response.status_code, "content": e.response.text}
    except Exception as e:
        log_performance("_get_single_level_content_from_115", start_time, False, error=str(e))
        logging.error(f"è·å–115ç›®å½•å†…å®¹å‡ºé”™ CID {cid}: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


@qps_limit("get_file_list", QPS_LIMIT)
@retry_on_failure()
def get_all_files_from_115_for_scraping(cid, offset, limit, all_files):
    """
    é€’å½’è·å–115ç½‘ç›˜æŒ‡å®šç›®å½•ä¸‹çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨ - ä¸“ç”¨äºåˆ®å‰Šé¢„è§ˆ
    åªæ”¶é›†è§†é¢‘æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰éŸ³é¢‘ã€å›¾ç‰‡ã€å­—å¹•ç­‰éè§†é¢‘æ–‡ä»¶

    Args:
        cid (str): ç›®å½•ID
        offset (int): åç§»é‡
        limit (int): é™åˆ¶æ•°é‡
        all_files (list): æ–‡ä»¶åˆ—è¡¨ï¼ˆå¼•ç”¨ä¼ é€’ï¼‰
    """
    start_time = time.time()

    try:
        logging.debug(f"ğŸ” è·å–115ç›®å½•å†…å®¹ç”¨äºåˆ®å‰Š: CID={cid}, offset={offset}, limit={limit}")

        # è·å–ç›®å½•å†…å®¹
        response_data = _get_single_level_content_from_115(cid, offset, limit)

        if not response_data or response_data.get("success") is False:
            logging.warning(f"è·å–ç›®å½•å†…å®¹å¤±è´¥: CID={cid}")
            return

        total_count = response_data.get("count", 0)
        total_pages = math.ceil(total_count / limit) if limit > 0 else 0

        # è·å–è·¯å¾„ä¿¡æ¯
        file_path_parts = [a['name'] for a in response_data.get("path", [])[1:]]
        current_path_prefix = "/".join(file_path_parts)

        files_processed = 0
        dirs_processed = 0

        # å¤„ç†å½“å‰é¡µçš„æ•°æ®
        for data in response_data.get("data", []):
            if 'fid' in data:  # æ–‡ä»¶ - åªæ”¶é›†è§†é¢‘æ–‡ä»¶
                filename = data.get("n", "")

                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œåªå¤„ç†è§†é¢‘æ–‡ä»¶
                _, ext = os.path.splitext(filename)
                ext_lower = ext.lower()[1:]  # å»æ‰ç‚¹å·å¹¶è½¬å°å†™

                # åªå¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰éŸ³é¢‘ã€å›¾ç‰‡ã€å­—å¹•ç­‰æ–‡ä»¶
                video_extensions = {
                    "mp4", "mkv", "avi", "mov", "wmv", "flv", "webm", "m4v", "3gp",
                    "ts", "m2ts", "mts", "vob", "rmvb", "rm", "asf", "divx", "xvid",
                    "f4v", "mpg", "mpeg", "m1v", "m2v", "dat", "ogv", "dv", "mxf",
                    "gxf", "lxf", "wrf", "wtv", "dvr-ms", "rec", "trp", "tp", "m2p",
                    "ps", "evo", "ifo", "bup", "iso"
                }

                if ext_lower not in video_extensions:
                    logging.debug(f"â­ï¸ è·³è¿‡éè§†é¢‘æ–‡ä»¶: {filename} (æ‰©å±•å: {ext_lower})")
                    continue

                # è½¬æ¢æ–‡ä»¶å¤§å°
                size_value = data.get('s', 0)
                if isinstance(size_value, str) and size_value.endswith('GB'):
                    # å·²ç»æ˜¯GBæ ¼å¼ï¼Œä¿æŒä¸å˜
                    pass
                else:
                    # æ˜¯å­—èŠ‚æ•°ï¼Œéœ€è¦è½¬æ¢
                    try:
                        bytes_value = int(size_value)
                        gb_in_bytes = 1024 ** 3
                        gb_value = bytes_value / gb_in_bytes
                        data['s'] = f"{gb_value:.1f}GB"
                    except (ValueError, TypeError):
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                        pass

                # æ„å»ºå®Œæ•´è·¯å¾„
                if current_path_prefix:
                    data["file_name"] = os.path.join(current_path_prefix, filename)
                else:
                    data["file_name"] = filename

                all_files.append(data)
                files_processed += 1
                logging.debug(f"ğŸ“„ æ·»åŠ è§†é¢‘æ–‡ä»¶: {filename}")

            elif 'cid' in data:  # ç›®å½•
                # é€’å½’å¤„ç†å­ç›®å½•ï¼Œæ·»åŠ å»¶è¿Ÿä»¥éµå®ˆQPSé™åˆ¶
                time.sleep(1.0 / QPS_LIMIT + 0.1)
                get_all_files_from_115_for_scraping(data['cid'], 0, limit, all_files)
                dirs_processed += 1

        # å¤„ç†åˆ†é¡µï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        if offset == 0 and total_pages > 1:
            logging.debug(f"å¤„ç†åˆ†é¡µ: æ€»é¡µæ•°={total_pages}")
            for page in range(1, total_pages):
                next_offset = page * limit

                # è·å–ä¸‹ä¸€é¡µæ•°æ®
                next_response_data = _get_single_level_content_from_115(cid, next_offset, limit)

                if not next_response_data or next_response_data.get("success") is False:
                    logging.warning(f"è·å–åˆ†é¡µæ•°æ®å¤±è´¥: CID={cid}, page={page}")
                    continue

                # å¤„ç†åˆ†é¡µæ•°æ®
                for data in next_response_data.get("data", []):
                    if 'fid' in data:  # æ–‡ä»¶ - åªæ”¶é›†è§†é¢‘æ–‡ä»¶
                        filename = data.get("n", "")

                        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œåªå¤„ç†è§†é¢‘æ–‡ä»¶
                        _, ext = os.path.splitext(filename)
                        ext_lower = ext.lower()[1:]  # å»æ‰ç‚¹å·å¹¶è½¬å°å†™

                        # åªå¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰éŸ³é¢‘ã€å›¾ç‰‡ã€å­—å¹•ç­‰æ–‡ä»¶
                        video_extensions = {
                            "mp4", "mkv", "avi", "mov", "wmv", "flv", "webm", "m4v", "3gp",
                            "ts", "m2ts", "mts", "vob", "rmvb", "rm", "asf", "divx", "xvid",
                            "f4v", "mpg", "mpeg", "m1v", "m2v", "dat", "ogv", "dv", "mxf",
                            "gxf", "lxf", "wrf", "wtv", "dvr-ms", "rec", "trp", "tp", "m2p",
                            "ps", "evo", "ifo", "bup", "iso"
                        }

                        if ext_lower not in video_extensions:
                            logging.debug(f"â­ï¸ è·³è¿‡éè§†é¢‘æ–‡ä»¶: {filename} (æ‰©å±•å: {ext_lower})")
                            continue

                        # è½¬æ¢æ–‡ä»¶å¤§å°
                        size_value = data.get('s', 0)
                        if isinstance(size_value, str) and size_value.endswith('GB'):
                            pass
                        else:
                            try:
                                bytes_value = int(size_value)
                                gb_in_bytes = 1024 ** 3
                                gb_value = bytes_value / gb_in_bytes
                                data['s'] = f"{gb_value:.1f}GB"
                            except (ValueError, TypeError):
                                pass

                        if current_path_prefix:
                            data["file_name"] = os.path.join(current_path_prefix, filename)
                        else:
                            data["file_name"] = filename

                        all_files.append(data)
                        files_processed += 1
                        logging.debug(f"ğŸ“„ æ·»åŠ è§†é¢‘æ–‡ä»¶(åˆ†é¡µ): {filename}")

                    elif 'cid' in data:
                        # é€’å½’å¤„ç†å­ç›®å½•ï¼Œæ·»åŠ å»¶è¿Ÿä»¥éµå®ˆQPSé™åˆ¶
                        time.sleep(1.0 / QPS_LIMIT + 0.1)
                        get_all_files_from_115_for_scraping(data['cid'], 0, limit, all_files)
                        dirs_processed += 1

        log_performance("get_all_files_from_115_for_scraping", start_time, True,
                       files_processed=files_processed, dirs_processed=dirs_processed)

        logging.debug(f"å®Œæˆåˆ®å‰Šæ–‡ä»¶åˆ—è¡¨è·å–: CID={cid}, æ–‡ä»¶={files_processed}, ç›®å½•={dirs_processed}")

    except Exception as e:
        log_performance("get_all_files_from_115_for_scraping", start_time, False, error=str(e))
        logging.error(f"è·å–åˆ®å‰Šæ–‡ä»¶åˆ—è¡¨å‡ºé”™ CID {cid}: {e}", exc_info=True)


def get_file_list_from_115(cid, offset, limit, all_files):
    """
    é€’å½’è·å–115ç½‘ç›˜æŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨ - å¢å¼ºç‰ˆæœ¬

    Args:
        cid (str): ç›®å½•ID
        offset (int): åç§»é‡
        limit (int): é™åˆ¶æ•°é‡
        all_files (list): æ–‡ä»¶åˆ—è¡¨ï¼ˆå¼•ç”¨ä¼ é€’ï¼‰
    """
    start_time = time.time()

    try:
        # è¾“å…¥éªŒè¯
        if not validate_input(str(cid), "string"):
            raise ValueError("æ— æ•ˆçš„ç›®å½•ID")
        if not validate_input(offset, "int") or offset < 0:
            raise ValueError("æ— æ•ˆçš„åç§»é‡")
        if not validate_input(limit, "int") or limit <= 0:
            raise ValueError("æ— æ•ˆçš„é™åˆ¶æ•°é‡")

        logging.debug(f"é€’å½’è·å–æ–‡ä»¶åˆ—è¡¨: CID={cid}, offset={offset}, limit={limit}")

        # ä½¿ç”¨å•å±‚å†…å®¹è·å–å‡½æ•°
        response_data = _get_single_level_content_from_115(cid, offset, limit)

        if not response_data or response_data.get("success") is False:
            logging.warning(f"è·å–ç›®å½•å†…å®¹å¤±è´¥: CID={cid}")
            return

        total_count = response_data.get("count", 0)
        total_pages = math.ceil(total_count / limit) if limit > 0 else 0

        # æ„å»ºè·¯å¾„å‰ç¼€
        file_path_parts = [a.get('name', '') for a in response_data.get("path", [])[1:]]
        current_path_prefix = "/".join(file_path_parts) if file_path_parts else ""

        files_processed = 0
        dirs_processed = 0

        # å¤„ç†å½“å‰é¡µçš„æ•°æ®
        for data in response_data.get("data", []):
            if 'fid' in data:  # æ–‡ä»¶
                if data.get('ico') in MEDIATYPES:
                    # è½¬æ¢æ–‡ä»¶å¤§å° - å¤„ç†å¯èƒ½å·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„æƒ…å†µ
                    size_value = data.get('s', 0)
                    if isinstance(size_value, str) and size_value.endswith('GB'):
                        # å·²ç»æ˜¯GBæ ¼å¼ï¼Œä¿æŒä¸å˜
                        pass
                    else:
                        # æ˜¯å­—èŠ‚æ•°ï¼Œéœ€è¦è½¬æ¢
                        try:
                            bytes_value = int(size_value)
                            gb_in_bytes = 1024 ** 3
                            gb_value = bytes_value / gb_in_bytes
                            data['s'] = f"{gb_value:.1f}GB"
                        except (ValueError, TypeError):
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒåŸå€¼
                            pass

                    # æ„å»ºå®Œæ•´è·¯å¾„
                    filename = data.get("n", "")
                    if current_path_prefix:
                        data["file_name"] = os.path.join(current_path_prefix, filename)
                    else:
                        data["file_name"] = filename

                    all_files.append(data)
                    files_processed += 1

            elif 'cid' in data:  # ç›®å½•
                # é€’å½’å¤„ç†å­ç›®å½•ï¼Œæ·»åŠ å»¶è¿Ÿä»¥éµå®ˆQPSé™åˆ¶
                time.sleep(1.0 / QPS_LIMIT + 0.1)  # æ ¹æ®QPSé™åˆ¶åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                get_file_list_from_115(data['cid'], 0, limit, all_files)
                dirs_processed += 1

        # å¤„ç†åˆ†é¡µï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        if offset == 0 and total_pages > 1:
            logging.debug(f"å¤„ç†åˆ†é¡µ: æ€»é¡µæ•°={total_pages}")
            for page in range(1, total_pages):
                next_offset = page * limit

                # è·å–ä¸‹ä¸€é¡µæ•°æ®
                next_response_data = _get_single_level_content_from_115(cid, next_offset, limit)

                if not next_response_data or next_response_data.get("success") is False:
                    logging.warning(f"è·å–åˆ†é¡µæ•°æ®å¤±è´¥: CID={cid}, page={page}")
                    continue

                # å¤„ç†åˆ†é¡µæ•°æ®
                for data in next_response_data.get("data", []):
                    if 'fid' in data:
                        if data.get('ico') in MEDIATYPES:
                            # å¤„ç†æ–‡ä»¶å¤§å°ï¼Œå¯èƒ½æ˜¯å­—èŠ‚æ•°æˆ–å·²æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
                            size_value = data.get('s', 0)
                            if isinstance(size_value, str):
                                # å¦‚æœå·²ç»æ˜¯æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ "86.4GB"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                                data['s'] = size_value
                            else:
                                # å¦‚æœæ˜¯å­—èŠ‚æ•°ï¼Œè½¬æ¢ä¸ºGB
                                try:
                                    bytes_value = int(size_value)
                                    gb_in_bytes = 1024 ** 3
                                    gb_value = bytes_value / gb_in_bytes
                                    data['s'] = f"{gb_value:.1f}GB"
                                except (ValueError, TypeError):
                                    data['s'] = "0.0GB"

                            filename = data.get("n", "")
                            if current_path_prefix:
                                data["file_name"] = os.path.join(current_path_prefix, filename)
                            else:
                                data["file_name"] = filename

                            all_files.append(data)
                            files_processed += 1

                    elif 'cid' in data:
                        # é€’å½’å¤„ç†å­ç›®å½•ï¼Œæ·»åŠ å»¶è¿Ÿä»¥éµå®ˆQPSé™åˆ¶
                        time.sleep(1.0 / QPS_LIMIT + 0.1)  # æ ¹æ®QPSé™åˆ¶åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                        get_file_list_from_115(data['cid'], 0, limit, all_files)
                        dirs_processed += 1

        log_performance("get_file_list_from_115", start_time, True,
                       files_processed=files_processed, dirs_processed=dirs_processed)

        logging.debug(f"å®Œæˆæ–‡ä»¶åˆ—è¡¨è·å–: CID={cid}, æ–‡ä»¶={files_processed}, ç›®å½•={dirs_processed}")

    except Exception as e:
        log_performance("get_file_list_from_115", start_time, False, error=str(e))
        logging.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å‡ºé”™ CID {cid}: {e}", exc_info=True)
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å…¶ä»–ç›®å½•ç»§ç»­å¤„ç†


# ================================
# 115äº‘ç›˜ç‰¹å®šAPIç«¯ç‚¹
# ================================

@app.route('/api/115/files', methods=['GET'])
def get_115_files():
    """è·å–115äº‘ç›˜æ–‡ä»¶åˆ—è¡¨"""
    try:
        cid = request.args.get('cid', '0')
        offset = int(request.args.get('offset', 0))
        limit = int(request.args.get('limit', 100))
        recursive = request.args.get('recursive', 'false').lower() == 'true'

        if recursive:
            all_files = []
            get_file_list_from_115(cid, offset, limit, all_files)
            return jsonify({
                'success': True,
                'files': all_files,
                'total_count': len(all_files),
                'recursive': True
            })
        else:
            response_data = _get_single_level_content_from_115(cid, offset, limit)
            if response_data and response_data.get("success") is not False:
                return jsonify({
                    'success': True,
                    'data': response_data.get('data', []),
                    'count': response_data.get('count', 0),
                    'path': response_data.get('path', [])
                })
            else:
                return jsonify({'success': False, 'error': 'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥'})

    except Exception as e:
        logging.error(f"è·å–115æ–‡ä»¶åˆ—è¡¨å‡ºé”™: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/115/rename', methods=['POST'])
def rename_115_files():
    """æ‰¹é‡é‡å‘½å115äº‘ç›˜æ–‡ä»¶"""
    try:
        data = request.get_json()
        if not data or 'rename_dict' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é‡å‘½åæ•°æ®'})

        rename_dict = data['rename_dict']
        current_folder_id = data.get('current_folder_id', '0')  # è·å–å½“å‰æ–‡ä»¶å¤¹ID

        logging.info(f"ğŸ”„ å¼€å§‹é‡å‘½åæ“ä½œï¼Œå½“å‰æ–‡ä»¶å¤¹ID: {current_folder_id}")

        result = batch_rename_file(rename_dict)

        # å¦‚æœé‡å‘½åæˆåŠŸï¼Œæ·»åŠ åˆ·æ–°æŒ‡ä»¤
        if result.get('success'):
            result['refresh_folder'] = True
            result['folder_id'] = current_folder_id
            logging.info(f"âœ… é‡å‘½åæˆåŠŸï¼Œå°†åˆ·æ–°æ–‡ä»¶å¤¹ {current_folder_id}")

        # æ³¨æ„ï¼šbatch_rename_file å‡½æ•°å†…éƒ¨å·²ç»å¤„ç†äº†ç¼“å­˜æ¸…ç†
        # è¿™é‡Œä¸éœ€è¦é‡å¤æ¸…ç†ç¼“å­˜

        return jsonify(result)

    except Exception as e:
        logging.error(f"âŒ æ‰¹é‡é‡å‘½åå‡ºé”™: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/115/move', methods=['POST'])
def move_115_files():
    """æ‰¹é‡ç§»åŠ¨115äº‘ç›˜æ–‡ä»¶"""
    try:
        data = request.get_json()
        if not data or 'file_ids' not in data or 'target_id' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ç§»åŠ¨æ•°æ®'})

        file_ids = data['file_ids']
        target_id = data['target_id']

        result = batch_move_file(file_ids, target_id)

        return jsonify(result)

    except Exception as e:
        logging.error(f"æ‰¹é‡ç§»åŠ¨å‡ºé”™: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/115/scrape', methods=['POST'])
def scrape_115_files():
    """åˆ®å‰Š115äº‘ç›˜æ–‡ä»¶ä¿¡æ¯"""
    try:
        data = request.get_json()
        if not data or 'files' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶æ•°æ®'})

        files = data['files']
        chunk_size = data.get('chunk_size', CHUNK_SIZE)

        all_results = []

        # åˆ†å—å¤„ç†
        for i in range(0, len(files), chunk_size):
            chunk = files[i:i + chunk_size]
            results = extract_movie_name_and_info(chunk)
            all_results.extend(results)

        return jsonify({
            'success': True,
            'results': all_results,
            'total_processed': len(files),
            'successful_matches': len([r for r in all_results if r.get('status') == 'success'])
        })

    except Exception as e:
        logging.error(f"åˆ®å‰Šæ–‡ä»¶ä¿¡æ¯å‡ºé”™: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/115/search', methods=['GET'])
def search_115_files():
    """æœç´¢115äº‘ç›˜æ–‡ä»¶"""
    try:
        query = request.args.get('query', '').strip()
        cid = request.args.get('cid', '0')
        file_type = request.args.get('type', 'all')  # all, video, audio, image, document
        limit = int(request.args.get('limit', 100))

        if not query:
            return jsonify({'success': False, 'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'})

        # ç¼“å­˜é€»è¾‘å·²åˆ é™¤

        # æ‰§è¡Œæœç´¢ - è¿™é‡Œéœ€è¦å®ç°115äº‘ç›˜çš„æœç´¢API
        # æš‚æ—¶ä½¿ç”¨æ–‡ä»¶åˆ—è¡¨è¿‡æ»¤ä½œä¸ºç¤ºä¾‹
        all_files = []
        get_file_list_from_115(cid, 0, 1000, all_files)

        # è¿‡æ»¤æœç´¢ç»“æœ
        results = []
        for file in all_files:
            filename = file.get('n', '').lower()
            if query.lower() in filename:
                # æ ¹æ®æ–‡ä»¶ç±»å‹è¿‡æ»¤
                if file_type != 'all':
                    file_ext = filename.split('.')[-1] if '.' in filename else ''
                    if file_type == 'video' and file_ext not in ['mp4', 'mkv', 'avi', 'mov', 'wmv', 'flv']:
                        continue
                    elif file_type == 'audio' and file_ext not in ['mp3', 'wav', 'flac', 'aac', 'm4a']:
                        continue
                    elif file_type == 'image' and file_ext not in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']:
                        continue
                    elif file_type == 'document' and file_ext not in ['pdf', 'doc', 'docx', 'txt', 'rtf']:
                        continue

                results.append(file)
                if len(results) >= limit:
                    break

        # ç¼“å­˜ç»“æœ - æœç´¢ç¼“å­˜å·²ç¦ç”¨

        return jsonify({
            'success': True,
            'results': results,
            'total_found': len(results),
            'query': query,
            'file_type': file_type
        })

    except Exception as e:
        logging.error(f"æœç´¢115æ–‡ä»¶å‡ºé”™: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/115/stats', methods=['GET'])
def get_115_stats():
    """è·å–115äº‘ç›˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        cid = request.args.get('cid', '0')

        # ç¼“å­˜é€»è¾‘å·²åˆ é™¤

        # è·å–æ–‡ä»¶åˆ—è¡¨è¿›è¡Œç»Ÿè®¡
        all_files = []
        get_file_list_from_115(cid, 0, 1000, all_files)  # é™åˆ¶æœ€å¤§æ–‡ä»¶æ•°

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_files': 0,
            'total_folders': 0,
            'total_size': 0,
            'file_types': {},
            'largest_files': [],
            'recent_files': []
        }

        for item in all_files:
            if item.get('is_dir'):
                stats['total_folders'] += 1
            else:
                stats['total_files'] += 1

                # æ–‡ä»¶å¤§å°
                size = int(item.get('s', 0))
                stats['total_size'] += size

                # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
                filename = item.get('n', '')
                ext = filename.split('.')[-1].lower() if '.' in filename else 'unknown'
                stats['file_types'][ext] = stats['file_types'].get(ext, 0) + 1

                # æœ€å¤§æ–‡ä»¶
                if len(stats['largest_files']) < 10:
                    stats['largest_files'].append({
                        'name': filename,
                        'size': size,
                        'id': item.get('fid')
                    })
                else:
                    # ä¿æŒæœ€å¤§çš„10ä¸ªæ–‡ä»¶
                    min_file = min(stats['largest_files'], key=lambda x: x['size'])
                    if size > min_file['size']:
                        stats['largest_files'].remove(min_file)
                        stats['largest_files'].append({
                            'name': filename,
                            'size': size,
                            'id': item.get('fid')
                        })

        # æ’åºæœ€å¤§æ–‡ä»¶
        stats['largest_files'].sort(key=lambda x: x['size'], reverse=True)

        # æ ¼å¼åŒ–å¤§å°
        stats['total_size_formatted'] = format_file_size(stats['total_size'])

        # ç¼“å­˜ç»“æœ

        return jsonify({
            'success': True,
            'stats': stats,
            'cid': cid
        })

    except Exception as e:
        logging.error(f"è·å–115ç»Ÿè®¡ä¿¡æ¯å‡ºé”™: {e}")
        return jsonify({'success': False, 'error': str(e)})

# ================================
# Flask è·¯ç”±
# ================================

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/logs')
def get_logs():
    # è¿”å›æ‰€æœ‰å½“å‰å­˜å‚¨çš„æ—¥å¿—
    return jsonify(list(log_queue))

@app.route('/config', methods=['GET'])
def get_config():
    """è¿”å›å½“å‰é…ç½®ã€‚"""
    # è¿‡æ»¤æ‰æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚ COOKIES
    display_config = app_config.copy()
    # display_config['COOKIES'] = '********' if display_config['COOKIES'] else ''
    return jsonify(display_config)

@app.route('/save_config', methods=['POST'])
def save_configuration():
    """ä¿å­˜ç”¨æˆ·æäº¤çš„é…ç½®ã€‚"""
    global app_config
    try:
        new_config_data = request.json
        logging.info(f"æ¥æ”¶åˆ°æ–°çš„é…ç½®æ•°æ®: {new_config_data}")

        # éªŒè¯å¹¶æ›´æ–°é…ç½®
        for key in ["QPS_LIMIT", "CHUNK_SIZE", "MAX_WORKERS", "COOKIES", "TMDB_API_KEY",
                   "GEMINI_API_KEY", "GEMINI_API_URL", "MODEL", "GROUPING_MODEL", "LANGUAGE",
                   "API_MAX_RETRIES", "API_RETRY_DELAY", "AI_API_TIMEOUT", "AI_MAX_RETRIES",
                   "AI_RETRY_DELAY", "TMDB_API_TIMEOUT", "TMDB_MAX_RETRIES", "TMDB_RETRY_DELAY",
                   "CLOUD_API_MAX_RETRIES", "CLOUD_API_RETRY_DELAY", "GROUPING_MAX_RETRIES",
                   "GROUPING_RETRY_DELAY", "TASK_QUEUE_GET_TIMEOUT", "ENABLE_QUALITY_ASSESSMENT",
                   "ENABLE_SCRAPING_QUALITY_ASSESSMENT", "KILL_OCCUPIED_PORT_PROCESS"]:
            if key in new_config_data:
                if key in ["QPS_LIMIT", "CHUNK_SIZE", "MAX_WORKERS", "API_MAX_RETRIES",
                          "AI_API_TIMEOUT", "AI_MAX_RETRIES", "TMDB_API_TIMEOUT", "TMDB_MAX_RETRIES",
                          "CLOUD_API_MAX_RETRIES", "GROUPING_MAX_RETRIES", "TASK_QUEUE_GET_TIMEOUT"]:
                    app_config[key] = int(new_config_data[key])
                elif key in ["API_RETRY_DELAY", "AI_RETRY_DELAY", "TMDB_RETRY_DELAY",
                            "CLOUD_API_RETRY_DELAY", "GROUPING_RETRY_DELAY"]:
                    app_config[key] = float(new_config_data[key])
                elif key in ["ENABLE_QUALITY_ASSESSMENT", "ENABLE_SCRAPING_QUALITY_ASSESSMENT",
                            "KILL_OCCUPIED_PORT_PROCESS"]:
                    app_config[key] = bool(new_config_data[key])
                else:
                    app_config[key] = new_config_data[key]

        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        if save_config():
            # é‡æ–°åŠ è½½æ‰€æœ‰é…ç½®å’Œç›¸å…³å…¨å±€å˜é‡
            load_config()
            load_cookies()

            # ç¡®ä¿ QPS é™åˆ¶å™¨ä¹Ÿæ›´æ–°
            global qps_limiter
            qps_limiter = QPSLimiter(qps_limit=app_config["QPS_LIMIT"])

            logging.info("é…ç½®å·²æ›´æ–°å¹¶åº”ç”¨ã€‚")
            return jsonify({'success': True, 'message': 'é…ç½®ä¿å­˜æˆåŠŸå¹¶å·²åº”ç”¨ã€‚'})
        else:
            return jsonify({'success': False, 'error': 'é…ç½®ä¿å­˜å¤±è´¥ã€‚'})
    except Exception as e:
        logging.error(f"ä¿å­˜é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'ä¿å­˜é…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'})

def get_parent_cid_from_path(current_path):
    if current_path == '/':
        return '0'

    if current_path.endswith('/'):
        current_path = current_path[:-1]

    parent_path = os.path.dirname(current_path)
    if parent_path == '':
        parent_path = '/'

    try:
        url = "https://webapi.115.com/files/getid?path=" + parse.quote(parent_path)
        logging.info(f"å°è¯•ä» 115 API è·å–çˆ¶ç›®å½•ID: {url}")
        response = requests.get(url, headers=headers, cookies=cookies)
        response.raise_for_status()
        rootobject = response.json()
        logging.info(rootobject)
        if rootobject.get("state"):
            parent_cid = str(rootobject.get("id", '0'))
            logging.info(f"è·å–åˆ°çˆ¶ç›®å½• '{parent_path}' çš„ CID: {parent_cid}")
            return parent_cid
        else:
            logging.error(f"è·å–çˆ¶ç›®å½• '{parent_path}' ID é”™è¯¯: {rootobject.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return '0' # å¤±è´¥æ—¶è¿”å›æ ¹ç›®å½•CID
    except requests.exceptions.HTTPError as e:
        logging.error(f"HTTP Error è·å–çˆ¶ç›®å½•ID for path {parent_path}: {e.response.status_code} - {e.response.text}")
        return '0'
    except Exception as e:
        logging.error(f"Error è·å–çˆ¶ç›®å½•ID for path {parent_path}: {e}")
        return '0'

@app.route('/get_folder_content', methods=['POST'])
def get_folder_content():
    cid = request.form.get('cid', DEFAULT_CID)
    limit = int(request.form.get('limit', 150)) # é™åˆ¶å•æ¬¡è¯·æ±‚è¿”å›çš„æ–‡ä»¶/ç›®å½•æ•°é‡
    offset = int(request.form.get('offset', 0))

    logging.info(f"ğŸ” è·å– CID: {cid} ä¸‹çš„å•å±‚å†…å®¹ï¼Œåç§»é‡ {offset}ï¼Œé™åˆ¶ {limit} æ¡ã€‚")

    # ç¼“å­˜æ¸…ç†é€»è¾‘å·²åˆ é™¤

    # é‡ç½®processed_itemsé›†åˆï¼Œç¡®ä¿æ–°è¯·æ±‚æ—¶ä»å¤´å¼€å§‹æ£€æµ‹é‡å¤
    if offset == 0:
        logging.info(f"ğŸ”„ å¼€å§‹æ–°çš„ç›®å½•è¯·æ±‚ï¼Œé‡ç½®é‡å¤æ£€æµ‹")

    all_files_and_folders = []
    current_offset = offset
    total_count = 0
    paths = []
    current_path_prefix = ""  # åˆå§‹åŒ–è·¯å¾„å‰ç¼€

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    processed_items = set()  # ç”¨äºæ£€æµ‹é‡å¤é¡¹
    duplicate_count = 0  # é‡å¤é¡¹è®¡æ•°

    while True:
        response_data = _get_single_level_content_from_115(cid, current_offset, limit)
        
        if response_data.get("success") is False:
            return jsonify(response_data) # è¿”å›é”™è¯¯ä¿¡æ¯

        if not paths: # ç¬¬ä¸€æ¬¡è¯·æ±‚æ—¶è·å–è·¯å¾„ä¿¡æ¯
            paths = response_data.get("path", [])
            # ç»Ÿä¸€æ ¹ç›®å½•çš„åç§°ä¸ºâ€œæ ¹ç›®å½•â€
            for p in paths:
                if p.get('cid') == '0':
                    p['name'] = 'æ ¹ç›®å½•'
        
        total_count = response_data.get("count", 0)
        data_items = response_data.get("data", [])
        
        if not data_items:
            break # æ²¡æœ‰æ›´å¤šæ•°æ®äº†

        current_path_parts = [a['name'] for a in paths[1:]]
        current_path_prefix = "/".join(current_path_parts)

        for data in data_items:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦æ¥æ£€æµ‹é‡å¤
            unique_id = data.get('fid') or data.get('cid')
            item_name = data.get('n')

            # æ£€æµ‹é‡å¤é¡¹
            if unique_id in processed_items:
                # åªåœ¨DEBUGçº§åˆ«è®°å½•é‡å¤é¡¹ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
                logging.debug(f"è·³è¿‡é‡å¤é¡¹: {item_name} (ID: {unique_id}) at offset {current_offset}")
                duplicate_count += 1
                continue

            processed_items.add(unique_id)

            item = {
                'name': item_name,
                'is_dir': 'cid' in data,
                'cid': data.get('cid'),
                'fid': data.get('fid'),
                'size': data.get('s'),
                'file_name': os.path.join(current_path_prefix, item_name) if 'fid' in data else ''
            }

            # æ·»åŠ ç»Ÿä¸€çš„fileIdå­—æ®µ
            if 'fid' in data: # æ–‡ä»¶
                item['fileId'] = data.get('fid')
                if data.get('ico') in MEDIATYPES:
                    # å¤„ç†æ–‡ä»¶å¤§å°ï¼Œå¯èƒ½æ˜¯å­—èŠ‚æ•°æˆ–å·²æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
                    size_value = data.get('s', 0)
                    if isinstance(size_value, str):
                        # å¦‚æœå·²ç»æ˜¯æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ "86.4GB"ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        item['size'] = size_value
                    else:
                        # å¦‚æœæ˜¯å­—èŠ‚æ•°ï¼Œè½¬æ¢ä¸ºGB
                        try:
                            bytes_value = int(size_value)
                            gb_in_bytes = 1024 ** 3
                            gb_value = bytes_value / gb_in_bytes
                            item['size'] = f"{gb_value:.1f}GB"
                        except (ValueError, TypeError):
                            item['size'] = "0.0GB"
                all_files_and_folders.append(item)  # ç§»åˆ°ifå¤–é¢ï¼Œæ‰€æœ‰æ–‡ä»¶éƒ½æ·»åŠ 
            else: # ç›®å½•
                item['fileId'] = data.get('cid')
                all_files_and_folders.append(item)
                logging.debug(f"ğŸ“ æ·»åŠ æ–‡ä»¶å¤¹: {item_name} (CID: {data.get('cid')})")
        
        current_offset += len(data_items)
        logging.info(f"current_offset: {current_offset}") # æ·»åŠ æ—¥å¿—

        if current_offset >= total_count:
            break # å·²è·å–æ‰€æœ‰æ•°æ®

    if len(paths) < 2:
        parent_cid_calculated = '0'
    else:
        parent_cid_calculated = paths[-2]['cid']

    logging.info(f"ğŸ“Š get_folder_content ç»Ÿè®¡:")
    logging.info(f"  - parent_cid: {parent_cid_calculated}")
    logging.info(f"  - current_path: {current_path_prefix}")
    logging.info(f"  - æ€»é¡¹ç›®æ•°: {len(all_files_and_folders)}")
    logging.info(f"  - å¤„ç†çš„å”¯ä¸€é¡¹ç›®æ•°: {len(processed_items)}")
    if duplicate_count > 0:
        logging.info(f"  - è·³è¿‡çš„é‡å¤é¡¹: {duplicate_count} ä¸ª")

    # ç»Ÿè®¡æ–‡ä»¶å¤¹å’Œæ–‡ä»¶æ•°é‡
    folder_count = sum(1 for item in all_files_and_folders if item.get('is_dir'))
    file_count = len(all_files_and_folders) - folder_count
    logging.info(f"  - æ–‡ä»¶å¤¹: {folder_count} ä¸ª, æ–‡ä»¶: {file_count} ä¸ª")

    return jsonify({
        'success': True,
        'current_cid': cid,
        'current_folder_id': cid,  # æ·»åŠ å‰ç«¯æœŸæœ›çš„å­—æ®µå
        'parent_cid': parent_cid_calculated,
        'parent_folder_id': parent_cid_calculated,  # æ·»åŠ å‰ç«¯æœŸæœ›çš„å­—æ®µå
        'current_path': "/" + current_path_prefix,
        'files_and_folders': all_files_and_folders,
        'total_count': total_count,
        'path_parts': paths # æ–°å¢ï¼šè¿”å›è·¯å¾„çš„å„ä¸ªéƒ¨åˆ†
    })


@app.route('/get_folder_properties', methods=['POST'])
def get_folder_properties():
    """è·å–æ–‡ä»¶å¤¹åŸºæœ¬å±æ€§ï¼ˆæ–‡ä»¶æ•°é‡å’Œæ€»å¤§å°ï¼‰- æ”¯æŒæ™ºèƒ½åˆ†ç»„"""
    try:
        # æ”¯æŒä¸¤ç§å‚æ•°åï¼šfolder_idï¼ˆæ–°ï¼‰å’Œcidï¼ˆæ—§ï¼‰
        folder_id = request.form.get('folder_id') or request.form.get('cid')
        include_grouping = request.form.get('include_grouping', 'false').lower() == 'true'

        if not folder_id:
            return jsonify({'success': False, 'error': 'folder_id is required.'})

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if folder_id == 'null' or folder_id == 'undefined':
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        try:
            folder_id = int(folder_id)
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'})

        logging.info(f"ğŸ” è·å–æ–‡ä»¶å¤¹ {folder_id} çš„åŸºæœ¬å±æ€§ï¼Œæ™ºèƒ½åˆ†ç»„: {include_grouping}")

        if include_grouping:
            # ğŸš€ ä½¿ç”¨æ–°çš„ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿè¿›è¡Œæ™ºèƒ½åˆ†ç»„
            try:
                # æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨æ˜¯å¦å¯ç”¨
                if grouping_task_manager is None:
                    return jsonify({
                        'success': False,
                        'error': 'ä»»åŠ¡ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€',
                        'task_queue_error': True
                    })

                # è·å–æ–‡ä»¶å¤¹åç§°
                folder_name = request.form.get('folder_name', f'æ–‡ä»¶å¤¹{folder_id}')

                # æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
                task_id = grouping_task_manager.submit_task(folder_id, folder_name)

                return jsonify({
                    'success': True,
                    'use_task_queue': True,
                    'task_id': task_id,
                    'message': f'æ™ºèƒ½åˆ†ç»„ä»»åŠ¡å·²æäº¤åˆ°é˜Ÿåˆ— (ä»»åŠ¡ID: {task_id})'
                })

            except ValueError as e:
                # å¦‚æœæ˜¯é‡å¤ä»»åŠ¡æˆ–é˜Ÿåˆ—æ»¡çš„é”™è¯¯ï¼Œè¿”å›ç›¸åº”ä¿¡æ¯
                return jsonify({
                    'success': False,
                    'error': str(e),
                    'task_queue_error': True
                })
        else:
            # ğŸ” åªè·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯ï¼Œä¸è¿›è¡Œæ™ºèƒ½åˆ†ç»„
            video_files = []
            try:
                get_video_files_recursively(folder_id, video_files)
            except Exception as e:
                logging.error(f"é€’å½’è·å–è§†é¢‘æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}'})

            return jsonify({
                'success': True,
                'count': len(video_files),
                'size': f"{sum(file.get('size', 0) for file in video_files) / (1024**3):.1f}GB",
                'video_files': video_files
            })

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å±æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/scrape_preview', methods=['POST'])
def scrape_preview():
    """åˆ®å‰Šé¢„è§ˆ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡ç®¡ç†å’Œç¼“å­˜"""
    start_time = time.time()

    try:
        logging.info("å¼€å§‹åˆ®å‰Šé¢„è§ˆ")

        # è·å–è¯·æ±‚æ•°æ®
        selected_items_json = request.form.get('items')
        if not selected_items_json:
            logging.warning("æ²¡æœ‰é€‰æ‹©ä»»ä½•é¡¹ç›®è¿›è¡Œåˆ®å‰Š")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©ä»»ä½•é¡¹ç›®è¿›è¡Œåˆ®å‰Š'})

        # è§£æé€‰ä¸­é¡¹ç›®
        try:
            selected_items = json.loads(selected_items_json)
        except json.JSONDecodeError:
            logging.error("é€‰æ‹©çš„é¡¹ç›®JSONæ— æ•ˆ")
            return jsonify({'success': False, 'error': 'é€‰æ‹©çš„é¡¹ç›®JSONæ— æ•ˆ'})

        if not selected_items:
            logging.warning("æ²¡æœ‰é¡¹ç›®å¯å¤„ç†")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é¡¹ç›®å¯å¤„ç†'})

        logging.info(f"é€‰æ‹©è¿›è¡Œåˆ®å‰Šçš„é¡¹ç›®æ•°é‡: {len(selected_items)}")

        # åˆ›å»ºåˆ®å‰Šä»»åŠ¡
        task_id = f"scrape_preview_{int(time.time())}"
        task = GroupingTask(
            task_id=task_id,
            folder_id="0",  # é»˜è®¤æ ¹ç›®å½•
            folder_name=f"åˆ®å‰Šé¢„è§ˆ {len(selected_items)} ä¸ªé¡¹ç›®"
        )

        # æ”¶é›†æ–‡ä»¶ - ä½¿ç”¨ä¸“é—¨çš„åˆ®å‰Šæ–‡ä»¶æ”¶é›†å‡½æ•°
        files = []
        for item in selected_items:
            if item.get('is_dir'):
                # å¦‚æœæ˜¯æ–‡ä»¶å¤¹ï¼Œé€’å½’è·å–å…¶ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä¸è¿‡æ»¤ç±»å‹ï¼‰
                folder_id = item.get('fileId')  # ä½¿ç”¨fileIdä½œä¸ºæ–‡ä»¶å¤¹ID
                if folder_id:
                    logging.info(f"ğŸ“ é€’å½’è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºåˆ®å‰Šé¢„è§ˆï¼‰")
                    before_count = len(files)
                    get_all_files_from_115_for_scraping(folder_id, 0, 150, files)
                    after_count = len(files)
                    logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ {folder_id} æ·»åŠ äº† {after_count - before_count} ä¸ªæ–‡ä»¶")
            else:
                # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥æ·»åŠ åˆ°åˆ—è¡¨
                files.append({
                    'fid': item.get('fileId'),
                    'n': item.get('name'),
                    'pc': item.get('file_name', ''),  # ä½¿ç”¨file_nameä½œä¸ºè·¯å¾„
                    's': item.get('size', 0)
                })
                logging.debug(f"ğŸ“„ ç›´æ¥æ·»åŠ æ–‡ä»¶: {item.get('name')}")

        logging.info(f"ğŸ“Š æ–‡ä»¶æ”¶é›†å®Œæˆ: æ€»å…±æ”¶é›†åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼ˆåŒ…æ‹¬æ‰€æœ‰ç±»å‹ï¼‰")

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬å·²å¤„ç†çš„æ–‡ä»¶
        all_files = []
        already_processed_files = []

        for f in files:
            filename = f.get("n", "") or ""  # ç¡®ä¿filenameä¸æ˜¯None
            # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«TMDBä¿¡æ¯
            if 'tmdb-' in filename:
                already_processed_files.append(f)
            else:
                all_files.append(f)

        # æ€»æ–‡ä»¶æ•°åŒ…æ‹¬å·²å¤„ç†å’Œå¾…å¤„ç†çš„æ–‡ä»¶
        total_files_count = len(all_files) + len(already_processed_files)
        task.total_items = total_files_count
        task_manager.add_task(task)

        logging.info(f"ğŸ“Š æ–‡ä»¶åˆ†ç±»ç»Ÿè®¡:")
        logging.info(f"  - å¾…å¤„ç†æ–‡ä»¶: {len(all_files)} ä¸ª")
        logging.info(f"  - å·²å¤„ç†æ–‡ä»¶: {len(already_processed_files)} ä¸ª")
        logging.info(f"  - æ€»æ–‡ä»¶æ•°: {total_files_count} ä¸ª")

        all_scraped_results = []

        # ä¸ºå·²å¤„ç†çš„æ–‡ä»¶åˆ›å»ºç»“æœæ¡ç›®
        for f in already_processed_files:
            filename = f.get("n", "")
            result_item = {
                'fid': f.get('fid'),
                'original_name': filename,
                'suggested_name': filename,  # å·²å¤„ç†æ–‡ä»¶ä¿æŒåŸå
                'size': f.get('s'),
                'tmdb_info': None,
                'file_info': None,
                'status': 'already_processed',
                'message': 'æ–‡ä»¶å·²åŒ…å«TMDBä¿¡æ¯'
            }
            all_scraped_results.append(result_item)
            logging.debug(f"ğŸ”µ æ·»åŠ å·²å¤„ç†æ–‡ä»¶: {filename}")

        logging.info(f"ğŸ”µ å·²ä¸º {len(already_processed_files)} ä¸ªå·²å¤„ç†æ–‡ä»¶åˆ›å»ºç»“æœæ¡ç›®")

        if all_files:
            # åˆ†å—å¤„ç†
            chunk_size = min(CHUNK_SIZE, 20)  # é™åˆ¶é¢„è§ˆæ—¶çš„å—å¤§å°
            chunks = [all_files[i:i + chunk_size] for i in range(0, len(all_files), chunk_size)]

            # ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨å¤„ç†
            with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, 3)) as executor:
                futures = []

                for i, chunk in enumerate(chunks):
                    if task.status == TaskStatus.CANCELLED:
                        break

                    future = executor.submit(extract_movie_name_and_info, chunk)
                    futures.append((future, i))

                # æ”¶é›†ç»“æœ
                for future, chunk_index in futures:
                    if task.status == TaskStatus.CANCELLED:
                        break

                    try:
                        results = future.result(timeout=120)  # å¢åŠ åˆ°120ç§’è¶…æ—¶
                        all_scraped_results.extend(results)
                        logging.info(f"âœ… å— {chunk_index} å¤„ç†å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")

                        # æ›´æ–°ä»»åŠ¡è¿›åº¦
                        processed_count = (chunk_index + 1) * len(chunks[chunk_index]) if chunks else 0
                        total_count = total_files_count if total_files_count else 1
                        task.progress = min(100, (processed_count / total_count) * 100)

                    except TimeoutError:
                        logging.error(f'â° å— {chunk_index} å¤„ç†è¶…æ—¶ (120ç§’)')
                        task.error = f"å— {chunk_index} å¤„ç†è¶…æ—¶"
                        # ä¸ºè¶…æ—¶çš„æ–‡ä»¶åˆ›å»ºå¤±è´¥ç»“æœ
                        chunk_files = chunks[chunk_index] if chunk_index < len(chunks) else []
                        for file_data in chunk_files:
                            timeout_result = {
                                'fid': file_data.get('fid'),
                                'original_name': file_data.get('n', ''),
                                'suggested_name': file_data.get('n', ''),
                                'size': file_data.get('s', ''),
                                'tmdb_info': None,
                                'file_info': None,
                                'status': 'timeout',
                                'message': 'å¤„ç†è¶…æ—¶'
                            }
                            all_scraped_results.append(timeout_result)
                        logging.info(f"âš ï¸ ä¸ºè¶…æ—¶å— {chunk_index} åˆ›å»ºäº† {len(chunk_files)} ä¸ªè¶…æ—¶ç»“æœ")
                    except Exception as exc:
                        logging.error(f'âŒ å— {chunk_index} å¤„ç†å¼‚å¸¸: {exc}', exc_info=True)
                        task.error = f"å— {chunk_index} å¤„ç†å¤±è´¥: {str(exc)}"
                        # ä¸ºå¼‚å¸¸çš„æ–‡ä»¶åˆ›å»ºå¤±è´¥ç»“æœ
                        chunk_files = chunks[chunk_index] if chunk_index < len(chunks) else []
                        for file_data in chunk_files:
                            error_result = {
                                'fid': file_data.get('fid'),
                                'original_name': file_data.get('n', ''),
                                'suggested_name': file_data.get('n', ''),
                                'size': file_data.get('s', ''),
                                'tmdb_info': None,
                                'file_info': None,
                                'status': 'error',
                                'message': f'å¤„ç†å¼‚å¸¸: {str(exc)}'
                            }
                            all_scraped_results.append(error_result)
                        logging.info(f"âš ï¸ ä¸ºå¼‚å¸¸å— {chunk_index} åˆ›å»ºäº† {len(chunk_files)} ä¸ªé”™è¯¯ç»“æœ")

        # å®Œæˆä»»åŠ¡
        if task.status != TaskStatus.CANCELLED:
            task.status = TaskStatus.COMPLETED
            task.progress = 100
            task.completed_at = time.time()

        # è®°å½•æ€§èƒ½
        log_performance("scrape_preview", start_time, True,
                       total_files=total_files_count,
                       successful_matches=len([r for r in all_scraped_results if r.get('status') == 'success']))

        logging.info(f"ğŸ¯ åˆ®å‰Šé¢„è§ˆå®Œæˆ: {len(all_scraped_results)} ä¸ªç»“æœ")

        # ç»Ÿè®¡å„ç§çŠ¶æ€çš„æ–‡ä»¶æ•°é‡
        status_stats = {}
        for result in all_scraped_results:
            status = result.get('status', 'unknown')
            status_stats[status] = status_stats.get(status, 0) + 1

        successful_matches = status_stats.get('success', 0)
        already_processed_count = status_stats.get('already_processed', 0)
        failed_matches = len(all_scraped_results) - successful_matches - already_processed_count

        logging.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ç»“æœ:")
        logging.info(f"  - æˆåŠŸåˆ®å‰Š: {successful_matches} ä¸ª")
        logging.info(f"  - å·²å¤„ç†: {already_processed_count} ä¸ª")
        logging.info(f"  - å¤±è´¥: {failed_matches} ä¸ª")
        logging.info(f"  - æ€»ç»“æœæ•°: {len(all_scraped_results)} ä¸ª")
        logging.info(f"  - åŸå§‹æ–‡ä»¶æ•°: {total_files_count} ä¸ª")
        logging.info(f"  - çŠ¶æ€åˆ†å¸ƒ: {status_stats}")

        # éªŒè¯ç»“æœæ•°é‡æ˜¯å¦åŒ¹é…
        if len(all_scraped_results) != total_files_count:
            logging.error(f"âŒ ç»“æœæ•°é‡ä¸åŒ¹é…! é¢„æœŸ: {total_files_count}, å®é™…: {len(all_scraped_results)}")
            logging.error(f"   å¾…å¤„ç†æ–‡ä»¶: {len(all_files)}, å·²å¤„ç†æ–‡ä»¶: {len(already_processed_files)}")
        else:
            logging.info(f"âœ… ç»“æœæ•°é‡åŒ¹é…: {len(all_scraped_results)} ä¸ª")

        return jsonify({
            'success': True,
            'results': all_scraped_results,
            'task_id': task_id,
            'total_files': total_files_count,
            'processed_files': len(all_scraped_results),
            'successful_matches': successful_matches,
            'already_processed_count': already_processed_count,
            'failed_matches': failed_matches,
            'status_stats': status_stats,
            'performance': {
                'duration': time.time() - start_time,
                'files_per_second': total_files_count / (time.time() - start_time) if time.time() - start_time > 0 else 0
            }
        })

    except Exception as e:
        log_performance("scrape_preview", start_time, False, error=str(e))
        logging.error(f"åˆ®å‰Šé¢„è§ˆå¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/apply_rename', methods=['POST'])
def apply_rename():
    """åº”ç”¨é‡å‘½å - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡ç®¡ç†å’Œæ€§èƒ½ç›‘æ§"""
    start_time = time.time()

    try:
        logging.info("å¼€å§‹åº”ç”¨é‡å‘½å")

        # è·å–é‡å‘½åæ•°æ®
        rename_data_json = request.form.get('rename_data')
        if not rename_data_json:
            logging.warning("æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®'})

        # è§£æé‡å‘½åæ•°æ®
        try:
            rename_data = json.loads(rename_data_json)
        except json.JSONDecodeError:
            logging.error("é‡å‘½åæ•°æ®JSONæ— æ•ˆ")
            return jsonify({'success': False, 'error': 'é‡å‘½åæ•°æ®JSONæ— æ•ˆ'})

        if not rename_data:
            logging.warning("æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½å")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½å'})

        logging.info(f"å‡†å¤‡é‡å‘½å {len(rename_data)} ä¸ªæ–‡ä»¶")

        # åˆ›å»ºé‡å‘½åä»»åŠ¡
        task_id = f"apply_rename_{int(time.time())}"
        task = GroupingTask(
            task_id=task_id,
            folder_id="0",  # é»˜è®¤æ ¹ç›®å½•
            folder_name=f"æ‰¹é‡é‡å‘½å {len(rename_data)} ä¸ªæ–‡ä»¶"
        )
        task_manager.add_task(task)

        # æ„å»ºé‡å‘½åå­—å…¸å’ŒåŸå§‹åç§°æ˜ å°„
        namedict = {}
        original_names_map = {}

        for item in rename_data:
            # å…¼å®¹ä¸åŒçš„IDå­—æ®µå
            item_id = item.get('id') or item.get('fid')
            new_name = item.get('suggested_name')
            original_name = item.get('original_name')

            # åªå¤„ç†æœ‰æ•ˆçš„é‡å‘½åé¡¹ç›®
            if item_id and new_name and original_name:
                # ä½¿ç”¨å®‰å…¨æ–‡ä»¶å
                safe_new_name = safe_filename(new_name)
                namedict[item_id] = safe_new_name
                original_names_map[item_id] = original_name
                logging.debug(f"å‡†å¤‡é‡å‘½å: {item_id} {original_name} -> {safe_new_name}")

        if not namedict:
            task.status = "failed"
            task.error = "æ²¡æœ‰æœ‰æ•ˆçš„é‡å‘½åæ•°æ®"
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„é‡å‘½åæ•°æ®")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„é‡å‘½åæ•°æ®'})

        # è®¾ç½®ä»»åŠ¡çš„æ€»é¡¹ç›®æ•°
        task.total_items = len(namedict)

        # å¤‡ä»½é‡å‘½åæ•°æ®
        backup_data = {
            'namedict': namedict,
            'original_names_map': original_names_map,
            'timestamp': datetime.datetime.now().isoformat(),
            'task_id': task_id
        }

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file_path = f'rename_backup_{timestamp}.json'

        try:
            with open(backup_file_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            logging.info(f"é‡å‘½åæ•°æ®å·²å¤‡ä»½åˆ°: {backup_file_path}")
        except Exception as e:
            logging.error(f"å¤‡ä»½é‡å‘½åæ•°æ®å¤±è´¥: {e}")
            task.error = f"å¤‡ä»½å¤±è´¥: {str(e)}"

        # åˆ†å—å¤„ç†é‡å‘½å
        chunk_size = min(CHUNK_SIZE, 100)  # é™åˆ¶é‡å‘½åå—å¤§å°
        namedict_chunks = [
            dict(list(namedict.items())[i:i + chunk_size])
            for i in range(0, len(namedict), chunk_size)
        ]

        # æ‰§è¡Œæ‰¹é‡é‡å‘½å
        all_results = []
        overall_success = True
        overall_errors = []

        logging.info(f"å¼€å§‹æ‰¹é‡é‡å‘½åï¼Œå…± {len(namedict_chunks)} ä¸ªæ‰¹æ¬¡")

        for i, chunk in enumerate(namedict_chunks):
            if task.status == TaskStatus.CANCELLED:
                logging.info("ä»»åŠ¡å·²å–æ¶ˆï¼Œåœæ­¢é‡å‘½å")
                break

            logging.info(f"å¤„ç†ç¬¬ {i+1}/{len(namedict_chunks)} æ‰¹ï¼ŒåŒ…å« {len(chunk)} ä¸ªæ–‡ä»¶")

            try:
                rename_result = batch_rename_file(chunk)

                if rename_result.get('success'):
                    # æˆåŠŸçš„æ–‡ä»¶
                    for item_id, new_name in chunk.items():
                        all_results.append({
                            'id': item_id,
                            'type': 'file',
                            'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                            'new_name': new_name,
                            'status': 'success'
                        })

                    logging.info(f"ç¬¬ {i+1} æ‰¹é‡å‘½åæˆåŠŸ: {len(chunk)} ä¸ªæ–‡ä»¶")

                else:
                    # å¤±è´¥çš„æ‰¹æ¬¡
                    overall_success = False
                    error_message = rename_result.get('error', 'æ‰¹é‡é‡å‘½åå¤±è´¥')

                    logging.error(f"ç¬¬ {i+1} æ‰¹é‡å‘½åå¤±è´¥: {error_message}")
                    overall_errors.append(f"æ‰¹æ¬¡ {i+1}: {error_message}")
                    # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                    if task.error:
                        task.error += f"; æ‰¹æ¬¡ {i+1} å¤±è´¥: {error_message}"
                    else:
                        task.error = f"æ‰¹æ¬¡ {i+1} å¤±è´¥: {error_message}"

                    # æ ‡è®°å¤±è´¥çš„æ–‡ä»¶
                    for item_id, new_name in chunk.items():
                        all_results.append({
                            'id': item_id,
                            'type': 'file',
                            'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                            'new_name': new_name,
                            'status': 'failed',
                            'error': error_message
                        })

                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                task.processed_items += len(chunk)
                if task.total_items > 0:
                    task.progress = min(100, (task.processed_items / task.total_items) * 100)
                else:
                    task.progress = 100

            except Exception as e:
                overall_success = False
                error_msg = f"æ‰¹æ¬¡ {i+1} å¤„ç†å¼‚å¸¸: {str(e)}"
                logging.error(error_msg, exc_info=True)
                overall_errors.append(error_msg)
                # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                if task.error:
                    task.error += f"; {error_msg}"
                else:
                    task.error = error_msg

                # æ ‡è®°å¼‚å¸¸çš„æ–‡ä»¶
                for item_id, new_name in chunk.items():
                    all_results.append({
                        'id': item_id,
                        'type': 'file',
                        'original_name': original_names_map.get(item_id, 'æœªçŸ¥'),
                        'new_name': new_name,
                        'status': 'error',
                        'error': str(e)
                    })

        # å®Œæˆä»»åŠ¡
        if task.status != TaskStatus.CANCELLED:
            task.status = TaskStatus.COMPLETED if overall_success else TaskStatus.FAILED
            task.progress = 100
            task.completed_at = time.time()

        # è®°å½•æ€§èƒ½
        successful_renames = len([r for r in all_results if r.get('status') == 'success'])
        log_performance("apply_rename", start_time, overall_success,
                       total_files=len(namedict), successful_renames=successful_renames)

        logging.info(f"é‡å‘½åå®Œæˆ: æˆåŠŸ {successful_renames}/{len(namedict)} ä¸ªæ–‡ä»¶")

        # è¿”å›ç»“æœ
        if overall_success:
            return jsonify({
                'success': True,
                'results': all_results,
                'task_id': task_id,
                'performance': {
                    'duration': time.time() - start_time,
                    'files_per_second': len(namedict) / (time.time() - start_time) if time.time() - start_time > 0 else 0
                }
            })
        else:
            return jsonify({
                'success': False,
                'error': 'éƒ¨åˆ†æˆ–å…¨éƒ¨é‡å‘½åå¤±è´¥',
                'details': overall_errors,
                'results': all_results,
                'task_id': task_id
            })

    except Exception as e:
        log_performance("apply_rename", start_time, False, error=str(e))
        logging.error(f"åº”ç”¨é‡å‘½åå¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.error(f"åº”ç”¨é‡å‘½åæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/apply_move', methods=['POST'])
def apply_move():
    """åº”ç”¨ç§»åŠ¨ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒä»»åŠ¡ç®¡ç†å’Œæ€§èƒ½ç›‘æ§"""
    start_time = time.time()

    try:
        logging.info("å¼€å§‹åº”ç”¨ç§»åŠ¨")

        # è·å–ç§»åŠ¨æ•°æ®
        move_data_json = request.form.get('move_data')
        target_pid = request.form.get('target_pid', DEFAULT_PID)

        if not move_data_json:
            logging.warning("æ²¡æœ‰æä¾›ç§»åŠ¨æ•°æ®")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›ç§»åŠ¨æ•°æ®'})

        # è§£æç§»åŠ¨æ•°æ®
        try:
            move_data = json.loads(move_data_json)
        except json.JSONDecodeError:
            logging.error("ç§»åŠ¨æ•°æ®JSONæ— æ•ˆ")
            return jsonify({'success': False, 'error': 'ç§»åŠ¨æ•°æ®JSONæ— æ•ˆ'})

        if not move_data:
            logging.warning("æ²¡æœ‰æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹éœ€è¦ç§»åŠ¨")
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹éœ€è¦ç§»åŠ¨'})

        # åˆ†ç¦»æ–‡ä»¶å’Œæ–‡ä»¶å¤¹ID
        fids_to_move = [item['id'] for item in move_data if item.get('type') == 'file' and 'id' in item]
        cids_to_move = [item['id'] for item in move_data if item.get('type') == 'folder' and 'id' in item]

        total_items = len(fids_to_move) + len(cids_to_move)

        # åˆ›å»ºç§»åŠ¨ä»»åŠ¡
        task_id = f"apply_move_{int(time.time())}"
        task = GroupingTask(
            task_id=task_id,
            folder_id=str(target_pid),
            folder_name=f"æ‰¹é‡ç§»åŠ¨ {total_items} ä¸ªé¡¹ç›®"
        )
        task_manager.add_task(task)

        logging.info(f"å‡†å¤‡ç§»åŠ¨: {len(fids_to_move)} ä¸ªæ–‡ä»¶, {len(cids_to_move)} ä¸ªæ–‡ä»¶å¤¹åˆ° {target_pid}")

        all_move_results = []
        overall_success = True

        # ç§»åŠ¨æ–‡ä»¶
        if fids_to_move:
            logging.info(f"ç§»åŠ¨æ–‡ä»¶: {fids_to_move}")
            try:
                move_result = batch_move_file(fids_to_move, target_pid)
                success = move_result.get('success', False)

                all_move_results.append({
                    'type': 'files',
                    'ids': fids_to_move,
                    'count': len(fids_to_move),
                    'status': 'success' if success else 'failed',
                    'message': move_result.get('result') or move_result.get('error', 'æœªçŸ¥é”™è¯¯')
                })

                if not success:
                    overall_success = False
                    error_msg = f"æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {move_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                    if task.error:
                        task.error += f"; {error_msg}"
                    else:
                        task.error = error_msg

                task.processed_items += len(fids_to_move)

            except Exception as e:
                overall_success = False
                error_msg = f"æ–‡ä»¶ç§»åŠ¨å¼‚å¸¸: {str(e)}"
                logging.error(error_msg, exc_info=True)
                # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                if task.error:
                    task.error += f"; {error_msg}"
                else:
                    task.error = error_msg

                all_move_results.append({
                    'type': 'files',
                    'ids': fids_to_move,
                    'count': len(fids_to_move),
                    'status': 'error',
                    'message': error_msg
                })
        else:
            all_move_results.append({
                'type': 'files',
                'ids': [],
                'count': 0,
                'status': 'skipped',
                'message': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦ç§»åŠ¨'
            })

        # ç§»åŠ¨æ–‡ä»¶å¤¹
        if cids_to_move:
            logging.info(f"ç§»åŠ¨æ–‡ä»¶å¤¹: {cids_to_move}")
            try:
                # æ–‡ä»¶å¤¹ç§»åŠ¨ä½¿ç”¨ç›¸åŒçš„API
                move_result = batch_move_file(cids_to_move, target_pid)
                success = move_result.get('success', False)

                all_move_results.append({
                    'type': 'folders',
                    'ids': cids_to_move,
                    'count': len(cids_to_move),
                    'status': 'success' if success else 'failed',
                    'message': move_result.get('result') or move_result.get('error', 'æœªçŸ¥é”™è¯¯')
                })

                if not success:
                    overall_success = False
                    error_msg = f"æ–‡ä»¶å¤¹ç§»åŠ¨å¤±è´¥: {move_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                    if task.error:
                        task.error += f"; {error_msg}"
                    else:
                        task.error = error_msg

                task.processed_items += len(cids_to_move)

            except Exception as e:
                overall_success = False
                error_msg = f"æ–‡ä»¶å¤¹ç§»åŠ¨å¼‚å¸¸: {str(e)}"
                logging.error(error_msg, exc_info=True)
                # ç´¯ç§¯é”™è¯¯ä¿¡æ¯åˆ°task.error
                if task.error:
                    task.error += f"; {error_msg}"
                else:
                    task.error = error_msg

                all_move_results.append({
                    'type': 'folders',
                    'ids': cids_to_move,
                    'count': len(cids_to_move),
                    'status': 'error',
                    'message': error_msg
                })
        else:
            all_move_results.append({
                'type': 'folders',
                'ids': [],
                'count': 0,
                'status': 'skipped',
                'message': 'æ²¡æœ‰æ–‡ä»¶å¤¹éœ€è¦ç§»åŠ¨'
            })

        # å®Œæˆä»»åŠ¡
        task.status = "completed" if overall_success else "failed"
        task.progress = 100
        task.end_time = time.time()

        # è®°å½•æ€§èƒ½
        log_performance("apply_move", start_time, overall_success,
                       total_items=total_items, files_moved=len(fids_to_move), folders_moved=len(cids_to_move))

        logging.info(f"ç§»åŠ¨æ“ä½œå®Œæˆ: æˆåŠŸ={overall_success}, ç»“æœ={len(all_move_results)}")

        return jsonify({
            'success': overall_success,
            'message': 'ç§»åŠ¨æ“ä½œå®Œæˆ',
            'results': all_move_results,
            'task_id': task_id,
            'performance': {
                'duration': time.time() - start_time,
                'items_per_second': total_items / (time.time() - start_time) if time.time() - start_time > 0 else 0
            }
        })

    except Exception as e:
        log_performance("apply_move", start_time, False, error=str(e))
        logging.error(f"åº”ç”¨ç§»åŠ¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})
    except Exception as e:
        logging.error(f"åº”ç”¨ç§»åŠ¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

# ================================
# é«˜çº§åŠŸèƒ½APIç«¯ç‚¹
# ================================

# ç¼“å­˜ç›¸å…³è·¯ç”±å·²åˆ é™¤

# ç¼“å­˜æ¸…ç†ä»£ç å·²åˆ é™¤

@app.route('/api/tasks', methods=['GET'])
def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    try:
        tasks = task_manager.get_all_tasks()
        stats = task_manager.get_task_stats()
        return jsonify({
            'success': True,
            'tasks': tasks,
            'stats': stats
        })
    except Exception as e:
        logging.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/<task_id>', methods=['GET'])
def get_task_status(task_id):
    """è·å–ç‰¹å®šä»»åŠ¡çŠ¶æ€"""
    try:
        task = task_manager.get_task(task_id)
        if task:
            return jsonify({'success': True, 'task': task.to_dict()})
        else:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'})
    except Exception as e:
        logging.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/tasks/<task_id>/cancel', methods=['POST'])
def cancel_task(task_id):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        success = task_manager.cancel_task(task_id)
        if success:
            return jsonify({'success': True, 'message': 'ä»»åŠ¡å·²å–æ¶ˆ'})
        else:
            return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨æˆ–æ— æ³•å–æ¶ˆ'})
    except Exception as e:
        logging.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/performance/stats', methods=['GET'])
def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = performance_monitor.get_all_stats()
        return jsonify({'success': True, 'stats': stats})
    except Exception as e:
        logging.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/performance/reset', methods=['POST'])
def reset_performance_stats():
    """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
    try:
        performance_monitor.reset_stats()
        return jsonify({'success': True, 'message': 'æ€§èƒ½ç»Ÿè®¡å·²é‡ç½®'})
    except Exception as e:
        logging.error(f"é‡ç½®æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/ai/group', methods=['POST'])
def ai_group_files():
    """ä½¿ç”¨AIæ™ºèƒ½åˆ†ç»„æ–‡ä»¶"""
    try:
        data = request.get_json()
        files = data.get('files', [])

        if not files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›æ–‡ä»¶åˆ—è¡¨'})

        # åˆ›å»ºåˆ†ç»„ä»»åŠ¡
        task = task_manager.create_task("ai_grouping", "ai_grouping")
        task.start()

        # è¿™é‡Œåº”è¯¥å®ç°AIåˆ†ç»„é€»è¾‘
        # æš‚æ—¶è¿”å›ä»»åŠ¡IDï¼Œå®é™…åˆ†ç»„åœ¨åå°è¿›è¡Œ

        return jsonify({
            'success': True,
            'task_id': task.task_id,
            'message': 'AIåˆ†ç»„ä»»åŠ¡å·²å¯åŠ¨'
        })
    except Exception as e:
        logging.error(f"AIåˆ†ç»„å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        health_info = {
            'status': 'healthy',
            'timestamp': datetime.datetime.now().isoformat(),
            'version': '2.0.0',
            'performance_monitoring': PERFORMANCE_MONITORING,
            'ai_enabled': bool(GEMINI_API_KEY),
            'tmdb_enabled': bool(TMDB_API_KEY),
            'running_tasks': task_manager.get_running_tasks_count()
        }
        return jsonify(health_info)
    except Exception as e:
        logging.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.datetime.now().isoformat()
        }), 500

@app.route('/api/test_connection', methods=['POST'])
def test_connection():
    """æµ‹è¯•å„ç§APIè¿æ¥"""
    try:
        data = request.get_json()
        cookies = data.get('cookies', '')
        tmdb_api_key = data.get('tmdb_api_key', '')
        gemini_api_key = data.get('gemini_api_key', '')

        results = {
            '115äº‘ç›˜': 'unknown',
            'TMDB': 'unknown',
            'Gemini': 'unknown'
        }

        # æµ‹è¯•115äº‘ç›˜è¿æ¥
        if cookies:
            try:
                headers = {
                    'Cookie': cookies,
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get('https://webapi.115.com/files', headers=headers, timeout=10)
                if response.status_code == 200:
                    results['115äº‘ç›˜'] = 'success'
                else:
                    results['115äº‘ç›˜'] = 'failed'
            except Exception as e:
                results['115äº‘ç›˜'] = f'error: {str(e)}'

        # æµ‹è¯•TMDBè¿æ¥
        if tmdb_api_key:
            try:
                response = requests.get(f'https://api.themoviedb.org/3/configuration?api_key={tmdb_api_key}', timeout=10)
                if response.status_code == 200:
                    results['TMDB'] = 'success'
                else:
                    results['TMDB'] = 'failed'
            except Exception as e:
                results['TMDB'] = f'error: {str(e)}'

        # æµ‹è¯•Geminiè¿æ¥
        if gemini_api_key:
            try:
                gemini_url = app_config.get('GEMINI_API_URL', '')
                if gemini_url:
                    headers = {
                        'Authorization': f'Bearer {gemini_api_key}',
                        'Content-Type': 'application/json'
                    }
                    test_payload = {
                        "model": app_config.get('MODEL', 'gemini-2.5-flash-lite-preview-06-17-search'),
                        "messages": [{"role": "user", "content": "Hello"}],
                        "max_tokens": 10
                    }
                    response = requests.post(gemini_url, headers=headers, json=test_payload, timeout=10)
                    if response.status_code == 200:
                        results['Gemini'] = 'success'
                    else:
                        results['Gemini'] = 'failed'
                else:
                    results['Gemini'] = 'no_url'
            except Exception as e:
                results['Gemini'] = f'error: {str(e)}'

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æˆåŠŸçš„è¿æ¥
        success_count = sum(1 for status in results.values() if status == 'success')

        return jsonify({
            'success': success_count > 0,
            'results': results,
            'message': f'æµ‹è¯•å®Œæˆï¼Œ{success_count}/{len(results)}ä¸ªæœåŠ¡è¿æ¥æˆåŠŸ'
        })

    except Exception as e:
        logging.error(f"æµ‹è¯•è¿æ¥å¤±è´¥: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'è¿æ¥æµ‹è¯•å¤±è´¥'
        })

def get_115_folder_content(folder_id, limit=100):
    """è·å–115äº‘ç›˜æ–‡ä»¶å¤¹å†…å®¹"""
    try:
        url = 'https://webapi.115.com/files'
        params = {
            'aid': 1,
            'cid': folder_id,
            'o': 'user_ptime',
            'asc': 0,
            'offset': 0,
            'show_dir': 1,
            'limit': limit,
            'code': '',
            'scid': '',
            'snap': 0,
            'natsort': 1,
            'record_open_time': 1,
            'source': '',
            'format': 'json'
        }

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, params=params, headers=headers, timeout=10)
        if response.status_code == 200:
            return response.json()
        else:
            logging.error(f"è·å–115äº‘ç›˜æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        logging.error(f"è·å–115äº‘ç›˜æ–‡ä»¶å¤¹å†…å®¹å¼‚å¸¸: {e}")
        return None

def is_video_file(filename):
    """åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
    if not filename:
        return False

    video_extensions = {
        '.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v',
        '.3gp', '.3g2', '.asf', '.asx', '.divx', '.f4v', '.m2ts', '.m2v',
        '.mts', '.mxf', '.ogv', '.rm', '.rmvb', '.ts', '.vob', '.xvid'
    }

    return any(filename.lower().endswith(ext) for ext in video_extensions)

@app.route('/get_folder_grouping_analysis', methods=['POST'])
def get_folder_grouping_analysis():
    """è·å–æ–‡ä»¶å¤¹çš„æ™ºèƒ½åˆ†ç»„åˆ†æ"""
    try:
        folder_id = request.form.get('folder_id', '0')

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        try:
            folder_id = int(folder_id)
        except (ValueError, TypeError):
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—'})

        logging.info(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶å¤¹ {folder_id} çš„æ™ºèƒ½åˆ†ç»„")

        # è·å–æ–‡ä»¶å¤¹å†…å®¹
        try:
            files_data = get_115_folder_content(folder_id, limit=1000)
            if not files_data or 'data' not in files_data:
                return jsonify({'success': False, 'error': 'è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥'})

            # è¿‡æ»¤è§†é¢‘æ–‡ä»¶
            video_files = []
            for item in files_data['data']:
                if item.get('fid') and is_video_file(item.get('n', '')):
                    video_files.append({
                        'fid': item['fid'],
                        'name': item['n'],
                        'size': item.get('s', 0),
                        'path': item.get('path', ''),
                        'parent_id': folder_id
                    })

            if not video_files:
                return jsonify({
                    'success': True,
                    'movie_info': [],
                    'message': 'æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶'
                })

            logging.info(f"âœ… å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

            # ä½¿ç”¨AIè¿›è¡Œæ™ºèƒ½åˆ†ç»„åˆ†æ
            if not GEMINI_API_KEY or not GEMINI_API_URL:
                return jsonify({'success': False, 'error': 'AI APIæœªé…ç½®'})

            # æ„å»ºAIåˆ†æè¯·æ±‚
            file_names = [f['name'] for f in video_files]
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è§†é¢‘æ–‡ä»¶åˆ—è¡¨ï¼Œå°†å®ƒä»¬æŒ‰ç…§ç”µå½±/ç”µè§†å‰§è¿›è¡Œæ™ºèƒ½åˆ†ç»„ã€‚

æ–‡ä»¶åˆ—è¡¨ï¼š
{chr(10).join(file_names)}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†ç»„ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "groups": [
        {{
            "title": "ç”µå½±/å‰§é›†åç§°",
            "files": ["æ–‡ä»¶å1", "æ–‡ä»¶å2"],
            "type": "movie" æˆ– "tv",
            "year": "å¹´ä»½(å¦‚æœèƒ½è¯†åˆ«)",
            "description": "ç®€çŸ­æè¿°"
        }}
    ]
}}

è¦æ±‚ï¼š
1. ç›¸åŒç”µå½±/å‰§é›†çš„ä¸åŒç‰ˆæœ¬ã€ä¸åŒé›†æ•°åº”å½’ä¸ºä¸€ç»„
2. è¯†åˆ«ç”µå½±å¹´ä»½ã€åˆ†è¾¨ç‡ç­‰ä¿¡æ¯
3. åŒºåˆ†ç”µå½±(movie)å’Œç”µè§†å‰§(tv)
4. æ¯ç»„è‡³å°‘åŒ…å«1ä¸ªæ–‡ä»¶
5. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
"""

            headers = {
                'Authorization': f'Bearer {GEMINI_API_KEY}',
                'Content-Type': 'application/json'
            }

            payload = {
                "model": MODEL,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000,
                "temperature": 0.1
            }

            response = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=60)

            if response.status_code == 200:
                ai_response = response.json()
                content = ai_response.get('choices', [{}])[0].get('message', {}).get('content', '')

                try:
                    # è§£æAIè¿”å›çš„JSON
                    import json
                    grouping_data = json.loads(content)
                    groups = grouping_data.get('groups', [])

                    # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
                    movie_info = []
                    for group in groups:
                        group_files = []
                        for file_name in group.get('files', []):
                            # æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ä¿¡æ¯
                            for video_file in video_files:
                                if video_file['name'] == file_name:
                                    group_files.append(video_file)
                                    break

                        if group_files:  # åªæœ‰æ‰¾åˆ°æ–‡ä»¶çš„åˆ†ç»„æ‰æ·»åŠ 
                            movie_info.append({
                                'title': group.get('title', 'æœªçŸ¥'),
                                'files': group_files,
                                'type': group.get('type', 'movie'),
                                'year': group.get('year', ''),
                                'description': group.get('description', ''),
                                'file_count': len(group_files)
                            })

                    return jsonify({
                        'success': True,
                        'movie_info': movie_info,
                        'total_groups': len(movie_info),
                        'total_files': len(video_files)
                    })

                except json.JSONDecodeError as e:
                    logging.error(f"AIè¿”å›çš„JSONæ ¼å¼é”™è¯¯: {e}")
                    return jsonify({'success': False, 'error': 'AIåˆ†æç»“æœæ ¼å¼é”™è¯¯'})
            else:
                logging.error(f"AI APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return jsonify({'success': False, 'error': 'AIåˆ†æè¯·æ±‚å¤±è´¥'})

        except Exception as e:
            logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {e}")
            return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶å¤¹å†…å®¹å¤±è´¥: {str(e)}'})

    except Exception as e:
        logging.error(f"æ™ºèƒ½åˆ†ç»„åˆ†æå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/organize_files_by_groups', methods=['POST'])
def organize_files_by_groups():
    """æ ¹æ®æ™ºèƒ½åˆ†ç»„ç»“æœç§»åŠ¨æ–‡ä»¶"""
    try:
        folder_id = request.form.get('folder_id', '0')
        selected_groups_json = request.form.get('selected_groups')
        create_subfolders = request.form.get('create_subfolders', 'true').lower() == 'true'

        if not selected_groups_json:
            return jsonify({'success': False, 'error': 'æœªé€‰æ‹©åˆ†ç»„'})

        try:
            import json
            selected_groups = json.loads(selected_groups_json)
        except json.JSONDecodeError:
            return jsonify({'success': False, 'error': 'åˆ†ç»„æ•°æ®æ ¼å¼é”™è¯¯'})

        if not selected_groups:
            return jsonify({'success': False, 'error': 'æœªé€‰æ‹©ä»»ä½•åˆ†ç»„'})

        logging.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ™ºèƒ½åˆ†ç»„ï¼Œæ–‡ä»¶å¤¹ID: {folder_id}")

        results = []
        success_count = 0
        error_count = 0

        for group in selected_groups:
            group_title = group.get('title', 'æœªçŸ¥åˆ†ç»„')
            files = group.get('files', [])

            if not files:
                continue

            try:
                # å¦‚æœéœ€è¦åˆ›å»ºå­æ–‡ä»¶å¤¹
                if create_subfolders:
                    # åˆ›å»ºåˆ†ç»„æ–‡ä»¶å¤¹
                    folder_name = sanitize_folder_name(group_title)
                    create_result = create_115_folder(folder_name, folder_id)

                    if create_result and create_result.get('success'):
                        target_folder_id = create_result.get('folder_id')

                        # ç§»åŠ¨æ–‡ä»¶åˆ°æ–°æ–‡ä»¶å¤¹
                        for file_info in files:
                            try:
                                move_result = move_115_file(file_info['fid'], target_folder_id)
                                if move_result and move_result.get('success'):
                                    success_count += 1
                                    results.append({
                                        'file': file_info['name'],
                                        'group': group_title,
                                        'status': 'success',
                                        'message': f'å·²ç§»åŠ¨åˆ°æ–‡ä»¶å¤¹: {folder_name}'
                                    })
                                else:
                                    error_count += 1
                                    results.append({
                                        'file': file_info['name'],
                                        'group': group_title,
                                        'status': 'error',
                                        'message': 'ç§»åŠ¨æ–‡ä»¶å¤±è´¥'
                                    })
                            except Exception as e:
                                error_count += 1
                                results.append({
                                    'file': file_info['name'],
                                    'group': group_title,
                                    'status': 'error',
                                    'message': f'ç§»åŠ¨æ–‡ä»¶å¼‚å¸¸: {str(e)}'
                                })
                    else:
                        error_count += len(files)
                        for file_info in files:
                            results.append({
                                'file': file_info['name'],
                                'group': group_title,
                                'status': 'error',
                                'message': f'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {folder_name}'
                            })
                else:
                    # ä¸åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼Œåªæ˜¯æ ‡è®°å¤„ç†
                    for file_info in files:
                        success_count += 1
                        results.append({
                            'file': file_info['name'],
                            'group': group_title,
                            'status': 'success',
                            'message': 'åˆ†ç»„å®Œæˆï¼ˆæœªç§»åŠ¨æ–‡ä»¶ï¼‰'
                        })

            except Exception as e:
                error_count += len(files)
                for file_info in files:
                    results.append({
                        'file': file_info['name'],
                        'group': group_title,
                        'status': 'error',
                        'message': f'å¤„ç†åˆ†ç»„å¼‚å¸¸: {str(e)}'
                    })

        return jsonify({
            'success': True,
            'results': results,
            'summary': {
                'total_files': success_count + error_count,
                'success_count': success_count,
                'error_count': error_count,
                'groups_processed': len(selected_groups)
            }
        })

    except Exception as e:
        logging.error(f"æ‰§è¡Œæ™ºèƒ½åˆ†ç»„å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

def sanitize_folder_name(name):
    """æ¸…ç†æ–‡ä»¶å¤¹åç§°ï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    import re
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    name = re.sub(r'[<>:"/\\|?*]', '_', name)
    # ç§»é™¤å‰åç©ºæ ¼
    name = name.strip()
    # é™åˆ¶é•¿åº¦
    if len(name) > 100:
        name = name[:100]
    return name or 'æœªå‘½åæ–‡ä»¶å¤¹'

def create_115_folder(folder_name, parent_id):
    """åœ¨115äº‘ç›˜åˆ›å»ºæ–‡ä»¶å¤¹"""
    try:
        url = 'https://webapi.115.com/files/add'
        data = {
            'pid': parent_id,
            'cname': folder_name
        }

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.post(url, data=data, headers=headers, timeout=10)
        if response.status_code == 200:
            result = response.json()
            if result.get('state'):
                return {
                    'success': True,
                    'folder_id': result.get('cid'),
                    'folder_name': folder_name
                }

        return {'success': False, 'error': 'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥'}
    except Exception as e:
        logging.error(f"åˆ›å»º115äº‘ç›˜æ–‡ä»¶å¤¹å¼‚å¸¸: {e}")
        return {'success': False, 'error': str(e)}

def move_115_file(file_id, target_folder_id):
    """ç§»åŠ¨115äº‘ç›˜æ–‡ä»¶"""
    try:
        url = 'https://webapi.115.com/files/move'
        data = {
            'fid': file_id,
            'pid': target_folder_id
        }

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.post(url, data=data, headers=headers, timeout=10)
        if response.status_code == 200:
            result = response.json()
            if result.get('state'):
                return {'success': True}

        return {'success': False, 'error': 'ç§»åŠ¨æ–‡ä»¶å¤±è´¥'}
    except Exception as e:
        logging.error(f"ç§»åŠ¨115äº‘ç›˜æ–‡ä»¶å¼‚å¸¸: {e}")
        return {'success': False, 'error': str(e)}

def move_files_115(file_ids, target_folder_id):
    """æ‰¹é‡ç§»åŠ¨115äº‘ç›˜æ–‡ä»¶ - ä½¿ç”¨movie115çš„æ ¼å¼"""
    try:
        if not file_ids:
            return {'success': False, 'message': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦ç§»åŠ¨'}

        # ä½¿ç”¨æ‰¹é‡ç§»åŠ¨API
        url = 'https://webapi.115.com/files/move'

        # ä½¿ç”¨movie115çš„æ•°æ®æ ¼å¼
        data = {
            'move_proid': '1749745456990_-69_0',
            'pid': target_folder_id
        }

        # æ·»åŠ æ–‡ä»¶IDï¼Œä½¿ç”¨ fid[0], fid[1], fid[2] æ ¼å¼
        if isinstance(file_ids, str):
            data['fid[0]'] = file_ids
        else:
            for i in range(len(file_ids)):
                data[f'fid[{i}]'] = str(file_ids[i])

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        logging.info(f"ğŸ” ç§»åŠ¨æ–‡ä»¶è¯·æ±‚æ•°æ®: {data}")
        response = requests.post(url, data=data, headers=headers, timeout=10)
        logging.info(f"ğŸ” ç§»åŠ¨æ–‡ä»¶å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            logging.info(f"ğŸ” ç§»åŠ¨æ–‡ä»¶å“åº”å†…å®¹: {result}")
            if result.get('state'):
                return {'success': True, 'message': f'æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶'}
            else:
                return {'success': False, 'message': f'ç§»åŠ¨å¤±è´¥: {result.get("error", "æœªçŸ¥é”™è¯¯")}'}

        return {'success': False, 'message': f'HTTPé”™è¯¯: {response.status_code}'}
    except Exception as e:
        logging.error(f"æ‰¹é‡ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def rename_115_file(file_id, new_name):
    """é‡å‘½å115äº‘ç›˜æ–‡ä»¶ - ä½¿ç”¨movie115çš„æ­£ç¡®æ ¼å¼"""
    try:
        url = 'https://webapi.115.com/files/batch_rename'

        # ä½¿ç”¨movie115çš„æ­£ç¡®æ ¼å¼ï¼šfiles_new_name[file_id] = new_name
        data = {
            f'files_new_name[{file_id}]': new_name
        }

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        logging.info(f"ğŸ”„ é‡å‘½åæ–‡ä»¶ {file_id}: {new_name}")
        logging.info(f"ğŸ” é‡å‘½åè¯·æ±‚æ•°æ®: {data}")

        response = requests.post(url, data=data, headers=headers, timeout=10)

        # è¯¦ç»†æ‰“å°è¿”å›å€¼
        logging.info(f"ğŸ” é‡å‘½åAPIå“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            logging.info(f"ğŸ” é‡å‘½åAPIè¿”å›å€¼: {json.dumps(result, ensure_ascii=False, indent=2)}")

            if result.get('state'):
                logging.info(f"âœ… æ–‡ä»¶ {file_id} é‡å‘½åæˆåŠŸ: {new_name}")
                return {'success': True, 'message': 'æ–‡ä»¶é‡å‘½åæˆåŠŸ', 'result': result}
            else:
                error_msg = result.get('error') or result.get('errno_desc') or result.get('errcode') or 'é‡å‘½åå¤±è´¥'
                logging.error(f"âŒ æ–‡ä»¶ {file_id} é‡å‘½åå¤±è´¥: {error_msg}")
                return {'success': False, 'error': error_msg, 'result': result}
        else:
            logging.error(f"âŒ é‡å‘½åAPIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
            return {'success': False, 'error': f'HTTP {response.status_code}: {response.text}'}

    except Exception as e:
        logging.error(f"âŒ é‡å‘½å115äº‘ç›˜æ–‡ä»¶å¼‚å¸¸: {e}", exc_info=True)
        return {'success': False, 'error': str(e)}

def delete_files_115(file_ids):
    """æ‰¹é‡åˆ é™¤115äº‘ç›˜æ–‡ä»¶ - ä½¿ç”¨ç´¢å¼•æ ¼å¼"""
    try:
        if not file_ids:
            return {'success': False, 'message': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦åˆ é™¤'}

        # ä½¿ç”¨æ‰¹é‡åˆ é™¤API
        url = 'https://webapi.115.com/rb/delete'

        # ä½¿ç”¨ç´¢å¼•æ ¼å¼ï¼Œç±»ä¼¼move_files_115çš„å®ç°
        data = {}

        # æ·»åŠ æ–‡ä»¶IDï¼Œä½¿ç”¨ fid[0], fid[1], fid[2] æ ¼å¼
        if isinstance(file_ids, str):
            data['fid[0]'] = file_ids
        else:
            for i in range(len(file_ids)):
                data[f'fid[{i}]'] = str(file_ids[i])

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        logging.info(f"ğŸ” åˆ é™¤æ–‡ä»¶è¯·æ±‚æ•°æ®: {data}")
        response = requests.post(url, data=data, headers=headers, timeout=10)
        logging.info(f"ğŸ” åˆ é™¤æ–‡ä»¶å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            logging.info(f"ğŸ” åˆ é™¤æ–‡ä»¶å“åº”å†…å®¹: {result}")
            if result.get('state'):
                return {'success': True, 'message': f'æˆåŠŸåˆ é™¤ {len(file_ids)} ä¸ªæ–‡ä»¶'}
            else:
                return {'success': False, 'message': f'åˆ é™¤å¤±è´¥: {result.get("error", "æœªçŸ¥é”™è¯¯")}'}

        return {'success': False, 'message': f'HTTPé”™è¯¯: {response.status_code}'}
    except Exception as e:
        logging.error(f"æ‰¹é‡åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        return {'success': False, 'error': str(e)}

def find_existing_folder_by_name(folder_name, parent_id='0'):
    """æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹çš„IDï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…"""
    try:
        logging.info(f"ğŸ” åœ¨çˆ¶æ–‡ä»¶å¤¹ {parent_id} ä¸­æŸ¥æ‰¾æ–‡ä»¶å¤¹: {folder_name}")
        # è·å–çˆ¶ç›®å½•çš„å†…å®¹
        content = _get_single_level_content_from_115(parent_id, 0, 100)
        if content and 'data' in content:
            logging.info(f"ğŸ” çˆ¶æ–‡ä»¶å¤¹åŒ…å« {len(content['data'])} ä¸ªé¡¹ç›®")

            # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
            for item in content['data']:
                item_name = item.get('n', '')
                # å°è¯•å¤šä¸ªå¯èƒ½çš„IDå­—æ®µ
                item_fid = item.get('fid', '') or item.get('cid', '') or item.get('id', '') or item.get('file_id', '')
                item_is_dir = item.get('pid', '') != '' or item.get('is_dir', False) or item.get('d', False)
                logging.info(f"ğŸ” æ£€æŸ¥é¡¹ç›®: {item_name} (fid: {item_fid}, is_dir: {item_is_dir})")
                logging.info(f"ğŸ” é¡¹ç›®å®Œæ•´æ•°æ®: {item}")
                if item_name == folder_name and item_fid and item_is_dir:
                    logging.info(f"âœ… ç²¾ç¡®åŒ¹é…æ‰¾åˆ°æ–‡ä»¶å¤¹: {folder_name} -> {item_fid}")
                    return item_fid

            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            logging.info(f"ğŸ” ç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…...")
            folder_name_lower = folder_name.lower()
            best_match = None
            best_score = 0

            for item in content['data']:
                item_name = item.get('n', '')
                # å°è¯•å¤šä¸ªå¯èƒ½çš„IDå­—æ®µ
                item_fid = item.get('fid', '') or item.get('cid', '') or item.get('id', '') or item.get('file_id', '')
                item_is_dir = item.get('pid', '') != '' or item.get('is_dir', False) or item.get('d', False)
                if not item_fid or not item_is_dir:  # åªè€ƒè™‘æ–‡ä»¶å¤¹
                    continue

                item_name_lower = item_name.lower()

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                if 'mission impossible' in folder_name_lower and 'mission impossible' in item_name_lower:
                    score = 0.8
                    logging.info(f"ğŸ” Mission Impossible åŒ¹é…: {item_name} (åˆ†æ•°: {score})")
                elif 'love, death' in folder_name_lower and 'love, death' in item_name_lower:
                    score = 0.8
                    logging.info(f"ğŸ” Love, Death & Robots åŒ¹é…: {item_name} (åˆ†æ•°: {score})")
                elif folder_name_lower in item_name_lower or item_name_lower in folder_name_lower:
                    score = 0.6
                    logging.info(f"ğŸ” åŒ…å«åŒ¹é…: {item_name} (åˆ†æ•°: {score})")
                else:
                    continue

                if score > best_score:
                    best_match = item_fid
                    best_score = score
                    logging.info(f"ğŸ” æ›´æ–°æœ€ä½³åŒ¹é…: {item_name} -> {item_fid} (åˆ†æ•°: {score})")

            if best_match and best_score >= 0.6:
                logging.info(f"âœ… æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°æ–‡ä»¶å¤¹: {folder_name} -> {best_match} (åˆ†æ•°: {best_score})")
                return best_match

            logging.info(f"âŒ åœ¨çˆ¶æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹: {folder_name}")
        else:
            logging.info(f"âŒ æ— æ³•è·å–çˆ¶æ–‡ä»¶å¤¹å†…å®¹æˆ–å†…å®¹ä¸ºç©º")
        return None
    except Exception as e:
        logging.error(f"æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹å¤±è´¥: {e}")
        return None

def create_folder_115(folder_name, parent_id):
    """åœ¨115äº‘ç›˜åˆ›å»ºæ–‡ä»¶å¤¹"""
    try:
        url = 'https://webapi.115.com/files/add'
        data = {
            'pid': parent_id,
            'cname': folder_name
        }

        headers = {
            'Cookie': COOKIES,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.post(url, data=data, headers=headers, timeout=10)
        logging.info(f"ğŸ” åˆ›å»ºæ–‡ä»¶å¤¹å“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            result = response.json()
            logging.info(f"ğŸ” åˆ›å»ºæ–‡ä»¶å¤¹å“åº”å†…å®¹: {result}")
            if result.get('state'):
                return {
                    'success': True,
                    'message': f'æˆåŠŸåˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}',
                    'folder_id': result.get('cid')
                }
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶å¤¹å·²å­˜åœ¨çš„é”™è¯¯
                if result.get('errno') == 20004 or 'å·²å­˜åœ¨' in result.get('error', ''):
                    logging.info(f"ğŸ” æ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹: {folder_name}")
                    # å°è¯•æŸ¥æ‰¾ç°æœ‰æ–‡ä»¶å¤¹
                    existing_folder_id = find_existing_folder_by_name(folder_name, parent_id)
                    if existing_folder_id:
                        logging.info(f"âœ… æ‰¾åˆ°ç°æœ‰æ–‡ä»¶å¤¹: {folder_name}, ID: {existing_folder_id}")
                        return {
                            'success': True,
                            'message': f'ä½¿ç”¨ç°æœ‰æ–‡ä»¶å¤¹: {folder_name}',
                            'folder_id': existing_folder_id
                        }
                    else:
                        logging.error(f"âŒ æ— æ³•æ‰¾åˆ°ç°æœ‰æ–‡ä»¶å¤¹: {folder_name}")
                        return {'success': False, 'error': f'æ–‡ä»¶å¤¹å·²å­˜åœ¨ä½†æ— æ³•æ‰¾åˆ°: {folder_name}'}
                else:
                    return {'success': False, 'error': f'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {result.get("error", "æœªçŸ¥é”™è¯¯")}'}

        return {'success': False, 'error': f'HTTPé”™è¯¯: {response.status_code}'}
    except Exception as e:
        logging.error(f"åˆ›å»º115äº‘ç›˜æ–‡ä»¶å¤¹å¼‚å¸¸: {e}")
        return {'success': False, 'error': str(e)}

def generate_folder_name_suggestion(file_names):
    """ä½¿ç”¨AIç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®"""
    try:
        if not GEMINI_API_KEY or not GEMINI_API_URL:
            logging.warning("AI APIæœªé…ç½®ï¼Œæ— æ³•ç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®")
            return None

        if not file_names:
            return None

        # æ„å»ºAIåˆ†æè¯·æ±‚
        prompt = f"""
æ ¹æ®ä»¥ä¸‹æ–‡ä»¶ååˆ—è¡¨ï¼Œä¸ºè¿™äº›æ–‡ä»¶ç”Ÿæˆä¸€ä¸ªåˆé€‚çš„æ–‡ä»¶å¤¹åç§°å»ºè®®ã€‚

æ–‡ä»¶ååˆ—è¡¨ï¼š
{chr(10).join(file_names)}

è¦æ±‚ï¼š
1. åˆ†ææ–‡ä»¶åçš„å…±åŒç‰¹å¾ï¼ˆå¦‚ç”µå½±åã€ç”µè§†å‰§åã€ç±»å‹ç­‰ï¼‰
2. ç”Ÿæˆä¸€ä¸ªç®€æ´ã€æè¿°æ€§çš„æ–‡ä»¶å¤¹åç§°
3. æ–‡ä»¶å¤¹åç§°åº”è¯¥æ˜¯ä¸­æ–‡ï¼Œé•¿åº¦ä¸è¶…è¿‡50ä¸ªå­—ç¬¦
4. åªè¿”å›æ–‡ä»¶å¤¹åç§°ï¼Œä¸è¦å…¶ä»–æ–‡å­—
5. å¦‚æœæ˜¯ç”µå½±æˆ–ç”µè§†å‰§ï¼ŒåŒ…å«å¹´ä»½ä¿¡æ¯ï¼ˆå¦‚æœèƒ½è¯†åˆ«å‡ºæ¥ï¼‰

ç¤ºä¾‹ï¼š
- å¦‚æœæ˜¯ã€Šå¤ä»‡è€…è”ç›Ÿã€‹ç›¸å…³æ–‡ä»¶ï¼Œè¿”å›ï¼šå¤ä»‡è€…è”ç›Ÿç³»åˆ—
- å¦‚æœæ˜¯2023å¹´çš„ç”µå½±ï¼Œè¿”å›ï¼šç”µå½±åç§° (2023)
- å¦‚æœæ˜¯ç”µè§†å‰§ï¼Œè¿”å›ï¼šç”µè§†å‰§åç§° å…¨é›†

è¯·ç›´æ¥è¿”å›å»ºè®®çš„æ–‡ä»¶å¤¹åç§°ï¼š
"""

        headers = {
            'Authorization': f'Bearer {GEMINI_API_KEY}',
            'Content-Type': 'application/json'
        }

        payload = {
            "model": MODEL,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 100,
            "temperature": 0.3
        }

        response = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=15)

        if response.status_code == 200:
            ai_response = response.json()
            content = ai_response.get('choices', [{}])[0].get('message', {}).get('content', '').strip()

            if content:
                # æ¸…ç†è¿”å›çš„å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¼•å·å’Œå¤šä½™çš„æ–‡å­—
                suggested_name = content.replace('"', '').replace("'", '').strip()

                # é™åˆ¶é•¿åº¦
                if len(suggested_name) > 50:
                    suggested_name = suggested_name[:50]

                logging.info(f"AIç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®: {suggested_name}")
                return suggested_name

        logging.warning("AIæœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æ–‡ä»¶å¤¹åç§°å»ºè®®")
        return None

    except Exception as e:
        logging.error(f"ç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®å¤±è´¥: {e}")
        return None

def find_empty_folders_115(parent_id, visited_folders=None, max_depth=10, current_depth=0):
    """æŸ¥æ‰¾115äº‘ç›˜ä¸­ä¸åŒ…å«è§†é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹

    åˆ é™¤ä¸åŒ…å«è§†é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼ˆå¯èƒ½åŒ…å«.nfoã€.jpgç­‰éè§†é¢‘æ–‡ä»¶ï¼‰
    ä¿ç•™åŒ…å«è§†é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹
    """
    try:
        # åˆå§‹åŒ–è®¿é—®è¿‡çš„æ–‡ä»¶å¤¹é›†åˆ
        if visited_folders is None:
            visited_folders = set()

        # é˜²æ­¢æ— é™é€’å½’
        if current_depth >= max_depth:
            logging.warning(f"è¾¾åˆ°æœ€å¤§é€’å½’æ·±åº¦ {max_depth}ï¼Œåœæ­¢é€’å½’")
            return []

        # é˜²æ­¢é‡å¤è®¿é—®åŒä¸€æ–‡ä»¶å¤¹
        if parent_id in visited_folders:
            logging.warning(f"æ–‡ä»¶å¤¹ {parent_id} å·²è®¿é—®è¿‡ï¼Œè·³è¿‡ä»¥é˜²å¾ªç¯")
            return []

        visited_folders.add(parent_id)
        empty_folders = []

        logging.info(f"ğŸ” æ‰«ææ–‡ä»¶å¤¹ {parent_id} æŸ¥æ‰¾ç©ºæ–‡ä»¶å¤¹ (æ·±åº¦: {current_depth})")

        # QPSé™åˆ¶
        qps_limiter.acquire()

        # è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„å†…å®¹
        response_data = _get_single_level_content_from_115(parent_id, 0, 1000)

        if not response_data or 'data' not in response_data:
            return empty_folders

        items = response_data.get('data', [])
        logging.info(f"ğŸ” æ–‡ä»¶å¤¹ {parent_id} åŒ…å« {len(items)} ä¸ªé¡¹ç›®")

        # åˆ†ç¦»æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘æ–‡ä»¶
        subfolders = []
        has_video_files = False

        # åˆ†ç¦»æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹
        files = []

        # éå†æ‰€æœ‰é¡¹ç›®ï¼Œæ­£ç¡®åŒºåˆ†æ–‡ä»¶å’Œæ–‡ä»¶å¤¹
        for item in items:
            file_name = item.get('n', 'æœªçŸ¥é¡¹ç›®')
            file_size = item.get('s', 0)
            has_pid = 'pid' in item and item.get('pid', '') != ''
            ico = item.get('ico', '')

            # æ›´ä¸¥æ ¼çš„æ–‡ä»¶å¤¹åˆ¤æ–­ï¼šæœ‰pidå­—æ®µï¼Œæ–‡ä»¶å¤§å°ä¸º0ï¼Œä¸”æ²¡æœ‰æ–‡ä»¶ç±»å‹å›¾æ ‡
            is_folder = (has_pid and file_size == 0 and (not ico or ico in ['folder', 'dir']))

            if is_folder:
                # è¿™æ˜¯ä¸€ä¸ªå­æ–‡ä»¶å¤¹
                folder_id = item.get('fid', '') or item.get('cid', '')
                if folder_id:
                    subfolders.append({
                        'id': folder_id,
                        'name': file_name
                    })
                    logging.info(f"ğŸ“ å‘ç°å­æ–‡ä»¶å¤¹: {file_name} (ID: {folder_id})")
            else:
                # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶
                files.append({
                    'name': file_name,
                    'size': file_size,
                    'ico': ico
                })
                logging.info(f"ğŸ“„ å‘ç°æ–‡ä»¶: {file_name} (å¤§å°: {file_size}, ç±»å‹: {ico})")

                # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                if is_video_file(file_name):
                    has_video_files = True

        # å…ˆé€’å½’æ£€æŸ¥æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for subfolder in subfolders:
            folder_id = subfolder['id']

            # é˜²æ­¢é‡å¤è®¿é—®
            if folder_id in visited_folders:
                logging.info(f"ğŸ” æ–‡ä»¶å¤¹ {folder_id} å·²è®¿é—®è¿‡ï¼Œè·³è¿‡")
                continue

            # é€’å½’æ£€æŸ¥å­æ–‡ä»¶å¤¹
            sub_empty_folders = find_empty_folders_115(
                folder_id,
                visited_folders,
                max_depth,
                current_depth + 1
            )
            empty_folders.extend(sub_empty_folders)

        # æ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹æ˜¯å¦ä¸åŒ…å«è§†é¢‘æ–‡ä»¶ä¸”ä¸ºå¶å­èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å­æ–‡ä»¶å¤¹ï¼‰
        is_leaf_node = len(subfolders) == 0
        if not has_video_files and is_leaf_node:
            empty_folders.append({
                'cid': parent_id,
                'name': f'æ— è§†é¢‘æ–‡ä»¶å¤¹_{parent_id}',
                'depth': current_depth
            })
            logging.info(f"ğŸ—‘ï¸ å‘ç°æ— è§†é¢‘å¶å­æ–‡ä»¶å¤¹: {parent_id} (æ·±åº¦: {current_depth}, æ–‡ä»¶æ•°: {len(files)})")
        elif not has_video_files and not is_leaf_node:
            logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ {parent_id} æ— è§†é¢‘æ–‡ä»¶ä½†åŒ…å«å­æ–‡ä»¶å¤¹ï¼Œè·³è¿‡åˆ é™¤ (æ–‡ä»¶æ•°: {len(files)}, å­æ–‡ä»¶å¤¹æ•°: {len(subfolders)})")
        else:
            logging.info(f"ğŸ“ æ–‡ä»¶å¤¹ {parent_id} åŒ…å«è§†é¢‘æ–‡ä»¶ï¼Œä¿ç•™ (æ–‡ä»¶æ•°: {len(files)}, å­æ–‡ä»¶å¤¹æ•°: {len(subfolders)})")

        logging.info(f"âœ… æ–‡ä»¶å¤¹ {parent_id} æ‰«æå®Œæˆï¼Œå‘ç° {len(empty_folders)} ä¸ªç©ºæ–‡ä»¶å¤¹")
        return empty_folders

    except Exception as e:
        logging.error(f"æŸ¥æ‰¾ç©ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
        return []

def restart_process_async():
    # å»¶è¿Ÿä¸€ç‚¹æ—¶é—´ï¼Œç¡®ä¿HTTPå“åº”å¯ä»¥å‘é€
    time.sleep(1) 
    print("Initiating new process...")
    try:
        # è·å–å½“å‰çš„å¯åŠ¨å‘½ä»¤
        # å‡è®¾ä½ é€šå¸¸ç”¨ 'python app.py' å¯åŠ¨
        command = [sys.executable] + sys.argv
        
        # ä½¿ç”¨ subprocess.Popen å¯åŠ¨æ–°è¿›ç¨‹ï¼Œä¸ç­‰å¾…å®ƒå®Œæˆ
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰å¤„ç†æ ‡å‡†è¾“å…¥/è¾“å‡ºé‡å®šå‘ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µæ·»åŠ 
        subprocess.Popen(command, 
                         # è¿™ä¸¤è¡Œå¾ˆé‡è¦ï¼Œè§£è€¦å­è¿›ç¨‹å’Œçˆ¶è¿›ç¨‹çš„stdout/stderr
                         # å¦åˆ™å­è¿›ç¨‹çš„è¾“å‡ºä¼šç»§ç»­è¾“å‡ºåˆ°æ—§è¿›ç¨‹çš„ç»ˆç«¯
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL,
                         # åœ¨Unix/Linuxä¸Šï¼Œè®¾ç½®preexec_fnå¯ä»¥è¿›ä¸€æ­¥è§£è€¦
                         # ä¾‹å¦‚ï¼špreexec_fn=os.setsid
                         ) 
        print("New process started. Exiting old process.")
        os._exit(0) # å¼ºåˆ¶é€€å‡ºå½“å‰è¿›ç¨‹ï¼Œä¸æ‰§è¡Œatexit handlers
    except Exception as e:
        print(f"Failed to start new process: {e}")

@app.route('/get_file_list', methods=['POST'])
def get_file_list():
    """è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨ï¼ˆé€’å½’ï¼‰"""
    try:
        folder_id = request.form.get('folder_id', '0')

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            folder_id = '0'

        try:
            folder_id = int(folder_id)
        except (ValueError, TypeError):
            logging.warning(f"æ— æ•ˆçš„folder_idå€¼: {folder_id}ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
            folder_id = 0

        logging.info(f"è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨")

        file_list = []
        get_video_files_recursively(folder_id, file_list)

        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        formatted_files = []
        for file_item in file_list:
            formatted_files.append({
                'parentFileId': file_item['parentFileId'],
                'fileId': file_item['fileId'],
                'filename': os.path.basename(file_item['file_path']),  # åªä¿ç•™æ–‡ä»¶å
                'file_name': file_item['file_path'],  # å®Œæ•´è·¯å¾„
                'size': file_item['size_gb'],
            })

        logging.info(f"æ‰¾åˆ° {len(formatted_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        return jsonify({
            'success': True,
            'files': formatted_files,
            'total_count': len(formatted_files)
        })

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/rename_files', methods=['POST'])
def rename_files():
    """é‡å‘½åé€‰ä¸­çš„æ–‡ä»¶"""
    try:
        rename_data_json = request.form.get('rename_data')
        if not rename_data_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›é‡å‘½åæ•°æ®ã€‚'})

        rename_data = json.loads(rename_data_json)
        if not rename_data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦é‡å‘½åã€‚'})

        logging.info(f"å‡†å¤‡é‡å‘½å {len(rename_data)} ä¸ªæ–‡ä»¶")

        successful_renames = []
        failed_renames = []

        for item in rename_data:
            file_id = item.get('fileId')
            # æ”¯æŒå¤šç§æ ¼å¼ï¼šnewNameï¼ˆæ™®é€šé‡å‘½åï¼‰ã€suggested_nameï¼ˆåˆ®å‰Šé¢„è§ˆé‡å‘½åï¼‰ã€new_nameï¼ˆæ™ºèƒ½é‡å‘½åï¼‰
            new_name = item.get('newName') or item.get('suggested_name') or item.get('new_name')

            if file_id and new_name:
                try:
                    result = rename_115_file(file_id, new_name)
                    if result.get('success'):
                        successful_renames.append({'fileId': file_id, 'newName': new_name})
                    else:
                        failed_renames.append({'fileId': file_id, 'error': result.get('error', 'é‡å‘½åå¤±è´¥')})
                except Exception as e:
                    failed_renames.append({'fileId': file_id, 'error': str(e)})

        total_files = len(rename_data)
        success_count = len(successful_renames)
        failed_count = len(failed_renames)

        logging.info(f"é‡å‘½åå®Œæˆ: æˆåŠŸ {success_count}/{total_files}, å¤±è´¥ {failed_count}")

        # å¦‚æœæœ‰æˆåŠŸçš„é‡å‘½åï¼Œæ¸…ç†ç¼“å­˜
        if success_count > 0:
            logging.info(f"ğŸ§¹ é‡å‘½åæ“ä½œå®Œæˆï¼Œå·²æ¸…ç†ç›¸å…³ç¼“å­˜")

        response_data = {
            'success': failed_count == 0,
            'message': f'é‡å‘½åå®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª',
            'successful_renames': successful_renames,
            'failed_renames': failed_renames,
            'total_files': total_files,
            'success_count': success_count,
            'failed_count': failed_count
        }

        # å¦‚æœæœ‰æˆåŠŸçš„é‡å‘½åï¼Œæ·»åŠ åˆ·æ–°æŒ‡ä»¤
        if success_count > 0:
            response_data['refresh_folder'] = True
            logging.info(f"âœ… é‡å‘½åæˆåŠŸï¼Œå°†è§¦å‘æ–‡ä»¶å¤¹åˆ·æ–°")

        return jsonify(response_data)

    except json.JSONDecodeError as e:
        logging.error(f"è§£æé‡å‘½åæ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'JSONè§£æé”™è¯¯: {str(e)}'})
    except Exception as e:
        logging.error(f"é‡å‘½åæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/delete_files', methods=['POST'])
def delete_files():
    """åˆ é™¤é€‰ä¸­çš„æ–‡ä»¶"""
    try:
        # å°è¯•è·å–å‰ç«¯å‘é€çš„delete_dataå‚æ•°
        delete_data_json = request.form.get('delete_data')
        file_ids_json = request.form.get('file_ids')

        if delete_data_json:
            # å‰ç«¯å‘é€çš„æ˜¯åŒ…å«fileIdã€fileNameã€isDirçš„å¯¹è±¡æ•°ç»„
            delete_data = json.loads(delete_data_json)
            if not delete_data:
                return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦åˆ é™¤ã€‚'})

            # æå–æ–‡ä»¶ID
            file_ids = [item['fileId'] for item in delete_data if item.get('fileId')]
            file_names = [item['fileName'] for item in delete_data if item.get('fileName')]

        elif file_ids_json:
            # å…¼å®¹æ—§çš„file_idså‚æ•°æ ¼å¼
            file_ids = json.loads(file_ids_json)
            file_names = []
        else:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›æ–‡ä»¶IDã€‚'})

        if not file_ids:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦åˆ é™¤ã€‚'})

        logging.info(f"å‡†å¤‡åˆ é™¤ {len(file_ids)} ä¸ªæ–‡ä»¶: {file_names[:3]}{'...' if len(file_names) > 3 else ''}")

        # ä½¿ç”¨115äº‘ç›˜åˆ é™¤API
        result = delete_files_115(file_ids)

        if result.get('success'):
            # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜æ¸…ç†æœºåˆ¶

            return jsonify({
                'success': True,
                'message': f'æˆåŠŸåˆ é™¤ {len(file_ids)} ä¸ªæ–‡ä»¶',
                'deleted_count': len(file_ids)
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'åˆ é™¤æ–‡ä»¶å¤±è´¥')
            })

    except json.JSONDecodeError as e:
        logging.error(f"è§£æåˆ é™¤æ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'JSONè§£æé”™è¯¯: {str(e)}'})
    except Exception as e:
        logging.error(f"åˆ é™¤æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/create_folder', methods=['POST'])
def create_folder():
    """åˆ›å»ºæ–°æ–‡ä»¶å¤¹"""
    try:
        folder_name = request.form.get('folder_name')
        parent_id = request.form.get('parent_id', '0')  # é»˜è®¤åœ¨æ ¹ç›®å½•åˆ›å»º

        if not folder_name:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤¹åç§°ä¸èƒ½ä¸ºç©ºã€‚'})

        logging.info(f"å‡†å¤‡åˆ›å»ºæ–‡ä»¶å¤¹: {folder_name} åœ¨çˆ¶ç›®å½•: {parent_id}")

        # ä½¿ç”¨115äº‘ç›˜åˆ›å»ºæ–‡ä»¶å¤¹API
        result = create_folder_115(folder_name, parent_id)

        if result.get('success'):
            # ç¼“å­˜æ¸…ç†é€»è¾‘å·²åˆ é™¤
            logging.info(f"åˆ›å»ºæ–‡ä»¶å¤¹æˆåŠŸ: {folder_name}")

            return jsonify({
                'success': True,
                'message': f'æˆåŠŸåˆ›å»ºæ–‡ä»¶å¤¹: {folder_name}',
                'folder_id': result.get('folder_id'),
                'folder_name': folder_name
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥')
            })

    except Exception as e:
        logging.error(f"åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/suggest_folder_name', methods=['POST'])
def suggest_folder_name():
    """æ ¹æ®æ–‡ä»¶å¤¹å†…å®¹æ™ºèƒ½å»ºè®®æ–‡ä»¶å¤¹åç§°"""
    try:
        folder_id_str = request.form.get('folder_id', '0')

        if not folder_id_str:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘folder_idå‚æ•°'})

        try:
            folder_id = int(folder_id_str)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶å¤¹ID'})

        logging.info(f"ä¸ºæ–‡ä»¶å¤¹ {folder_id} ç”Ÿæˆæ™ºèƒ½å‘½åå»ºè®®")

        # è·å–æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘æ–‡ä»¶
        video_files = []

        # å…ˆæ£€æŸ¥æ–‡ä»¶å¤¹å†…å®¹
        all_files = get_all_files_in_folder(folder_id, limit=100)
        logging.info(f"ğŸ” æ–‡ä»¶å¤¹ {folder_id} ä¸­å…±æœ‰ {len(all_files)} ä¸ªé¡¹ç›®")

        # è¯¦ç»†è®°å½•æ–‡ä»¶ç±»å‹
        file_count = 0
        folder_count = 0
        video_count = 0

        for item in all_files:
            if item['type'] == 0:  # æ–‡ä»¶
                file_count += 1
                _, ext = os.path.splitext(item['filename'])
                ext_lower = ext.lower()[1:]
                logging.info(f"ğŸ“„ æ–‡ä»¶: {item['filename']}, æ‰©å±•å: {ext_lower}")
                if ext_lower in SUPPORTED_MEDIA_TYPES:
                    video_count += 1
                    logging.info(f"âœ… è¯†åˆ«ä¸ºè§†é¢‘æ–‡ä»¶: {item['filename']}")
            elif item['type'] == 1:  # æ–‡ä»¶å¤¹
                folder_count += 1
                logging.info(f"ğŸ“ æ–‡ä»¶å¤¹: {item['filename']}")

        logging.info(f"ğŸ“Š ç»Ÿè®¡: æ–‡ä»¶ {file_count} ä¸ª, æ–‡ä»¶å¤¹ {folder_count} ä¸ª, è§†é¢‘æ–‡ä»¶ {video_count} ä¸ª")

        get_video_files_recursively(folder_id, video_files)

        if not video_files:
            return jsonify({'success': False, 'error': f'æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶ (å…±æ£€æŸ¥äº† {len(all_files)} ä¸ªé¡¹ç›®)'})

        # æå–æ–‡ä»¶åç”¨äºAIåˆ†æï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜æ€§èƒ½ï¼‰
        sampled_files = video_files[:20] if len(video_files) > 20 else video_files
        file_names = [os.path.basename(file_item['file_path']) for file_item in sampled_files]

        if not file_names:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶åã€‚'})

        # ä½¿ç”¨AIç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®
        suggested_name = generate_folder_name_suggestion(file_names)

        if suggested_name:
            return jsonify({
                'success': True,
                'suggested_name': suggested_name,
                'file_count': len(video_files)
            })
        else:
            return jsonify({
                'success': False,
                'error': 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆå»ºè®®'
            })

    except Exception as e:
        logging.error(f"ç”Ÿæˆæ–‡ä»¶å¤¹åç§°å»ºè®®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/execute_selected_groups', methods=['POST'])
def execute_selected_groups():
    """æ‰§è¡Œé€‰ä¸­çš„æ™ºèƒ½åˆ†ç»„"""
    try:
        # è·å–ç›®æ ‡æ–‡ä»¶å¤¹ID
        target_parent_folder_id = request.form.get('folder_id', '0')
        logging.info(f"ğŸ” ç›®æ ‡çˆ¶æ–‡ä»¶å¤¹ID: {target_parent_folder_id}")

        # å…¼å®¹ä¸¤ç§å‚æ•°åï¼šgroups å’Œ selected_groups
        groups_json = request.form.get('groups') or request.form.get('selected_groups')
        if not groups_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›åˆ†ç»„ä¿¡æ¯ã€‚'})

        groups = json.loads(groups_json)
        if not groups:
            return jsonify({'success': False, 'error': 'åˆ†ç»„åˆ—è¡¨ä¸ºç©ºã€‚'})

        logging.info(f"å‡†å¤‡æ‰§è¡Œ {len(groups)} ä¸ªæ™ºèƒ½åˆ†ç»„")
        logging.info(f"ğŸ” è°ƒè¯• - æ¥æ”¶åˆ°çš„åˆ†ç»„æ•°æ®: {groups}")  # æ‰“å°å®Œæ•´çš„åˆ†ç»„æ•°æ®

        successful_groups = []
        failed_groups = []

        for group in groups:
            # å…¼å®¹ä¸¤ç§å­—æ®µåï¼šname å’Œ group_name
            group_name = group.get('name', '') or group.get('group_name', '')
            files = group.get('files', [])
            target_folder_id = group.get('target_folder_id')

            # æ£€æŸ¥æ˜¯å¦æœ‰ fileIds å­—æ®µï¼ˆå‰ç«¯å‘é€çš„æ–‡ä»¶IDåˆ—è¡¨ï¼‰
            file_ids_from_group = group.get('fileIds', [])
            if file_ids_from_group:
                logging.info(f"ğŸ” ä»åˆ†ç»„ä¸­ç›´æ¥è·å–æ–‡ä»¶ID: {file_ids_from_group}")
                # å¦‚æœæœ‰ fileIds å­—æ®µï¼Œå°†å…¶è½¬æ¢ä¸º files æ ¼å¼
                files = [{'fid': fid} for fid in file_ids_from_group]
                logging.info(f"ğŸ” è½¬æ¢åçš„ files ç»“æ„: {files}")
            elif files and isinstance(files[0], str):
                # å¦‚æœfilesæœ¬èº«å°±æ˜¯æ–‡ä»¶IDåˆ—è¡¨
                files = [{'fid': fid} for fid in files]

            logging.info(f"ğŸ” å¤„ç†åˆ†ç»„: {group_name}, æ–‡ä»¶æ•°é‡: {len(files)}")

            if not group_name or not files:
                error_msg = f'åˆ†ç»„ä¿¡æ¯ä¸å®Œæ•´: group_name={group_name}, files_count={len(files) if files else 0}'
                logging.error(f"âŒ {error_msg}")
                failed_groups.append({
                    'group_name': group_name,
                    'error': error_msg
                })
                continue

            try:
                # å¦‚æœéœ€è¦åˆ›å»ºæ–°æ–‡ä»¶å¤¹
                if not target_folder_id:
                    logging.info(f"ğŸ” åˆ›å»ºæ–‡ä»¶å¤¹: {group_name} åœ¨çˆ¶æ–‡ä»¶å¤¹: {target_parent_folder_id}")
                    folder_result = create_folder_115(group_name, target_parent_folder_id)  # åœ¨æŒ‡å®šçš„çˆ¶æ–‡ä»¶å¤¹åˆ›å»º
                    logging.info(f"ğŸ” æ–‡ä»¶å¤¹åˆ›å»ºç»“æœ: {folder_result}")
                    if folder_result.get('success'):
                        target_folder_id = folder_result.get('folder_id')
                        logging.info(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºæˆåŠŸ: {group_name}, ID: {target_folder_id}")
                    else:
                        error_msg = f'åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {folder_result.get("error")}'
                        logging.error(f"âŒ {error_msg}")
                        failed_groups.append({
                            'group_name': group_name,
                            'error': error_msg
                        })
                        continue

                # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹
                logging.info(f"ğŸ” è°ƒè¯• - files æ•°æ®ç»“æ„: {files}")
                file_ids = [f.get('fid') for f in files if f.get('fid')]
                logging.info(f"ğŸ” æå–çš„æ–‡ä»¶ID: {file_ids}")

                # å¦‚æœ fid å­—æ®µä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µå
                if not file_ids:
                    logging.info("ğŸ” å°è¯•å…¶ä»–æ–‡ä»¶IDå­—æ®µ...")
                    file_ids = [f.get('fileId') for f in files if f.get('fileId')]
                    logging.info(f"ğŸ” ä½¿ç”¨ fileId å­—æ®µæå–çš„ID: {file_ids}")

                if not file_ids:
                    file_ids = [f.get('fileID') for f in files if f.get('fileID')]
                    logging.info(f"ğŸ” ä½¿ç”¨ fileID å­—æ®µæå–çš„ID: {file_ids}")

                if not file_ids:
                    file_ids = [f.get('id') for f in files if f.get('id')]
                    logging.info(f"ğŸ” ä½¿ç”¨ id å­—æ®µæå–çš„ID: {file_ids}")
                if file_ids:
                    logging.info(f"ğŸ” ç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
                    move_result = move_files_115(file_ids, target_folder_id)
                    logging.info(f"ğŸ” æ–‡ä»¶ç§»åŠ¨ç»“æœ: {move_result}")
                    if move_result.get('success'):
                        logging.info(f"âœ… æ–‡ä»¶ç§»åŠ¨æˆåŠŸ: {group_name}")
                        successful_groups.append({
                            'group_name': group_name,
                            'file_count': len(file_ids),
                            'folder_id': target_folder_id
                        })
                    else:
                        error_msg = f'ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {move_result.get("error")}'
                        logging.error(f"âŒ {error_msg}")
                        failed_groups.append({
                            'group_name': group_name,
                            'error': error_msg
                        })
                else:
                    error_msg = 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶ID'
                    logging.error(f"âŒ {error_msg}")
                    failed_groups.append({
                        'group_name': group_name,
                        'error': error_msg
                    })

            except Exception as e:
                failed_groups.append({
                    'group_name': group_name,
                    'error': str(e)
                })

        total_groups = len(groups)
        success_count = len(successful_groups)
        failed_count = len(failed_groups)

        logging.info(f"æ™ºèƒ½åˆ†ç»„æ‰§è¡Œå®Œæˆ: æˆåŠŸ {success_count}/{total_groups}, å¤±è´¥ {failed_count}")

        return jsonify({
            'success': failed_count == 0,
            'message': f'æ™ºèƒ½åˆ†ç»„æ‰§è¡Œå®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª',
            'successful_groups': successful_groups,
            'failed_groups': failed_groups,
            'total_groups': total_groups,
            'success_count': success_count,
            'failed_count': failed_count
        })

    except json.JSONDecodeError as e:
        logging.error(f"è§£æåˆ†ç»„ä¿¡æ¯JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'JSONè§£æé”™è¯¯: {str(e)}'})
    except Exception as e:
        logging.error(f"æ‰§è¡Œæ™ºèƒ½åˆ†ç»„æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/delete_empty_folders', methods=['POST'])
def delete_empty_folders():
    """åˆ é™¤ç”¨æˆ·é€‰æ‹©çš„çˆ¶æ–‡ä»¶å¤¹å†…éƒ¨ä¸åŒ…å«è§†é¢‘æ–‡ä»¶çš„å­æ–‡ä»¶å¤¹"""
    try:
        # è·å–è¯·æ±‚æ•°æ®
        if request.is_json:
            data = request.get_json()
            selected_folders = data.get('selected_folders', [])
        else:
            # å…¼å®¹æ—§çš„è¡¨å•æ ¼å¼ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
            folder_id = request.form.get('folder_id', '0')
            if folder_id != '0':
                # å¦‚æœæ˜¯æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                selected_folders = [{'fileId': folder_id, 'fileName': f'æ–‡ä»¶å¤¹_{folder_id}'}]
            else:
                return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©è¦æ£€æŸ¥çš„çˆ¶æ–‡ä»¶å¤¹'})

        if not selected_folders:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©è¦æ£€æŸ¥çš„çˆ¶æ–‡ä»¶å¤¹'})

        logging.info(f"ğŸ—‘ï¸ å¼€å§‹æ‰«æç”¨æˆ·é€‰æ‹©çš„ {len(selected_folders)} ä¸ªçˆ¶æ–‡ä»¶å¤¹")

        # åˆ†ææ¯ä¸ªé€‰æ‹©çš„çˆ¶æ–‡ä»¶å¤¹å†…éƒ¨çš„å­æ–‡ä»¶å¤¹
        folders_to_delete = []  # éœ€è¦åˆ é™¤çš„å­æ–‡ä»¶å¤¹
        folders_with_videos = []  # åŒ…å«è§†é¢‘çš„å­æ–‡ä»¶å¤¹
        failed_analysis = []  # åˆ†æå¤±è´¥çš„æ–‡ä»¶å¤¹

        for parent_folder_info in selected_folders:
            parent_folder_id = parent_folder_info.get('fileId') or parent_folder_info.get('fid')
            parent_folder_name = parent_folder_info.get('fileName') or parent_folder_info.get('filename', f'æ–‡ä»¶å¤¹_{parent_folder_id}')

            if not parent_folder_id:
                failed_analysis.append({
                    'folder_name': parent_folder_name,
                    'error': 'çˆ¶æ–‡ä»¶å¤¹IDç¼ºå¤±'
                })
                continue

            try:
                logging.info(f"ğŸ” æ‰«æçˆ¶æ–‡ä»¶å¤¹: {parent_folder_name} (ID: {parent_folder_id})")

                # è·å–çˆ¶æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å†…å®¹
                qps_limiter.acquire()
                folder_content = _get_single_level_content_from_115(parent_folder_id, 0, 1000)

                if not folder_content or 'data' not in folder_content:
                    logging.warning(f"æ— æ³•è·å–çˆ¶æ–‡ä»¶å¤¹å†…å®¹: {parent_folder_name}")
                    failed_analysis.append({
                        'folder_name': parent_folder_name,
                        'error': 'æ— æ³•è·å–æ–‡ä»¶å¤¹å†…å®¹'
                    })
                    continue

                # æ‰¾å‡ºæ‰€æœ‰å­æ–‡ä»¶å¤¹
                subfolders = []
                for item in folder_content['data']:
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ–‡ä»¶å¤¹ï¼šæœ‰pidå­—æ®µä¸”æ–‡ä»¶å¤§å°ä¸º0
                    if 'pid' in item and item.get('s', 0) == 0:
                        subfolders.append({
                            'folder_id': item.get('cid', ''),
                            'folder_name': item.get('n', 'æœªçŸ¥'),
                            'parent_folder': parent_folder_name
                        })

                logging.info(f"åœ¨çˆ¶æ–‡ä»¶å¤¹ {parent_folder_name} ä¸­å‘ç° {len(subfolders)} ä¸ªå­æ–‡ä»¶å¤¹")

                # æ£€æŸ¥æ¯ä¸ªå­æ–‡ä»¶å¤¹æ˜¯å¦åŒ…å«è§†é¢‘æ–‡ä»¶
                for subfolder in subfolders:
                    subfolder_id = subfolder['folder_id']
                    subfolder_name = subfolder['folder_name']
                    parent_name = subfolder['parent_folder']

                    try:
                        logging.info(f"ğŸ” æ£€æŸ¥å­æ–‡ä»¶å¤¹: {subfolder_name} (ID: {subfolder_id})")

                        # ä½¿ç”¨ç°æœ‰çš„ get_video_files_recursively å‡½æ•°æ£€æµ‹è§†é¢‘æ–‡ä»¶
                        video_files = []
                        get_video_files_recursively(subfolder_id, video_files)

                        if not video_files:
                            # æ²¡æœ‰è§†é¢‘æ–‡ä»¶ï¼Œæ ‡è®°ä¸ºå¯åˆ é™¤
                            folders_to_delete.append({
                                'folder_id': subfolder_id,
                                'folder_name': subfolder_name,
                                'parent_folder': parent_name
                            })
                            logging.info(f"ğŸ—‘ï¸ å­æ–‡ä»¶å¤¹ '{subfolder_name}' ä¸åŒ…å«è§†é¢‘æ–‡ä»¶ï¼Œæ ‡è®°åˆ é™¤")
                        else:
                            # åŒ…å«è§†é¢‘æ–‡ä»¶ï¼Œä¿ç•™
                            folders_with_videos.append({
                                'folder_id': subfolder_id,
                                'folder_name': subfolder_name,
                                'parent_folder': parent_name,
                                'video_count': len(video_files)
                            })
                            logging.info(f"ğŸ“ å­æ–‡ä»¶å¤¹ '{subfolder_name}' åŒ…å« {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œä¿ç•™")

                    except Exception as e:
                        logging.error(f"åˆ†æå­æ–‡ä»¶å¤¹ '{subfolder_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        failed_analysis.append({
                            'folder_name': subfolder_name,
                            'parent_folder': parent_name,
                            'error': str(e)
                        })

            except Exception as e:
                logging.error(f"æ‰«æçˆ¶æ–‡ä»¶å¤¹ '{parent_folder_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                failed_analysis.append({
                    'folder_name': parent_folder_name,
                    'error': f'æ‰«æçˆ¶æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}'
                })

        # å¦‚æœæ²¡æœ‰éœ€è¦åˆ é™¤çš„å­æ–‡ä»¶å¤¹
        if not folders_to_delete:
            message = "æ²¡æœ‰å‘ç°éœ€è¦åˆ é™¤çš„ç©ºå­æ–‡ä»¶å¤¹"
            if folders_with_videos:
                message += f"ï¼Œ{len(folders_with_videos)} ä¸ªå­æ–‡ä»¶å¤¹åŒ…å«è§†é¢‘æ–‡ä»¶å·²ä¿ç•™"
            if failed_analysis:
                message += f"ï¼Œ{len(failed_analysis)} ä¸ªæ–‡ä»¶å¤¹åˆ†æå¤±è´¥"

            return jsonify({
                'success': True,
                'message': message,
                'deleted_count': 0,
                'folders_with_videos': folders_with_videos,
                'failed_analysis': failed_analysis
            })

        logging.info(f"ğŸ—‘ï¸ å‡†å¤‡åˆ é™¤ {len(folders_to_delete)} ä¸ªæ— è§†é¢‘å­æ–‡ä»¶å¤¹")

        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        deleted_folders = []
        failed_deletions = []

        for i, folder in enumerate(folders_to_delete):
            folder_id = folder['folder_id']
            folder_name = folder['folder_name']
            parent_folder = folder['parent_folder']

            try:
                logging.info(f"ğŸ—‘ï¸ æ­£åœ¨åˆ é™¤å­æ–‡ä»¶å¤¹ ({i+1}/{len(folders_to_delete)}): {folder_name} (ID: {folder_id}) æ¥è‡ªçˆ¶æ–‡ä»¶å¤¹: {parent_folder}")
                result = delete_files_115([folder_id])

                if result.get('success'):
                    deleted_folders.append({
                        'folder_id': folder_id,
                        'folder_name': folder_name,
                        'parent_folder': parent_folder
                    })
                    logging.info(f"âœ… æˆåŠŸåˆ é™¤å­æ–‡ä»¶å¤¹: {folder_name}")
                else:
                    failed_deletions.append({
                        'folder_id': folder_id,
                        'folder_name': folder_name,
                        'parent_folder': parent_folder,
                        'error': result.get('error', 'åˆ é™¤å¤±è´¥')
                    })
                    logging.error(f"âŒ åˆ é™¤å­æ–‡ä»¶å¤¹å¤±è´¥: {folder_name} - {result.get('error', 'åˆ é™¤å¤±è´¥')}")

            except Exception as e:
                failed_deletions.append({
                    'folder_id': folder_id,
                    'folder_name': folder_name,
                    'parent_folder': parent_folder,
                    'error': str(e)
                })
                logging.error(f"âŒ åˆ é™¤å­æ–‡ä»¶å¤¹å¼‚å¸¸: {folder_name} - {str(e)}")

            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…115äº‘ç›˜å¼‚æ­¥åˆ é™¤å†²çª
            if i < len(folders_to_delete) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                logging.info(f"â³ ç­‰å¾…2ç§’è®©åˆ é™¤æ“ä½œå®Œæˆ...")
                time.sleep(2)

        # ç»Ÿè®¡ç»“æœ
        total_parent_folders = len(selected_folders)
        total_subfolders_found = len(folders_to_delete) + len(folders_with_videos)
        deleted_count = len(deleted_folders)
        failed_count = len(failed_deletions) + len(failed_analysis)
        preserved_count = len(folders_with_videos)

        logging.info(f"ğŸ¯ åˆ é™¤ç©ºå­æ–‡ä»¶å¤¹å®Œæˆ: æ‰«æ {total_parent_folders} ä¸ªçˆ¶æ–‡ä»¶å¤¹ï¼Œå‘ç° {total_subfolders_found} ä¸ªå­æ–‡ä»¶å¤¹ï¼Œåˆ é™¤ {deleted_count} ä¸ªï¼Œä¿ç•™ {preserved_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")

        # æ¸…ç†ç¼“å­˜
        if deleted_folders:
            logging.info("å·²æ¸…ç†ç›®å½•ç¼“å­˜")

        return jsonify({
            'success': failed_count == 0,
            'message': f'åˆ é™¤å®Œæˆ: æ‰«æ {total_parent_folders} ä¸ªçˆ¶æ–‡ä»¶å¤¹ï¼Œåˆ é™¤ {deleted_count} ä¸ªç©ºå­æ–‡ä»¶å¤¹ï¼Œä¿ç•™ {preserved_count} ä¸ªæœ‰è§†é¢‘çš„å­æ–‡ä»¶å¤¹ï¼Œå¤±è´¥ {failed_count} ä¸ª',
            'deleted_folders': deleted_folders,
            'folders_with_videos': folders_with_videos,
            'failed_deletions': failed_deletions,
            'failed_analysis': failed_analysis,
            'total_parent_folders': total_parent_folders,
            'total_subfolders_found': total_subfolders_found,
            'deleted_count': deleted_count,
            'preserved_count': preserved_count,
            'failed_count': failed_count
        })

    except Exception as e:
        logging.error(f"åˆ é™¤ç©ºæ–‡ä»¶å¤¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/test_ai_api', methods=['POST'])
def test_ai_api():
    """æµ‹è¯•AI APIè¿æ¥"""
    try:
        # æ£€æŸ¥AI APIé…ç½®
        if not GEMINI_API_KEY or not GEMINI_API_URL:
            return jsonify({
                'success': False,
                'error': 'AI APIæœªé…ç½®',
                'details': {
                    'gemini_api_key': bool(GEMINI_API_KEY),
                    'gemini_api_url': bool(GEMINI_API_URL)
                }
            })

        logging.info("å¼€å§‹æµ‹è¯•AI APIè¿æ¥")

        # å‘é€æµ‹è¯•è¯·æ±‚
        headers = {
            'Authorization': f'Bearer {GEMINI_API_KEY}',
            'Content-Type': 'application/json'
        }

        test_payload = {
            "model": MODEL,
            "messages": [{"role": "user", "content": "Hello, this is a test message. Please respond with 'AI API test successful'."}],
            "max_tokens": 50,
            "temperature": 0.1
        }

        response = requests.post(GEMINI_API_URL, headers=headers, json=test_payload, timeout=15)

        if response.status_code == 200:
            ai_response = response.json()
            content = ai_response.get('choices', [{}])[0].get('message', {}).get('content', '').strip()

            logging.info(f"AI APIæµ‹è¯•æˆåŠŸï¼Œå“åº”: {content}")

            return jsonify({
                'success': True,
                'message': 'AI APIè¿æ¥æµ‹è¯•æˆåŠŸ',
                'response': content,
                'model': MODEL,
                'status_code': response.status_code
            })
        else:
            error_msg = f"AI APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}"
            logging.error(f"{error_msg}, å“åº”: {response.text}")

            return jsonify({
                'success': False,
                'error': error_msg,
                'status_code': response.status_code,
                'response_text': response.text[:500]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
            })

    except requests.exceptions.Timeout:
        error_msg = "AI APIè¯·æ±‚è¶…æ—¶"
        logging.error(error_msg)
        return jsonify({'success': False, 'error': error_msg})
    except requests.exceptions.ConnectionError:
        error_msg = "æ— æ³•è¿æ¥åˆ°AI APIæœåŠ¡å™¨"
        logging.error(error_msg)
        return jsonify({'success': False, 'error': error_msg})
    except Exception as e:
        error_msg = f"æµ‹è¯•AI APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logging.error(error_msg, exc_info=True)
        return jsonify({'success': False, 'error': error_msg})

@app.route('/get_folder_content/<int:folder_id>', methods=['GET'])
def get_folder_content_by_id(folder_id):
    """é€šè¿‡GETæ–¹æ³•è·å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆç”¨äºæ–‡ä»¶å¤¹æµè§ˆå™¨ï¼‰"""
    try:
        logging.info(f"ğŸ“‚ è·å–æ–‡ä»¶å¤¹ {folder_id} ä¸‹çš„å†…å®¹")

        # è·å–æ–‡ä»¶å¤¹å†…å®¹
        folder_content = get_all_files_in_folder(folder_id, limit=100)

        if not folder_content or 'files' not in folder_content:
            return jsonify({
                'success': False,
                'error': 'æ— æ³•è·å–æ–‡ä»¶å¤¹å†…å®¹',
                'folders': [],
                'files': []
            })

        files = folder_content['files']

        # åˆ†ç¦»æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
        folders = [item for item in files if item.get('is_directory', False)]
        regular_files = [item for item in files if not item.get('is_directory', False)]

        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        formatted_folders = []
        for folder in folders:
            formatted_folders.append({
                'fileId': folder.get('id', ''),
                'filename': folder.get('name', ''),
                'type': 1,  # æ–‡ä»¶å¤¹ç±»å‹
                'size': 0,
                'is_directory': True
            })

        formatted_files = []
        for file in regular_files:
            formatted_files.append({
                'fileId': file.get('id', ''),
                'filename': file.get('name', ''),
                'type': 0,  # æ–‡ä»¶ç±»å‹
                'size': file.get('size', 0),
                'is_directory': False
            })

        return jsonify({
            'success': True,
            'folders': formatted_folders,
            'files': formatted_files,
            'current_folder_id': folder_id,
            'total_folders': len(formatted_folders),
            'total_files': len(formatted_files)
        })

    except Exception as e:
        logging.error(f"è·å–æ–‡ä»¶å¤¹å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e),
            'folders': [],
            'files': []
        })

@app.route('/move_files_direct', methods=['POST'])
def move_files_direct():
    """ç›´æ¥ç§»åŠ¨é€‰ä¸­çš„æ–‡ä»¶åˆ°æŒ‡å®šæ–‡ä»¶å¤¹"""
    try:
        move_data_json = request.form.get('move_data')
        target_folder_id = request.form.get('target_folder_id')

        if not move_data_json:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›ç§»åŠ¨æ•°æ®ã€‚'})

        if not target_folder_id:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›ç›®æ ‡æ–‡ä»¶å¤¹IDã€‚'})

        try:
            target_folder_id = int(target_folder_id)
        except ValueError:
            return jsonify({'success': False, 'error': 'ç›®æ ‡æ–‡ä»¶å¤¹IDå¿…é¡»æ˜¯æ•°å­—ã€‚'})

        move_data = json.loads(move_data_json)
        if not move_data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ–‡ä»¶éœ€è¦ç§»åŠ¨ã€‚'})

        # æå–æ–‡ä»¶IDåˆ—è¡¨
        file_ids = []
        for item in move_data:
            file_id = item.get('fileId')
            if file_id:
                try:
                    file_ids.append(int(file_id))
                except ValueError:
                    logging.warning(f"æ— æ•ˆçš„æ–‡ä»¶ID: {file_id}")
                    continue

        if not file_ids:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶IDã€‚'})

        logging.info(f"ğŸ“¦ å‡†å¤‡ç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
        logging.info(f"ğŸ“‹ æ–‡ä»¶IDåˆ—è¡¨: {file_ids}")

        # è°ƒç”¨115äº‘ç›˜ç§»åŠ¨API
        result = move_files_115(file_ids, target_folder_id)

        if result.get("success"):
            logging.info(f"âœ… æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ {target_folder_id}")
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸç§»åŠ¨ {len(file_ids)} ä¸ªæ–‡ä»¶åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ã€‚',
                'moved_count': len(file_ids),
                'target_folder_id': target_folder_id
            })
        else:
            error_message = result.get("message", "ç§»åŠ¨æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹IDæ˜¯å¦æ­£ç¡®ã€‚")
            logging.error(f"ç§»åŠ¨æ–‡ä»¶å¤±è´¥: {error_message}")
            return jsonify({'success': False, 'error': error_message})

    except json.JSONDecodeError as e:
        logging.error(f"è§£æç§»åŠ¨æ•°æ®JSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': f'JSONè§£æé”™è¯¯: {str(e)}'})
    except Exception as e:
        logging.error(f"ç§»åŠ¨æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)})

@app.route('/get_folder_info', methods=['GET'])
def get_folder_info():
    """è·å–å•ä¸ªæ–‡ä»¶å¤¹çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè½»é‡çº§ï¼Œç”¨äºç‚¹å‡»æŸ¥çœ‹ï¼‰"""
    try:
        folder_id = request.args.get('folder_id', '0')

        # å¤„ç†æ— æ•ˆçš„folder_idå€¼
        if not folder_id or folder_id == 'null' or folder_id == 'undefined':
            folder_id = '0'

        logging.info(f"ğŸ” è·å–æ–‡ä»¶å¤¹ {folder_id} çš„åŸºæœ¬ä¿¡æ¯")

        # åªè·å–ç›´æ¥å­é¡¹ï¼Œä¸é€’å½’ï¼Œæé«˜é€Ÿåº¦
        try:
            files = get_all_files_in_folder(folder_id, limit=100)
            if not files:
                return jsonify({'success': False, 'error': 'æ— æ³•è·å–æ–‡ä»¶å¤¹å†…å®¹'})

            # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
            total_items = len(files)
            folder_count = sum(1 for item in files if item.get('type') == 1)  # type=1è¡¨ç¤ºæ–‡ä»¶å¤¹
            file_count = sum(1 for item in files if item.get('type') == 0)    # type=0è¡¨ç¤ºæ–‡ä»¶
            video_count = 0
            total_size = 0

            # ç»Ÿè®¡è§†é¢‘æ–‡ä»¶å’Œå¤§å°
            for item in files:
                if item.get('type') == 0:  # æ–‡ä»¶
                    file_size = item.get('size', 0)
                    if isinstance(file_size, (int, float)):
                        total_size += file_size

                    # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                    filename = item.get('filename', '')
                    _, ext = os.path.splitext(filename)
                    if ext.lower()[1:] in SUPPORTED_MEDIA_TYPES:
                        video_count += 1

            # æ ¼å¼åŒ–å¤§å°
            if total_size >= 1024 ** 4:  # TB
                size_str = f"{total_size / (1024 ** 4):.1f}TB"
            elif total_size >= 1024 ** 3:  # GB
                size_str = f"{total_size / (1024 ** 3):.1f}GB"
            elif total_size >= 1024 ** 2:  # MB
                size_str = f"{total_size / (1024 ** 2):.1f}MB"
            else:
                size_str = f"{total_size}B"

            return jsonify({
                'success': True,
                'folder_id': folder_id,
                'total_items': total_items,
                'folder_count': folder_count,
                'file_count': file_count,
                'video_count': video_count,
                'total_size': total_size,
                'size_str': size_str
            })

        except Exception as e:
            logging.error(f"è·å–æ–‡ä»¶å¤¹ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return jsonify({'success': False, 'error': f'è·å–æ–‡ä»¶å¤¹ä¿¡æ¯å¤±è´¥: {str(e)}'})

    except Exception as e:
        logging.error(f"å¤„ç†æ–‡ä»¶å¤¹ä¿¡æ¯è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/restart_status', methods=['GET'])
def restart_status():
    """æ£€æŸ¥é‡å¯åŠŸèƒ½çŠ¶æ€"""
    try:
        is_packaged = getattr(sys, 'frozen', False)

        status = {
            'restart_available': True,
            'environment': 'packaged' if is_packaged else 'development',
            'platform': sys.platform,
            'executable_path': sys.executable,
            'argv': sys.argv,
            'working_directory': os.getcwd()
        }

        if is_packaged:
            # æ£€æŸ¥å¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            possible_executables = []
            if sys.platform == "darwin":
                possible_executables = [
                    './pan115-scraper-mac',
                    'pan115-scraper-mac',
                    './dist/pan115-scraper-mac',
                    'dist/pan115-scraper-mac',
                    sys.executable  # å½“å‰è¿è¡Œçš„å¯æ‰§è¡Œæ–‡ä»¶
                ]
            elif sys.platform == "win32":
                possible_executables = [
                    './pan115-scraper-win.exe',
                    'pan115-scraper-win.exe',
                    './dist/pan115-scraper-win.exe',
                    'dist/pan115-scraper-win.exe',
                    sys.executable
                ]
            else:
                possible_executables = [sys.executable]

            # æ£€æŸ¥å“ªä¸ªå¯æ‰§è¡Œæ–‡ä»¶å­˜åœ¨
            found_executable = None
            for exe_path in possible_executables:
                if os.path.exists(exe_path):
                    found_executable = exe_path
                    break

            status['found_executable'] = found_executable
            status['restart_available'] = found_executable is not None
        else:
            # å¼€å‘ç¯å¢ƒï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é‡å¯Pythonè„šæœ¬
            status['restart_available'] = True

        return jsonify(status)

    except Exception as e:
        logging.error(f"æ£€æŸ¥é‡å¯çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return jsonify({
            'restart_available': False,
            'error': str(e),
            'environment': 'unknown'
        })

@app.route('/restart', methods=['POST'])
def restart_app():
    logging.info("æ¥æ”¶åˆ°é‡å¯åº”ç”¨ç¨‹åºçš„è¯·æ±‚ã€‚")
    try:
        # ç«‹å³è¿”å›å“åº”ï¼Œå‘ŠçŸ¥å®¢æˆ·ç«¯åº”ç”¨ç¨‹åºæ­£åœ¨é‡å¯
        response = jsonify({'success': True, 'message': 'åº”ç”¨ç¨‹åºæ­£åœ¨é‡å¯...'})
        
        if request.remote_addr != '127.0.0.1': # ä»…å…è®¸æœ¬åœ°é‡å¯
            return "Unauthorized", 403
        print("Restart request received. Scheduling restart...")
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­æ‰§è¡Œé‡å¯é€»è¾‘ï¼Œè¿™æ ·å¯ä»¥ç«‹å³è¿”å›HTTPå“åº”
        # å¦åˆ™ï¼Œé‡å¯æ“ä½œä¼šé˜»å¡HTTPè¯·æ±‚ï¼Œå¯¼è‡´å®¢æˆ·ç«¯è¶…æ—¶
        Thread(target=restart_process_async).start()
            
        return response
    except Exception as e:
        logging.error(f"é‡å¯åº”ç”¨ç¨‹åºå¤±è´¥: {e}", exc_info=True)
        return jsonify({'success': False, 'error': f'é‡å¯å¤±è´¥: {str(e)}'})

if __name__ == '__main__':
    logging.info("å¯åŠ¨ Flask åº”ç”¨ç¨‹åºã€‚")
    # ç¡®ä¿ templates ç›®å½•å­˜åœ¨
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)

    # å¯åŠ¨ç¼“å­˜æ¸…ç†åå°ä»»åŠ¡

    print("å¯åŠ¨ Flask åº”ç”¨ç¨‹åºåœ¨ç«¯å£ 5001...")
    app.run(debug=True, host='127.0.0.1', port=5001) # å°†ç«¯å£æ”¹ä¸º 5001

